#+title: Programming/Development Notes
#+author: Craig Astill
#+OPTIONS: toc:2
#+PROPERTY: header-args:mermaid :prologue "exec 2>&1" :epilogue ":" :pupeteer-config-file ~/.puppeteerrc
#+PROPERTY: header-args:shell :prologue "exec 2>&1" :epilogue ":" :results drawer :async
#+STARTUP: overview
* AI:
- [[https://github.com/github/awesome-copilot][Github: github/awesome-copilot]] - Enhance your GitHub Copilot
  experience with community-contributed instructions, prompts, and
  configurations. Get consistent AI assistance that follows your
  team's coding standards and project requirements.
* Arch:
** Package Management:
*** [[https://forum.manjaro.org/t/how-do-i-limit-pamac-or-yays-cpu-core-usage-while-compiling/55043][How do I limit Pamac or yay's CPU core usage while compiling?]]
- Edit ~/etc/makepkg.conf~.
- Modify: ~MAKEFLAGS="-j$(($(nproc)+1))"~ to another value.
  - eg. ~MAKEFLAGS="-j$(($(nproc)-2))"~, to leave 2 cores free.
*** [[https://www.reddit.com/r/archlinux/comments/woh8fr/list_packages_installed_by_yay/][List packages installed by yay?]]
- List packages: ~<yay|pacman> -Qm~.
- List Dependencies for a package: ~yay -Rcps <package>~.
- Remove package + dependencies: ~yay -Rcs <package>~.
*** How to view `.heic` images from an iphone on arch?
Install `libheif` and then set to use an image application by default
(eg. viewnior).
#+BEGIN_SRC shell :results silent
  yay -S libheif
#+END_SRC
** Power Management:
*** Broken Hibernate:
Swap keeps being set to the main partition, instead of the swap partition.
Manually updated the ~resume=~ in: ~/etc/default/grub~ to be correct, then
regenerated Grub with: ~sudo grub-mkconfig -o /boot/grub/grub.cfg.~ See:
https://bbs.archlinux.org/viewtopic.php?pid=1928690#p1928690.
* BackEnd:
** Casing:
- =snake_case=.
- =SNAKE_UPPERCASE=.
- =kebab-case=.
- =camelCase=.
- =PascalCase=.
** C# (csharp):
*** Commands:
- [[https://learn.microsoft.com/en-us/dotnet/core/tools/dotnet-nuget-why][~dotnet nuget why~]] - Dependency graph.
*** Tools:
- [[https://dotnetfiddle.net/][C# repl]].
- [[https://docs.fluentvalidation.net/en/latest/index.html][FluentValidation]] - FluentValidation is a .NET library for building
  strongly-typed validation rules.
  - [[https://codewithmukesh.com/blog/fluentvalidation-in-aspnet-core/][Code with Mukesh: How to use FluentValidation in ASP.NET Core - Super Powerful Validations]].
**** [[https://www.cursor.com/][Cursor AI Editor]]:
Based off VSCodium (Open source VSCode fork).

Cursor cannot use the Microsoft debugger for C#, since Microsoft has
locked it down to only it's products. Everyone in the wild uses the
open source C# debugger provided by Samsung

Links for more information:
- https://github.com/Samsung/netcoredbg
- https://github.com/Cliffback/netcoredbg-macOS-arm64.nvim
- https://github.com/getcursor/cursor/issues/1634 - Extensions defined in devcontainer.json are not installing in Dev Container
  - Cursor doesn't install the ~ms-dotnettools.csharp~ extension in the Dev Container. So have to manually install it on every rebuild!
  - Once installed, the debugging session will work!
- https://guiferreira.me/archive/2025/how-to-debug-dotnet-code-in-cursor-ai/
- https://github.com/dgokcin/dotnet-cursor-debugging-with-breakpoints

Dockerfile:

#+BEGIN_EXAMPLE dockerfile
  # DevContainer dockerfile
  FROM mcr.microsoft.com/devcontainers/dotnet:9.0 AS build-deps
  ENV TZ=Europe/London
  RUN apt-get update && apt-get install -y curl tzdata unzip && rm -rf /var/lib/apt/lists/*
  ARG configuration=Development
  WORKDIR /workspace/

  # Microsoft has locked down the .NET Debugger to their products, so this
  # installs the Samsung Debugger as an alternative that can be called from
  # `launch.json`.
  FROM build-deps as cursor-debugger-deps
  # Samsungs Debugger for DevContainers running on Mac's.
  RUN mkdir -p /home/vscode/arm64 \
      && curl -sSL https://github.com/Samsung/netcoredbg/releases/download/3.1.2-1054/netcoredbg-linux-arm64.tar.gz \
      | tar -xz -C /home/vscode/arm64 \
      && chmod +x /home/vscode/arm64/netcoredbg/netcoredbg
#+END_EXAMPLE

And ~launch.json~ file for an ASP.NET C# project in a DevContainer.:

#+BEGIN_EXAMPLE json
  {
      "version": "0.2.0",
      "configurations": [
          {
              "name": "DevContainer + Cursor (netcoredbg): .NET Core Launch (Backend)",
              "type": "coreclr",
              "request": "launch",
              "preLaunchTask": "build",
              "program": "${workspaceFolder}/path/to/app.dll",
              "args": [],
              "cwd": "${workspaceFolder}/path/to/src/",
              "stopAtEntry": false,
              "console": "internalConsole",
              "pipeTransport": {
                  "pipeCwd": "${workspaceFolder}",
                  "pipeProgram": "bash",
                  "pipeArgs": [
                      "-c"
                  ],
                  "debuggerPath": "/home/vscode/arm64/netcoredbg/netcoredbg",
                  "debuggerArgs": [
                      "--interpreter=vscode"
                  ],
                  "quoteArgs": true
              },
              "env": {
                  "ASPNETCORE_ENVIRONMENT": "DevContainer"
              },
              "sourceFileMap": {
                  "/Views": "${workspaceFolder}/Views"
              }
          }
      ]
  }
#+END_EXAMPLE

Example ~devcontainer.json~:

#+BEGIN_EXAMPLE json
  // For format details, see https://aka.ms/devcontainer.json. For config options, see the
  // README at: https://github.com/devcontainers/templates/tree/main/src/dotnet
  {
  	"name": "C# DevContainer",
  	"dockerComposeFile": "docker-compose.yaml",
  	"service": "app_dev",
  	"workspaceFolder": "/workspace",
  	"mounts": [
  		// https://code.visualstudio.com/docs/devcontainers/tips-and-tricks#_persisting-user-profile
  		"source=profile,target=/root,type=volume",
  		"target=/root/.vscode-server,type=volume",
  		// https://www.marcandreuf.com/blog/2024-07-12-gitdevcont/
  		"source=${localEnv:HOME}${localEnv:USERPROFILE}/.gitconfig,target=/home/vscode/.gitconfig,type=bind,consistency=cached",
  		"source=${localEnv:HOME}${localEnv:USERPROFILE}/.ssh/,target=/home/vscode/.ssh/,type=bind,consistency=cached"
  	],
  	"customizations": {
  		"vscode": {
  			"extensions": [
  				"vsls-contrib.codetour",
  				"streetsidesoftware.code-spell-checker",
  				"ms-dotnettools.csharp",
  				"oderwat.indent-rainbow",
  				"ms-vsliveshare.vsliveshare",
  				"yzhang.markdown-all-in-one",
  				"bierner.markdown-mermaid",
  				"DavidAnson.vscode-markdownlint",
  				"42Crunch.vscode-openapi",
  				"humao.rest-client",
  				"redhat.vscode-yaml",
  				"ms-dotnettools.vscode-dotnet-runtime",
  				"ms-azuretools.vscode-docker",
  				"ms-azuretools.vscode-bicep"
  			],
  			"settings": {
  				// https://marketplace.visualstudio.com/items?itemName=ms-dotnettools.vscode-dotnet-runtime
  				// See: `I already have a .NET Runtime or SDK installed, and I want to use it`
  				"dotnetAcquisitionExtension.sharedExistingDotnetPath": "/usr/share/dotnet/dotnet",
  				// NOTE: The extension didn't like the format of the above file, so would still install
  				// .NET8. This setting forces it to use what is already installed in the container.
  				"dotnetAcquisitionExtension.allowInvalidPaths": true
  			}
  		},
  		"cursor": {
  			"extensions": [
  				"ms-dotnettools.csharp"
  			],
  			"settings": {
  				"dotnetAcquisitionExtension.sharedExistingDotnetPath": "/usr/share/dotnet/dotnet",
  				"dotnetAcquisitionExtension.allowInvalidPaths": true,
  				"omnisharp.useGlobalMono": "never",
  				"omnisharp.path": "latest",
  				"csharp.debug.netcoredbg.path": "/root/.local/bin/netcoredbg"
  			}
  		}
  	},
  	"remoteEnv": {
  		"LOCAL_WORKSPACE_FOLDER": "${localWorkspaceFolder}",
  	}
  }
#+END_EXAMPLE
*** C# Learning notes.

A good chunk of most courses are about how to program in general and OO concepts in particular. I'd suggest a schedule something like:

1. Write a console app that does something relatively simple but that you can apply proper design to.  So for example, write a console app that takes two numbers and an operator as inputs and returns an output ("3", "2", "Add" - returns 5).  We can introduce the Host, configuration providers and the builder pattern at this point, which isn't in the default Console app templates but is used by everything else.
2. Then start refactoring that to separate business and presentation logic, and prove it works by having another way of calling it - e.g. by hosting a worker service that monitors the file system for a file drop
3. Take that refactored app and start introducing dependency injection so you can replace implementations and help with testing, and use the same mechanism to allow new operations to be added with minimal code changes.
4. Write unit tests for it to introduce Moq and xUnit
5. Host the business logic in a web api and write a web front end to call it to introduce ASP.Net (for the client), and WebAPI and minimal APIs (since you can write the api in at least two different-but-related frameworks).

Basically if you can do everything in steps 1-5, then that covers a huge amount of the basics of what we do here.  The rest of it it just hosting- and IaaS/SaaS-dependent frameworks such as functions/webapps/container apps or Cosmos/EventHubs/Whatever

The DI stuff is pretty key

*** C# Learning Links:
- [[https://github.com/milanm/DotNet-Developer-Roadmap][Github: .NET Developer Roadmap 2025]] - This is a step-by-step guide
  to becoming a .NET Engineer, with links to relevant learning
  resources.
- [[https://code.visualstudio.com/docs/csharp/debugging][VSCode: C# debugging]]
*** C# Microsoft Learn Courses:
- [[https://learn.microsoft.com/en-us/training/paths/design-business-continuity-solutions/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][AZ-305: Design business continuity solutions]] - 2 modules1 hr 46 min
- [[https://learn.microsoft.com/en-us/training/paths/design-identity-governance-monitor-solutions/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][AZ-305: Design identity, governance, and monitor solutions]] - 3 modules3 hr 5 min
- [[https://learn.microsoft.com/en-us/training/paths/microsoft-azure-architect-design-prerequisites/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][AZ-305: Microsoft Azure Architect Design Prerequisites]] - 6 modules5 hr 19 min - 51% Completed
- [[https://learn.microsoft.com/en-us/training/paths/get-started-c-sharp-part-6/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][Debug C# console applications (Get started with C#, Part 6)]] - 6 modules5 hr 27 min
- [[https://learn.microsoft.com/en-us/training/paths/create-microservices-with-dotnet/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][Create cloud-native apps and services with .NET and ASP.NET Core]] - 7 modules4 hr 25 min
- [[https://learn.microsoft.com/en-us/training/modules/build-web-api-aspnet-core/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][Create a web API with ASP.NET Core controllers]] - 41 min
- [[https://learn.microsoft.com/en-us/training/paths/aspnet-core-minimal-api/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][Create web apps and services with ASP.NET Core, minimal API, and .NET]] - 3 modules1 hr 35 min
- [[https://learn.microsoft.com/en-us/training/paths/build-apps-with-dotnet-maui/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][Build mobile and desktop apps with .NET MAUI]] - 9 modules7 hr 18 min
- [[https://learn.microsoft.com/en-us/training/modules/install-configure-visual-studio-code/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][Install and configure Visual Studio Code for C# development]] - 35 min
- [[https://learn.microsoft.com/en-us/training/paths/build-dotnet-applications-csharp/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][Build .NET applications with C#]] - 6 modules3 hr 14 min
- [[https://learn.microsoft.com/en-us/training/paths/accelerate-app-development-using-github-copilot/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][Accelerate app development by using GitHub Copilot]] - 6 modules8 hr 32 min
- [[https://learn.microsoft.com/en-us/training/paths/copilot/?source=docs&tab=applied-skills-tab&ns-enrollment-type=Collection&ns-enrollment-id=bookmarks][GitHub Copilot Fundamentals - Understand the AI pair programmer]] - 12 modules5 hr 46 min

** Forth:
Some interesting links that fell out of a chat about what programming on Forth
was like:

- [[Link: https://www.forth.com/][Forth (Swift)]].
- [[https://forth-standard.org/][Forth (Standard)]].
- [[Link: Link: https://wiki.c2.com/?ExampleForthCode][C2 Wiki: Example Forth Code]].
** Go:
- [[https://go.dev/][go dev site]].
- [[https://github.com/golang/tools][Github: golang/tools]] - Repo of all =golang/x/tools= tools & static checkers.
- [[https://www.jetbrains.com/go/][Jetbrains: GoLand]] editor.
*** Cross-compiling:
- ~go env GOARCH GOOS~ - Current Systems Architecture & OS.
- ~GOOS=<darwin|linux|windows> go build~ - Build binary for a particular OS.
*** Go Tutorials:
- https://gobyexample.com/ - Minimal site with lots of good examples.
- [[https://go.dev/tour/welcome/3][Go: A Tour of Go]] - This will place a tour binary in your GOPATH's bin
  directory (=~/go/bin/tour=). When you run the tour program, it will open a
  web browser displaying your local version of the tour.
  #+BEGIN_SRC shell :results quiet
    go install golang.org/x/website/tour@latest
  #+END_SRC
*** Functions vs Methods:
- *Closure:* A function that references (access/assign) variables from outside
  its body. ie. the closure function is /"bound"/ to the variables.
- *Methods:*
  - Go has no classes, so methods apply new functionality on types.
  - Can pass the function into the constructor as an alternative to methods.
  - To declare a method with a receiver on a type, they *must* both be in the
    same package!
  - Can define methods with Pointer Receivers.
- ~func Myfunc(a ...any) (n int, err error)~ Means:
  - The [[https://pkg.go.dev/builtin#any][~any~]] type takes in any number of arguments. ie. a Variadic Parameter
    when used in a functions parameter list.
  - Returns both an Integer + Error.
  - The Uppercase =M= of =Myfunc= means that this function will be exported out
    of the package.
  - Lowercase functions are unexported (ie. *not* exported! Or Private).
*** [[https://go.dev/doc/tutorial/call-module-code][Multi-modules]]:
- Create folder for the module/package.
- Dependency Tracking: Initialise the module, so that it is added to the
  ~go.mod~ file: ~go mod init <site>/<package>~.
  - *NOTE:* the expectation is to have a route-able FQDN (like Java packages),
    to the site that holds the code (eg. Company site). For local only code,
    make it a personal namespace.
- /OPTIONAL:/ Edit routing to unpublished package: ~go mod edit -replace
  <site>/<package>=</path/to/<package>~.
  - Sync Changes: ~go mod tidy~.
- Usage: ~import "<site>/<package>"~.
*** [[https://go.dev/doc/tutorial/workspaces][Multi-Go packages (Monorepo)]]:
The following avoids doing replace directives across multiple modules + enables
local monorepos:

- Create a workspace/project to hold the multiple Go modules.
- Workspace Tracking: ~go work init ./<module_folder>~ for each go module.
- Run code: ~go run <site>/<package>~ from workspace.

Additional:

- ~go work use [-r] [dir]~: adds a use directive to the ~go.work~ file for dir,
  if it exists, and removes the use directory if the argument directory doesn’t
  exist. The =-r= flag examines subdirectories of dir recursively.
- ~go work edit~: edits the go.work file similarly to: ~go mod edit~.
- ~go work sync~: syncs dependencies from the workspace’s build list into each
  of the workspace modules.
*** Syntax:
- Type Conversion: ~T(v)~. Convert variable: =v=, to type: =T=.
  eg. ~f := Float64(42)  // 42.0~.
- Bitwise shift: ~<<~, ~>>~. eg. ~v := 1 << 10 // 1024~.
- iota: Define enumerated [[https://pkg.go.dev/builtin#pkg-constants][Constants]] with iota (untyped integer ordinal
  number). Example take from: [[https://go.dev/doc/effective_go#constants][Effective Go: Constants]].
  #+BEGIN_SRC go
    import "fmt"
    type ByteSize float64

    const (
        _           = iota // ignore first value by assigning to blank identifier
        KB ByteSize = 1 << (10 * iota)
        MB
        GB
        TB
        PB
        EB
        ZB
        YB
    )
    v := 5 MB
    fmt.Println(v)
  #+END_SRC

  #+RESULTS:
**** Packages:
- Use one-word package names.
- ~package main~ in a file that contains: ~func main() {...}~.
- ~package <folder>~ for all other files.
- Run code:
  - Relative: ~go run .~.
  - Declarative: ~go run <site>/<package>~.
*** Temporarily rewrite mod to use an unpublished package.
From the [[https://go.dev/doc/tutorial/call-module-code][Go Tutorial: Call Module Code]], you can set the ~go.mod~ to search
locally for an unpublished package instead of looking up from the
namespace. ie. no Code changes between local/remote packages.

#+BEGIN_EXAMPLE shell
  go mod edit -replace <namespace/of/package>=<../local/path/to/package>
  go mod tidy
#+END_EXAMPLE
*** Zero Values:
- boolean: ~false~.
- int: ~0~.
- float: ~0.0~.
- string: ~""~.
- pointers, functions, interfaces, slices, channels, maps: ~nil~.
** node.js:
- [[https://developer.mozilla.org/en-US/docs/Web/JavaScript/Language_overview][Mozilla Dev: JavaScript Language Overview]].
- Backend version of Javascript.
- Runs on the (Google) V8 engine, with latest code (Browsers are behind on the
  language for backwards capabilities).
- Defaults to =development= mode by default.
  - Production mode: ~NODE_ENV=production node app.js~
  - Conditionals by mode types:
    #+BEGIN_EXAMPLE js
      if (process.env.NODE_ENV === 'development') {
        app.use(express.errorHandler({ dumpExceptions: true, showStack: true }));
      }

      if (process.env.NODE_ENV === 'production') {
        app.use(express.errorHandler());
      }
    #+END_EXAMPLE
*** Asynchronous Programming:
- JavaScript is single threaded and only supports concurrency.
- 3 ways to write async code:
  - Callbacks.
    #+BEGIN_EXAMPLE js
      fs.readFile(filename, (err, content) => {
        // This callback is invoked when the file is read, which could be after a while
        if (err) {
          throw err;
        }
        console.log(content);
      });
      // Code here will be executed while the file is waiting to be read
    #+END_EXAMPLE
  - =Promise=-based.
    #+BEGIN_EXAMPLE js
      fs.readFile(filename)
        .then((content) => {
          // What to do when the file is read
          console.log(content);
        })
        .catch((err) => {
          throw err;
        });
      // Code here will be executed while the file is waiting to be read
    #+END_EXAMPLE
  - =async=/=await= - syntatic sugar for Promises.
    #+BEGIN_EXAMPLE js
      async function readFile(filename) {
        const content = await fs.readFile(filename);
        console.log(content);
      }
    #+END_EXAMPLE
- True parallelism requires Workers!
*** Classes:
- Functions that must be instantiated with: =new=.
- No code organisation enforcement (X classes per-file. Functions can return
  classes).
- Mixin: Arrow function that returns a class.
  #+BEGIN_EXAMPLE js
    const withAuthentication = (cls) =>
      class extends cls {
        authenticate() {
          // …
        }
      };

    class Admin extends withAuthentication(Person) {
      // …
    }
  #+END_EXAMPLE
- Properties:
  - Static: prepend =static=.
  - Private: prepend =#= (not =private=)(Think python =_=).
    - Private is truly private. Not accessible outside or derived classes!
*** [[https://developer.mozilla.org/en-US/docs/Web/JavaScript/Closures][Closures]] & Scopes.:
- *Closure:* Nested functions, where the inner function is using variables from
  the parent functions scope.
  - Can create variables of a closure with pre-canned lexical data, so that it
    can be reused elsewhere.
- *Lexical scoping:* How the parser determines the scope of variables available
  to the inner function, from the parent function.
- =var=: Always creates in the *global* scope!
  #+BEGIN_EXAMPLE js
    if (...) { var a = 5; }
    // Would fail with out of scope in other languages, but
    // returns `5` in JS due to `var` creating in global scope.
    console.log(a);
  #+END_EXAMPLE
- =let=: local scoped variables that can be re-initialised.
- =const= local scoped variables that can't be re-initialised, but can be
  modified (eg. adding/modifying parameters on a static object).
*** Functions:
- Functions are First Class Objects:
  - Assign to variables.
  - Passed as arguments to other functions.
  - Returned from other functions.
  - Closures support.
- Arguments:
  - Too many parameters = ignored.
  - Unspecified parameters = =undefined=.
  - Swallow additional parameters with: =...args= (like python =*args=).
  - Spread/Expand a list of args when calling a function: =myFunc(...argsArray)=.
  - No named parameters (=kwargs=), but can cheat with /"object
    destructuring"/: =area({width:2, height:3})=.
  - Default parameters: =function x(a, b, c = 5) {...}=.
- Anonymous Functions:
  - Anonymous Function: =const x = function (a, b) {}=, called by: =x()=.
    - Equivalent to: =function x(a, b) {}=.
  - Anonymous Arrow Function Expression: =const x = (...args) => {}=.
  - Anonymous Immediately Invoked Function Expression (IIFE): =(function () {})();=.
- Recursive Function:
  =function x(a) {for (let i = 0, y; (y = a.blah[i]); i++) {x(y);}}=.
- Nested function definitions can access variables in parents scope.
*** Modules:
- Host-defined module resolution: URL's or file paths (eg. relative to current
  module (not /"Project"/ root)).
- Import other modules via: =import { foo } from "./foo.js";=.
- Export functionality: =export const a = 1;=.
  - No =export= means module local variables.
*** TypeScript on Node.js:
- JavaScript superset with types.
  - Can iterate on adding TypeScript to a Project for development safety/speed
    and then compile back to JavaScript for executing.
- Install: ~npm i -D typescript~.
- Local compile to JavaScript with Node Package Execute (=npx=) and the
  TypeScript compiler (=tsc=): ~npx tsc example.ts~. Generates: =*.js= files.
- Run the generated Node.js files.
*** WebAssembly on Node.js:
- Multi-language support (C/C++/Rust) to compiled assembly-like binary
  (=.wasm=) / text (=.wat=) formats.
- Browser/Node.js supported.
- No direct OS access.
- Key Concepts
  - Module: A compiled WebAssembly binary, ie a .wasm file.
  - Memory: A resizable ArrayBuffer.
  - Table: A resizable typed array of references not stored in Memory.
  - Instance: An instantiation of a Module with its Memory, Table, and
    variables.
- Integration:
  #+BEGIN_EXAMPLE js
    // Assume add.wasm file exists that contains a single function adding 2 provided arguments
    const fs = require('node:fs');

    const wasmBuffer = fs.readFileSync('/path/to/add.wasm');
    WebAssembly.instantiate(wasmBuffer).then(wasmModule => {
      // Exported function live under instance.exports
      const { add } = wasmModule.instance.exports;
      const sum = add(5, 6);
      console.log(sum); // Outputs: 11
    });
  #+END_EXAMPLE
** PHP:
- https://onlinephp.io/ - Online PHP Repr.
- https://github.com/felixfbecker/php-language-server,
  https://hub.docker.com/r/felixfbecker/php-language-server - LSP server.
- https://docs.phpunit.de/en/10.3/index.html - test framework.
- https://github.com/mockery/mockery, https://docs.mockery.io/en/latest/ -
  mocking library.
- https://phpstan.org/user-guide/getting-started - static linter.
- https://www.slimframework.com/ - Web Apps/API framework.
*** PHP Dev Server complains of: =file or folder not found=, when period in path?:
*DON't use the PHP Dev Server!*. It will try to route to a static file if you
have a period in the path (eg. =GET /UserByEmail/fake.user@domain.com=). There
is a WONTFIX bug: https://bugs.php.net/bug.php?id=61286 with details.

Just spin up =nginx= in front of your App container, if you want to run
functional / Contract Boundary tests!!
** Telemetry:
*** [[https://opentelemetry.io/docs/][OpenTelemetry]]:
Open metrics/logging/tracing telemetry.

- [[https://opentelemetry.io/docs/instrumentation/js/getting-started/nodejs/][OpenTelemetry Docs: nodejs]].
- [[https://opentelemetry.io/docs/instrumentation/go/getting-started/][OpenTelemetry Docs: go]].
- [[https://www.apollographql.com/docs/federation/opentelemetry/][Apollo GraphQL: OpenTelemetry]].
- [[https://nextjs.org/docs/app/building-your-application/optimizing/open-telemetry][NextJS: OpenTelemetry]].
* Bash:
- ~ps -aef --forest~ for tree view of processes.
** cat:
*** Concatenate multiple mp3's together (and fix ID3 tags):
For when I want to merge a whole load of mp3 files into 1 (eg. Audio Books) for ease of management.

#+BEGIN_SRC shell
  brew install id3lib
#+END_SRC

#+BEGIN_SRC shell :dir ~/Music/<path> :var output="output.mp3" title="title" track="1" total_tracks="1"
  cat *.mp3 > $output
  id3tag -t$track -T$total_tracks -s$title $output
#+END_SRC

** certificates:
- Get certificate contents as text: ~openssl x509 -text -nout -in </path/to/cert.pem>~.
- Pull down certificate from Service: ~openssl s_client -connect <FQDN>:<port> </dev/null | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' > cert.pem~.
- Connect to a Service with a certificate: ~curl --cacert </path/to/cert.<pem|crt>> https://<FQDN>~.
** Curl:
- [[https://tldr.ostera.io/curl][Curl examples - tldr]] | simplified, community driven man pages. Transfers data
  from or to a server. Supports most protocols, including HTTP, FTP, and
  POP3. More information: https://curl.se.
- [[http://cht.sh/curl][cheat.sh/curl]].
** De/Compress Files:
- Decompress =.xz=: ~unxz <file.ext.xz>~.
** ffmpeg:
*** [[https://trac.ffmpeg.org/wiki/Encode/H.264][FFmpeg wiki: Encode/H.264]]:
Single pass with a \"visually loss-less"\ look (~output=/path/to/file.<mp4|mkv>~):

#+BEGIN_SRC shell :var input="" output=""
  ffmpeg -i $input -map 0 -c:v libx264 -crf 17 -preset slow -c:s copy -c:a copy $output
#+END_SRC
*** [[https://trac.ffmpeg.org/wiki/Encode/H.265][FFmpeg wiki: Encode/H.265]]:
Single pass with a \"visually loss-less"\ look (~output=/path/to/file.<mp4|mkv>~):

#+BEGIN_SRC shell :var input="" output=""
  ffmpeg -i $input -map 0 -c:v libx265 -crf 23 -preset fast -c:a copy -c:s copy $output
#+END_SRC

*** [[https://superuser.com/questions/556029/how-do-i-convert-a-video-to-gif-using-ffmpeg-with-reasonable-quality#556031][FFMPEG:Convert video files to GIF]]:

#+BEGIN_SRC shell
  ffmpeg -y -ss <secs_to_start_from> -t <duration_in_secs> -i <source_video> -vf fps=10,scale=320:-1:flags=lanczos,palettegen palette.png && ffmpeg -ss <secs_to_start_from> -t <duration_in_secs> -i <source_video> -i palette.png -filter_complex "fps=10,scale=320:-1:flags=lanczos[x];[x][1:v]paletteuse" <output.gif>
  ffmpeg -y -ss 8 -t 4 -i P1020397.MP4 -vf fps=10,scale=320:-1:flags=lanczos,palettegen palette.png && ffmpeg -ss 8 -t 4 -i P1020397.MP4 -i palette.png -filter_complex "fps=10,scale=320:-1:flags=lanczos[x];[x][1:v]paletteuse" output4.gif
#+END_SRC
** PDF:
*** How to decrypt a PDF for printing?:
~docker run --user $UID:$GROUPS --rm -v $(pwd):/work mgodlewski/qpdf --decrypt <input>.pdf <output>.pdf~
*** How to resize a PDF + scale content?:
~docker run --rm -v .:/app -w /app minidocks/ghostscript -o <output>_a5.pdf -sPAPERSIZE=a5 -dFIXEDMEDIA -dPDFFitPage -sDEVICE=pdfwrite <input>.pdf~
** makemkv ([[https://www.makemkv.com/developers/usage.txt][~makemkvcon~ CLI]]):
#+BEGIN_SRC shell :dir ~/Downloads/dvd_rips/ :var drive="disk4" output="<dvd_folder>"
  mkdir -p $output
  makemkvcon mkv disc:0 all $output
  sleep 2
  diskutil eject $drive
#+END_SRC

*** Applescript to rip DVD's via Mac's auto-play:

#+NAME: rip_dvd.scpt
#+BEGIN_EXAMPLE applescript
  (*
  applescript to:

  - get volume name for the disc in the disc drive.
  - Create a folder using the above volume name.
  - run a CLI application.
  - sleep 5 seconds.
  - eject disc

  ,*)

  -- set discVolume to do shell script "diskutil info $(drutil status | grep -Eo '/dev/disk[0-9]+')" & " | grep -i 'Volume Name' | awk -F': ' '{print $2}'"
  -- set discVolume to do shell script "diskutil info /dev/disk4 | grep -i 'Volume Name' | awk -F': ' '{print $2}'"
  set discVolume to do shell script "diskutil info /dev/disk4 | grep -i 'Volume Name' | cut -c 31-9999 | sed 's/ /_/g'"

  set parentFolder to POSIX path of (path to downloads folder) & "dvd_rips/"

  tell application "Finder"
      if not (exists folder (parentFolder & discVolume)) then
          -- make new folder at folder parentFolder with properties {name:discVolume}
          do shell script "ls -al " & parentFolder
          do shell script "mkdir -p " & (parentFolder & discVolume)
      end if
  end tell

  do shell script "/opt/homebrew/bin/makemkvcon mkv disc:0 all " & (parentFolder & discVolume)
  do shell script "ls -al " & (parentFolder & discVolume)

  delay 5
  do shell script "diskutil eject disk4"
#+END_EXAMPLE

Steps:

- Open Applescript Editor.
- Dump above example code and save as: =rip_dvd.scpt=.
- Export as a Read-Only, signed Application:
  - =File > Export=
    - =File Format: Application=.
    - =Run-only= ticked.
    - =Code Sign: Sign to run locally=.
  - Read-only + signing fixes issue of requesting access on every run!
- Set System Settings to run exported Application on DVD insertions:
  - =Settings > CDs & DVDs=
    - =When you insert a video DVD: Application > Open rip_dvd.app=.
- Set System Settings to grant Application Full Disk Access:
  - =Settings > Privacy & Security > Files and Folders > Add > rip_dvd.app=.
- Insert a DVD and accept access requests (should only need allowing
  on first run).

FIXME:

- Disk Eject is flakey.
- Disk drive is hard-coded.
** RDP:
- =xfreerdp= example connection string: ~xfreerdp /w:1920 /h:1080 /u:<username>
  /v:<hostname>~
- RDP to system on a Windows Domain:
  ~xfreerdp /w:1920 /h:1080 /u:<username> /d:<domain> /v:<host>~
** [[https://surf.suckless.org/][surf | suckless.org]] software that sucks less
surf is a simple web browser based on WebKit2/GTK+. It is able to display
websites and follow links. It supports the XEmbed protocol which makes it
possible to embed it in another application. Furthermore, one can point surf to
another URI by setting its XProperties.
** wget:
- [[https://www.digitalocean.com/community/tutorials/how-to-use-wget-to-download-files-and-interact-with-rest-apis][DigitalOcean: How To Use Wget to Download Files and Interact with REST APIs]].
*** REST commands:
- *GET:* ~wget -O- -q <url>~
- *POST:*
  - ~wget --method=post -O- --body-data=<json_string> --header="<key: value>" <url>~
  - ~wget -O- --post-data=<json_string> --header=<key:value> <url>~
- *PUT:* ~wget --method=put -O- --body-data=<json_string> --header="<key: value>" <url>~
- *DELETE:* ~wget --method=delete -O- --header="<key: value>" <url>~

NOTE:

- =-O-= outputs to stdout.
- =-q= quiet.
- can have 0..many: ~--header="<key: value>"~ parameters.
* BuildTools:
- [[https://www.gnu.org/software/make/][GNU Make: Makefiles with build targets]].
- [[https://taskfile.dev/#/][Task: task runner / build tool that aims to be simpler than GNU Make]].
- [[https://nx.dev/][NX: Build system for FrontEnd code, with CI/Tasks/Caching]].
* CI/CD:
** Azure DevOps:
- [[https://marketplace.visualstudio.com/items?itemName=tingle-software.dependabot][Dependabot - Visual Studio Marketplace]].
- [[https://oshamrai.wordpress.com/2019/12/27/automated-creation-of-git-pull-requests-through-azure-devops-build-pipelines/][Automated creation of GIT Pull Requests through Azure DevOps Build pipelines]].
** Gitlab:
- [[https://docs.gitlab.com/ee/user/project/quick_actions.html][Gitlab Docs: Quick Actions]] - slash commands in Gitlab.
- [[https://docs.gitlab.com/ee/user/markdown.html#gitlab-specific-references][Gitlab Docs: Special markdown syntax]] - Markdown syntax to do actions from
  commit/comments in Gitlab.
- [[https://docs.gitlab.com/ee/administration/integration/plantuml.html][Gitlab Docs: PlantUML integration]] - Configure a docker container to generate
  in-line PlantUML code blocks into images when rendering Markdown/Restructred
  Text.
- [[https://gitlab.com/gitlab-org/gitlab/-/tree/master/.gitlab/merge_request_templates][Gitlab: ~.gitlab/merge_request_templates/~]] - Gitlab's current [[https://docs.gitlab.com/ee/user/project/description_templates.html][Gitlab Docs: MR
  templates]].
- https://docs.gitlab.com/ee/user/project/releases/release_cicd_examples.html
  - Release stage for an Agent to explicitly tag the repo and handle generating
    tagged artifacts in a release job.
    - https://docs.gitlab.com/ee/ci/yaml/#release.
  - This is different to using a tag trigger and having a job that does work
    when a tag has been pushed.
    - https://docs.gitlab.com/ee/ci/yaml/#rules.
** Gitlab Articles:
- https://about.gitlab.com/blog/2022/09/06/speed-up-your-monorepo-workflow-in-git/
- https://about.gitlab.com/blog/2022/08/31/the-changing-roles-in-devsecops/ - Why and How DevOps roles are changing.
- https://about.gitlab.com/blog/2022/08/30/the-ultimate-guide-to-software-supply-chain-security/
- https://about.gitlab.com/blog/2022/08/30/top-reasons-for-software-release-delays/
- https://about.gitlab.com/blog/2022/07/21/quickly-onboarding-engineers-successfully/
- https://about.gitlab.com/blog/2022/06/29/a-story-of-runner-scaling/
- https://about.gitlab.com/blog/2022/02/16/a-community-driven-advisory-database/
- https://about.gitlab.com/blog/2022/01/20/securing-the-container-host-with-falco/
- https://about.gitlab.com/blog/2021/11/15/top-five-actions-owasp-2021/
- https://about.gitlab.com/blog/2021/11/11/situational-leadership-strategy/
- https://about.gitlab.com/blog/2021/10/11/how-ten-steps-over-ten-years-led-to-the-devops-platform/
- https://about.gitlab.com/blog/2022/08/10/securing-the-software-supply-chain-through-automated-attestation/
- https://about.gitlab.com/blog/2022/08/15/the-importance-of-compliance-in-devops/
- https://about.gitlab.com/blog/2022/08/16/eight-steps-to-prepare-your-team-for-a-devops-platform-migration/
- https://about.gitlab.com/blog/2022/08/17/why-devops-and-zero-trust-go-together/
- https://about.gitlab.com/blog/2022/08/18/the-gitlab-guide-to-modern-software-testing/
- https://about.gitlab.com/blog/2022/08/23/gitlabs-2022-global-devsecops-survey-security-is-the-top-concern-investment/
- [[https://about.gitlab.com/blog/2022/09/20/mobile-devops-with-gitlab-part-1/][Mobile DevOps with GitLab, Part 1 - Code signing with Project-level Secure Files]].
** Releases:
- https://github.com/changesets/changesets - A tool to manage versioning and changelogs
with a focus on multi-package repositories .
* Databases:
- [[https://github.com/AltimateAI/awesome-data-contracts][Github: AltimateAI/awesome-data-contracts]] - A curated list of awesome blogs,
  videos, tools and resources about Data Contracts.
** ACID Transactions:
- *Atomicity:* Failing/Succesful Transaction event.
- *Consistency:* Data consistency is maintained on success/failure of
  a transaction.
- *Isolation:* Sequential changes to overlapping data.
- *Durability:* Completed transactions are recorded in event of
  disaster.
** CAP Theorem:
You can only achieve 2 of these 3 properties of databases:

- *Consistency:* All Clients see the same data at the same time, regardless of
  Node connected to.
- *Availability:* Respond to Client Requests, even during partial Node failure.
- *Partition Tolerance:* System can tolerate network partitions (breaks)
  between some Nodes.
*** Distributed Database:
Typically will have a CP or AP database cluster since CA is not possible in a
distributed scenario due to needing to handle network partitions! ie. *There
will always be partitions, so the choices is Consistency vs Availability!*

- *Consistency (CP):* requires block further writes to all other nodes until data is
  written across them all. Need to return warnings during this
  period. eg. Banking.
  - Buy off-the-shelf Systems, *don't roll your own Distributed
    Transaction Management System!*
- *Availability (AP):*
  - Reads: Keep accepting, but may return stale data.
  - Writes: Keep accepting writes, sync once network partition is resolved.
  - *Eventual Consistency*.
  - Easier to roll out and usually good enough if User's seeing stale
    data is fine.
    - Follow up corrective measures once partition has been resolved
      an Consistent again. (eg. Order -> Delay/Reimburse email).
** Database vs Data Lake vs Data Warehouse:
Quick summary: [[https://youtu.be/-bSkREem8dM][YouTube: Database vs Data Warehouse vs Data Lake | What is the
Difference?]]
*** Database:
- OLTP - Designed to capture and record data (transactions).
- Live, Real-time data.
- Highly detailed data.
- Flexible Schema.
- Can be a bottleneck for Application/System processing.
*** Data Lake:
- Designed to capture large amounts of raw ([un-|semi-]structured) data.
  - ML/AI in current state.
  - Analytics/Reporting after processing.
- Normalised from a Lake to a Database or Data Warehosue.
*** Data Warehouse:
- OLAP (Online Analytical Processing) - Designed for Analytics/Reporting.
- Data is historical to near-real-time based on when it is updated from Source
  systems.
  - ETL process to push data into the Warehouse..
- Summarized data.
- Rigid Schema (Normalised).
- Decoupled from Application/System, so queries do not affect processing.
** DB Admin:
- [[https://hub.docker.com/_/adminer/][Docker Hub: adminer]] - Adminer (formerly phpMinAdmin) is a full-featured
  database management tool written in PHP. Conversely to phpMyAdmin, it consist
  of a single file ready to deploy to the target server. Adminer is available
  for MySQL, PostgreSQL, SQLite, MS SQL, Oracle, Firebird, SimpleDB,
  Elasticsearch and MongoDB.
  - https://www.adminer.org/ - Replace phpMyAdmin with Adminer and you will get
    a tidier user interface, better support for MySQL features, higher
    performance and more security.
** Materialised View:
- Pre-computed result set, typically for complex (time consuming) queries.
  - Best used to quickly return a result, for non-realtime data, where the
    calculation is prohibitively expensive (CPU, DB lock, User waiting on App
    response).
- Requires explicit update to pull in new data!!
  - Redshift supports auto-refresh on changes to base tables + schedule based
    on workload (support SLA's).
- Can build materialised views on top of other materialised views.
  - eg. layering different =GROUP BY='s or =JOIN='s on top of an already
    materialised set of expensive =JOIN='s.

Links:

- [[https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-overview.html][AWS Redshift: Materialised Views]].
- [[https://docs.getdbt.com/docs/build/materializations][DBT: materialisations]] - DBT uses materialisations as views for transformed
  data by default.
** MongoDB:
- Document DB.
  - BSON (Binary JSON) storage.
    - Store a range of structures. eg. Documents, objects (eg. Users).
    - Non-JSON data-types support: datatime/date, int, long, floating
      point, decimal128, byte array.
  - Denormalize data, so Document is fully contained = Faster Reads
    (minimal/no JOIN's).
  - Amorphous structure of fields between documents.
    - Live manipulation of fields.
    - Easy to change schema (Dev. driven changes).
      - *PRO: vs *SQL requiring upfront schema definitions + defined
        migrations (usually DBA)*, but I think it's a weak argument
        when you need a Boundary Contract to decouple Teams (if you
        have multiple teams accessing the same DB Data).
      - I think this /could/ be more beneficial in PoC's.
  - Schema Validation - Data Governance.
  - Migrations via: [[https://documentation.red-gate.com/flyway/learn-more-about-flyway/system-requirements/supported-databases-and-versions#Supporteddatabasesandversions-MongoDB(preview)][Flyway DB]] is in preview atm.
  - Change streams API
    - Enable Event-Driven architectures.
    - Before/after states.
    - Change Event can trigger other Events (eg. Counters/Timers).
  - ACID Transactions:
    - Document updates are atomic operations.
    - Multi-document transactions + rollbacks (RDB equivalent).
    - Distributed Transactions support.
- Queries:
  - Indexing into arrays / nested fields.
  - Automatically adds Geospatial/Time-series tags for time-based queries.
  - MQL (Mongo Query Language) = SQL equivalent.
    - Idiomatic [[https://docs.mongodb.com/drivers/][Mongo Docs: Language Drivers]].
- Distributed DB design:
  - Horizontally scale across 100's of small machines in Cluster.
  - Sharding:
    - Does not split Documents.
    - Tag to pin Documents to region(s). (Governance or dec. latency).
    - Each shard = 3+ Servers in a ReplicaSet (mini-)cluster.
    - Replicas can be installed across DataCenters.
    - *PRO: vs PostgreSQL's Vertical scaling Writer + no sharding!*
  - *PRO: vs PostgreSQL is /better/ Cloud/Language integration's.*
- Security:
  - Client-side (KMIP-compliant Key Management Provider) Field-level
    Encryption.
  - Queryable Encryption: Search encrypted data without decrypting.
- Postgresql vs Mongo:
  - [[https://www.mongodb.com/compare/mongodb-postgresql][MongoDB: Comparing MongoDB vs PostgreSQL]].
  - [[https://aws.amazon.com/compare/the-difference-between-mongodb-and-postgresql/][AWS: Wha's the Difference Between MongoDB and PostgreSQL?]]
  - [[https://www.educative.io/blog/mongodb-versus-postgresql-databases][Educative.io: MongoDB vs PostgreSQL: What to consider when
    choosing a database]].
*** When to use MongoDB?
- *Content Management Systems:* Rapid query/retrieve unstructured data
  (images, videos, text).
- *Transaction data:* Horizontal Scaling + High Availability = great
  for Transactional Financial Systems.
- *Stream Analysis:* High Scalabilty, Horizontal Partitioning and
  Flexible Schema = Streaming Data Applications + Real-time analytics.
** Postgres:
- Relational DB (RDB).
  - Mature EcoSystem/Tooling.
- ACID Transactions:
  - Read committed isolation.
  - Transaction required to update all parent/child tables at once.
- Scaling:
  - Vertically scale Primary Writer node.
  - Horizontally scale Read replicas (No Sharding!).
- [[https://postgrest.org/en/stable/][PostgREST: Serve a RESTful API from any Postgres database]].
- [[https://www.docker.com/blog/how-to-use-the-postgres-docker-official-image/][Docker Blog: How to use the Postgres Docker Official Image]].
- [[https://ellisvalentiner.com/post/2022-01-06-flattening-json-in-postgres/#:~:text=Flattening%20JSON%20objects%20using%20jsonb_to_record,objects%2C%20and%20returns%20a%20record][Flattening JSON in postgres]].
*** When to use PostgreSQL?
- *Data Warehousing:* Rapid, complex querying of structured data.
- *Ecommerce & Web Applications:* Transactions + data consistency.
- *Flexible Connections:* JSON/Foreign-data-wrappers to connect to
  other databases.
** Reporting/Tooling/Visualisation:
- https://observablehq.com/ - Jupiter Notebooks style notebooks that can
  connect to multiple data sources (no need for a Data Lake??) to provide
  customisable graphs for Analytics. Also supports comments/collaboration.
- https://sequel-ace.com/ - CLI. MySQL/MariaDB database management for macOS
  (brew).
- https://tableplus.com/ - Modern, native, and friendly GUI tool for relational
  databases: MySQL, PostgreSQL, SQLite & more. Cross-OS.
- https://documentation.red-gate.com/flyway - CLI-based DB migrations
  from a folder of SQL files. ie. Decoupled from Application/Language
  and can be hosted in a centralised repo.
*** ERD (Entity Relationship Diagram):
An ERD (Entity Relationship Diagram) is used to describe the Database Schema
with the inter-relationships between each table (entity). Typically it is a UML
style diagram. Similar to a UML Class diagram for programming.

** SQL (Structured Query Language):
- (Table(s)) Schema + Data relationships defined before data
  population.
- Schema changes = migration = /potential/ downtime or reduced
  performance.
* Data Pipelines:
** ETL: Extract, Transform, Load.
The mechanism of Extracting data from a Source (API, file, DB, Web Scraping,
...), transforming that data (PII redaction, schema changes, ...) and then
Loading it into a Target location (DB, Data Lake, Data Warehouse) for later
use.

- *Source(s) to Data Lake:* may be an EL or ETL process with minimal PII
  transforms. to keep the data RAW (or near-RAW) in the Data Lake.
- *Data Lake to Data Warehouse:* is usually an ETL process with schema
  changing + data sanitising transforms to make it suitable for consistent
  Analysis/Reporting.
** [[https://learn.microsoft.com/en-us/azure/databricks/introduction/][Azure DataBricks]]:
Azure Databricks is a unified, open analytics platform for building,
deploying, sharing, and maintaining enterprise-grade data, analytics,
and AI solutions at scale. The Databricks Data Intelligence Platform
integrates with cloud storage and security in your cloud account, and
manages and deploys cloud infrastructure on your behalf.

- Generative AI against Data Lakehouse (mix of Data Lake and Data
  Warehouse).
- NLP (ML-based) to build unique model for your business domain.
  - Natural Language document search.
  - Data Discovery.
  - ML modeling, tracking, serving.
- OpenAI support ((LLM) Large Language Models).
- Strong Governance & Security.
- ETL.
- Generating dashboards/visualisations.
- Integration:
  - [[https://learn.microsoft.com/en-us/azure/databricks/introduction/delta-comparison][Delta Lake/Sharing]] ([[https://delta.io/][Delta.io]]).
    - Delta is the foundation for storing data and tables in the
      Databricks lakehouse.
    - Extends Parquet data files with a file-based transaction log for
      ACID transactions and scalable metadata handling.
    - Data Lake = open source transactional storage layer on top of
      cloud storage.
    - Delta Tables (SQL layer on top of Delta Lake).
  - MLFlow.
  - [[https://spark.apache.org/][Apache Spark]] (including Structured Streaming. ie. ETL Streaming).
    - Multi-language engine for executing data engineering, data
      science, and ML on single-node machines or clusters.
  - Redash.
- Programmatic Access:
  - Workflows.
  - Unity Catalog - unified data governance model for data store(s).
  - Delta Live Tables.
    - Manages flow of data between many Delta Tables.
    - Declarative pipelines.
    - Batch/streaming (even on same table).
    - Management - Task Orchestration, cluster management, monitoring.
  - Databricks SQL.
  - Photon compute clusters.
  - REST API.
  - CLI.
  - Terraform.
** [[https://meltano.com/][Meltano]] (Data Pipeline):
[[https://meltano.com/][meltano]] - /"Your CLI for ELT+: Open Source, Flexible, and Scalable."/

/"Move, transform and test your data with confidence using a streamlined data
engineering workflow you’ll love."/

Basically it uses plugins to create an ETL (Extraction, Transform, Loader)
pipeline, which can be configured in YAML.

- [[https://docs.meltano.com/][Meltano Docs]].
- [[https://github.com/meltano/meltano][Github: meltano/meltano]].
- [[https://docs.meltano.com/reference/command-line-interface][Meltano Docs: CLI Reference.]]
- [[https://youtu.be/sL3RvXZOTvE][YouTube: Meltano Speedrun 2.0]] - Quick demo of: Extraction, Loading,
  Transformation + Dashboard of transformed data in ~7mins (Suggest play at
  x1.5 speed).

** DBT (Transforms):
- [[https://docs.getdbt.com/docs/quickstarts/dbt-core/quickstart][Docs DBT: DBT Core - Quick Start]] - Pretty thorough tutorial. Worth going
  through!
- [[https://github.com/dbt-labs/dbt-utils/][Github: dbt-labs/dbt-utils/]] - Additional utilities and test schema's.

** [[https://www.metabase.com/docs/latest/][Metabase]]:
Metabase is an open-source business intelligence platform. You can use Metabase
to ask questions about your data, or embed Metabase in your app to let your
customers explore their data on their own.
** [[https://segment.com/docs/getting-started/][Segment]]:
- Mix of a Message Queue / Notification system + (minimal!?) Data-Pipelines, to
  discover Customer engagement on your Site/Application.
  - Analytics, tracking actions, past aliases, screens/pages on, group/orgs,
    events.
- Similar to Meltano + DBT on the data-pipelines with
  Source/Destination/Transform concepts.
- Segment Server (SaaS solution? Enterprise??)
- Integration's:
  - Sources: PHP, Javascript, iOS.
  - Destinations: Google Analytics, Email marketing, live-chats, Data
    Warehouse, S3.
* Dev Environment Setup:
** Browsers:
*** Chrome:
**** How to enable scrolling the tab strip?
- Goto: =chrome://flags/#scrollable-tabstrip=
- Select one of the options to enable.
** Drawing tablets:
- [[https://linuxwacom.github.io/][The Linux Wacom Project]] – Wacom device support on Linux.
- [[https://docs.krita.org/en/index.html][Krita Manual]] — Krita is a sketching and painting program designed for digital
  artists.
- [[https://linux.die.net/man/1/xsetwacom][xsetwacom(1)]] - commandline utility to query and modify wacom driver settings.
- [[https://github.com/Huion-Linux/DIGImend-kernel-drivers-for-Huion
][Github: Huion-Linux/DIGImend-kernel-drivers-for-Huion]] - This is a collection of
  huion graphics tablet drivers for the Linux kernel, produced and maintained
  by the DIGImend project.
- [[https://github.com/linuxwacom/xf86-input-wacom/wiki/Calibration
][Github: linuxwacom/xf86-input-wacom - Wiki/Calibration]].
** Factory Reset / Erase / Format / Wipe:
*** Mac:
- Reboot and hold ~Command + r~ until you see the Apple logo and/or hear a
  chime.
  - On an M1 mac, you need to hold the power button down until the ~Start up
    Options~ appears.
- A macOS Utilities window should pop up.
- Select: ~Disk Utility > Drive > Erase~.
**** Secure erase an SSD:
Need to get to the ~Secure Erase Options~ to do full disk erasing.
- Pick: ~Mac OS Extended (Journaled, Encrypted)~ and set an easy password.
- After first erase, change to: ~Mac OS Extended (Journaled)~ and then select
  a: ~Secure Erase Options~, to do full disk wipe.
** LAGG (LACP):
*** [[https://forum.proxmox.com/threads/setting-up-lag-inside-proxmox.103235/][proxmox LAGG]]:
#+BEGIN_EXAMPLE
  auto lo
  iface lo inet loopback

  auto eno1
  iface eno1 inet manual

  auto eno2
  iface eno2 inet manual

  auto eno3
  iface eno3 inet manual

  auto eno4
  iface eno4 inet manual

  auto bond1
  iface bond1 inet manual
          bond-slaves eno1 eno2 eno3 eno4
          bond-miimon 100
          bond-mode 802.3ad
          bond-xmit-hash-policy layer3+4

  auto vmbr1
  iface vmbr1 inet static
          address 192.168.1.100/24
          gateway 192.168.1.254
          bridge-ports bond1
          bridge-stp off
          bridge-fd 0
          bridge-vlan-aware yes
          bridge-vids 2-4094
#+END_EXAMPLE
** Mac config:
*** iterm2
- ~Preferences > Profiles > Keys > General > <Left/Right> Option Key = Esc+~ -
  to fix ~Alt~ to be the ~Meta~ key again.
- ~Preferences > Profiles > Keys > Key Mappings~ Added a new mapping: ~Send:
  "#"~, when ~Alt+3~ is pressed. Fixes sending ~#~ when my keyboard is on the
  Mac layer + ~Esc+~ is set above.
- ~Preferences > Profiles > Colors~ - Tweak the Blue to be brighter to make it
  readable.
- ~Preferences > Profiles > Terminal > Infinite Scrollback~.
*** System
- changed mouse scrolling direction to be normal.
- ~scaled~ + ~smallest~ font = native display resolution.
- Up display timeout time in Power menu.
- Finder: [[https://discussions.apple.com/thread/251374769][How to show hidden files in finder?]] ~Command+Shift+.~ in a Finder
  window.
- ~Preferences > Sharing > AirPlayReceiver~ Disabled due to port conflict
  on 5000.
*** Brew
- [[https://brew.sh][Homebrew]].
  #+BEGIN_SRC sh
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
  #+END_SRC
- ~brew leaves~ list packages without dependencies.
- Backup via: ~brew bundle~:
  #+BEGIN_SRC shell
    echo "---- Brew Bundle. Restore with: brew bundle ..."
    brew bundle dump -f --describe
    echo "---- Brew Bundle contents..."
    brew bundle list
  #+END_SRC
- Restore from a brew bundle:
  #+BEGIN_SRC shell
    brew bundle
  #+END_SRC
  - Additional restore steps:
    #+BEGIN_SRC shell
      echo "---- Enable autoraise service ..."
      brew services start autoraise
      echo "---- Symlink Emacs, but also need to Command+Option drag the Emacs app to: /Applications/ to show in spotlight ..."
      # ln -s /opt/homebrew/opt/emacs-plus*/Emacs.app /Applications
      echo "---- New way of Symlinking Emacs into the /Applications/ folder to work with spotlight ..."
      osascript -e 'tell application "Finder" to make alias file to posix file "/opt/homebrew/opt/emacs-plus@30/Emacs.app" at POSIX file "/Applications"'
    #+END_SRC
**** emacs:
- [[https://github.com/d12frosted/homebrew-emacs-plus][Github: d12frosted/homebrew-emacs-plus]] set to the latest branch:
  #+BEGIN_SRC sh
    brew tap d12frosted/emacs-plus
    brew install emacs-plus@30 --with-native-comp --with-mailutils --with-xwidgets
  #+END_SRC
- Then =Command+Option= drag =/opt/homebrew/opt/emacs-plus*/Emacs.app= to
  =/Applications= in Finder.
- *NOTE:* need to do the reinstall dance because of the use of options:
  #+BEGIN_SRC sh
    brew uninstall emacs-plus@30
    brew install emacs-plus@30 --with-native-comp --with-mailutils --with-xwidgets
  #+END_SRC
- mu.
- aspell.
- cmake.
- cmake-docs
- ~markdown~ (markdown-preview).
***** Fix =Ctrl+<arrow>= getting swallowed.
Check =Settings > Keyboard Shortcuts > Mission Control=, to see if they have
the control arrow keys (=^<arrow>=) in use.
**** Dev:
- git-lfs (had to pin, see wiki).
- ~helm~.
- ~lens~ (GUI Kubernetes).
- ~awscli~
- ~xquartz~ for X11 server.
- ~wget~
- ~swig~.
- ~miniforge~ (M1 macs need this instead of miniconda to work).
- ~poetry~.
- ~docker --cask~ to pull down the Docker Desktop (https://formulae.brew.sh/cask/docker).
- ~dive~ (inspect size of docker layers).
- ~yq~ (YAML/XML/TOML CLI
  processor)(https://github.com/kislyuk/yq)(https://github.com/wagoodman/dive/issues/300
  ~yq -r .services[].image docker-compose.yml | xargs -n 1 dive --ci~
- ~hadolint~ - lint dockerfiles (https://github.com/hadolint/hadolint))
***** DBT:
#+BEGIN_SRC shell :results silent
  brew tap dbt-labs/dbt
  brew install dbt-postgres
#+END_SRC
***** postgres:
- Utilities (like =psql=) without installing =postgres=: :results drawer
  #+BEGIN_SRC shell
    brew reinstall libpq
  #+END_SRC
  - Then add: ~export PATH="/usr/local/opt/libpq/bin:$PATH"~, to: =~/.zshrc=.
  - See: [[https://stackoverflow.com/questions/44654216/correct-way-to-install-psql-without-full-postgres-on-macos][StackOverflow: Correct way to install =psql= without full postgress on MacOS]].
***** [[https://postgrest.org/en/stable/][postgrest]]:
PostgREST is a standalone web server that turns your PostgreSQL database
directly into a RESTful API. The structural constraints and permissions in the
database determine the API endpoints and operations.

- ~brew services stop postgres~ to avoid conflict with any dev containers.
- Install:
  #+BEGIN_EXAMPLE shell
    brew install postgrest
  #+END_EXAMPLE
***** python:
You can install python via brew, but it doesn't symlink: ~python3~ to
~python~. This is how to install + fix:

#+BEGIN_SRC shell :results silent
  brew install python
  ln -sF /usr/local/bin/python3 /usr/local/bin/python
#+END_SRC
**** Experiments:
- ~rust~, ~rustup~.
**** Fix symlink not making =<program>.app= show up in spotlight:
Problem is that standard symlinks (~ln -s /path/to/program.app /Applications/~)
doesn't work as an alias for discovery in spotlight since the Mac may confuse
the link as a path to a folder (~.app~ files are really folders).

[[https://apple.stackexchange.com/questions/106249/spotlight-and-alfred-cant-find-alias-to-emacs-app][Workaround]]:

- Open =Finder= and search for Program e.g. ~Cmd+Shift+G~ type path.
- Create an alias by ~Cmd+Opt~ clicking Program and dragging to ~/Applications/
  folder.
**** laptop:
- iterm2
- [[https://github.com/ankitpokhrel/jira-cli][Github: ankitpokhrel/jira-cli]].
***** autoraise:
- [[https://github.com/sbmpost/AutoRaise][Github: sbmpost/AutoRaise]] - focus follows mouse.
- [[https://github.com/Dimentium/homebrew-autoraise][Github: Dimentium/homebrew-autoraise]] - Brew formulae.
#+BEGIN_SRC shell :results silent
  brew tap dimentium/autoraise
  brew install autoraise
  brew services start autoraise
#+END_SRC
***** [[https://rectangleapp.com/][rectangle]]:
rectangle (snap to area shortcuts).
#+BEGIN_SRC shell :results silent
  brew install rectangle
#+END_SRC
*** FireFox
- ~about:config~ ~browser.tabs.tabMinWidth = 0~ to disable tab scrolling.
*** Docker
**** Install [[https://formulae.brew.sh/cask/docker][Docker Desktop]]:
#+BEGIN_SRC shell :results silent
  brew install --cask docker
#+END_SRC
- Follow [[https://docs.docker.com/desktop/mac/permission-requirements/][Docker Docs: Understanding permission requirements for Mac]] to update
  =/etc/hosts= to have the following:
  #+BEGIN_EXAMPLE shell
    127.0.0.1	localhost
    127.0.0.1	kubernetes.docker.internal
  #+END_EXAMPLE
**** Best-Practices
- https://pythonspeed.com/articles/poetry-vs-docker-caching/
- Create an explicit Bridge network for Host access to a container. Default
  network is locked down. eg.
  #+BEGIN_EXAMPLE yaml
    services:
      container-name:
      image: app:tag
      networks:
        - backend

    networks:
      # Without setting a `driver` field, this is a User-defined `bridge` network.
      # Which will be ideal for Production environments for inter-cluster connections.
      backend:
  #+END_EXAMPLE
**** Run AMD64 containers on ADM64:
- https://erica.works/docker-on-mac-m1/
- https://forums.macrumors.com/threads/docker-on-m1-max-horrible-performance.2321545/
- https://stackoverflow.com/questions/70649002/running-docker-amd64-images-on-arm64-architecture-apple-m1-without-rebuilding
- https://enjoi.dev/posts/2021-07-23-docker-using-amd64-images-on-apple-m1/
- https://www.reddit.com/r/docker/comments/o7u8uy/run_linuxamd64_images_on_m1_mac/
- https://medium.com/homullus/beating-some-performance-into-docker-for-mac-f5d1e732032c
-
**** Building AMD64 containers on ARM64:
- https://docs.docker.com/desktop/multi-arch/
- https://hublog.hubmed.org/archives/002027
- [[https://github.com/docker/for-mac/issues/5364][Github: docker/for-mac: "platform" option in docker-compose.yml ignored (preview version) #5364]]
- https://tongfamily.com/2021/12/15/the-weirdness-that-is-amd64-on-apple-m1-silicon/
- http://www.randallkent.com/2021/12/31/how-to-build-an-amd64-and-arm64-docker-image-on-a-m1-mac/
- https://docs.docker.com/buildx/working-with-buildx/
-
**** Podman (Docker alternative)
- https://medium.com/team-rockstars-it/how-to-implement-a-docker-desktop-alternative-in-macos-with-podman-bbf728d033da
- https://stackoverflow.com/questions/70892894/run-docker-compose-with-podman-as-a-backend-on-macos
- [[https://github.com/containers/podman/issues/13456][Github: containers/podman -  MacOS helper daemon (podman-mac-helper) fails to start and "mount" /var/run/docker.sock #13456]]
- https://devopscube.com/podman-tutorial-beginners/
-
**** Tooling
- [[https://github.com/emacs-lsp/dap-mode/issues/406][Github emacs-lsp/dap-mode: Feature request: support docker #406]]
** Raspberry Pi:
*** [[https://forum.manjaro.org/t/guide-install-manjaro-arm-minimal-headless-on-rpi4-with-wifi/96515][Manjaro headless install directly to a MicroSD card]]:
- Download minimal ARM iso from: https://manjaro.org/download/.
- Unpack compressed image.
- Burn to MicroSD card with: ~sudo dd if=~/Downloads/Manjaro-ARM-minimal*.img of=/dev/mmcblk0 bs=1M status=progress && sync~
- Mount ~ROOT_MNJRO~
  - Click in Thunar, which auto-mounts to: ~/var/run/media/root/~.
  - Or: ~sudo mount -o rw /dev/mmcblk0p2 /mnt~.
- Add WiFi config:
  #+BEGIN_SRC bash
    sudo mkdir -p /mnt/var/lib/iwd
    sudo touch /mnt/var/lib/iwd/<ssid>.psk
    echo "[Security]" >> /mnt/var/lib/<ssid>.psk
    echo "Passphrase=<password>" >> /mnt/var/lib/<ssid>.psk
  #+END_SRC
- Unmount and plug into the Pi and boot.
- ~ssh root@<ip>~
- You'll connect into the CLI Wizard.
*** Kiosk mode:
- *TODO:* Fill out with other details (retroactively looking at an existing
  Pi3B+ with a [[https://shop.pimoroni.com/products/hyperpixel-4?variant=12569539706963][Pimoroni: HyperPixel 4.0 (non-touch) display).]]
- Autostart Chromium by editing:
  ~/rootfs/home/pi/.config/lxsession/LXDE-pi/autostart~ with:
  #+BEGIN_EXAMPLE shell
    @xset s off
    @xset -dpms
    @xset s noblank
    @chromium-browser --kiosk http://<ip/fqdn> --start-fullscreen --incognito
  #+END_EXAMPLE
** Window Managers:
- [[https://polybar.github.io/][Polybar]] - A fast and easy-to-use tool for creating status bars
- [[https://suckless.org/][Dwm, dmenu | suckless.org]] software that sucks less. Home of dwm, dmenu and
  other quality software with a focus on simplicity, clarity, and frugality.
- [[https://github.com/i3/i3/discussions][Github: i3/i3 - Discussions]].
** Terminals:
- [[https://github.com/alacritty/alacritty][Github: alacritty/alacritty]]: A cross-platform, OpenGL terminal emulator.
- [[https://sw.kovidgoyal.net/kitty/][kitty]] - The fast, feature-rich, GPU based terminal emulator.
* Docker:
- [[https://www.youtube.com/watch?v=fqMOX6JJhGo][YouTube: Docker Tutorial for Beginners - A Full DevOps Course on How to Run
  Applications in Containers]].
- [[https://nodramadevops.com/containers/][No Drama DevOps: Containers]].
- ~--progress=plain --no-cache~ - Plaintext output + don't condensed cached
  layer output.
** Best Practices:
*** No Root Access:
A container should never be run with root-level access. A role-based access
control system will reduce the possibility of accidental access to other
processes running in the same namespace. Either:

- Create a non-root user in the container:
  #+BEGIN_EXAMPLE dockerfile
    FROM python:3.5
    RUN groupadd -r myuser && useradd -r -g myuser myuser
    <HERE DO WHAT YOU HAVE TO DO AS A ROOT USER LIKE INSTALLING PACKAGES ETC.>
    USER myuser
  #+END_EXAMPLE
- Or while running a container from the image use, ~docker run -u 4000
  python:3.5~. This will run the container as a non-root user.
*** Trusted Image Source:
- Docker 1.8 feature that is disabled by default.
- ~export DOCKER_CONTENT_TRUST=1~ to enable.
- Verifies the integrity, authenticity, and publication date of all Docker
  images from the Docker Hub registry, by preventing access to unsigned images.
** Clean-up:
- Nuclear remove everything:

  #+BEGIN_SRC shell :results quiet
    docker system prune -af
  #+END_SRC

- Removing containers, volumes and dangling images:

  #+BEGIN_SRC shell :results quiet
    docker container prune -f
    docker volume prune -f
    docker image prune -f
  #+END_SRC
- Remove unused images: ~docker image prune --all~.
** [[https://containers.dev/][DevContainers]]:
[[https://docs.docker.com/build/guide/multi-stage/][Docker Docs: multi-stage]] containers with a /"dev"/ target that you can connect
to from your IDE of choice to have a consistent development environment.

- [[https://github.com/devcontainers/cli][Github: devcontainers/cli]] - This repository holds the dev container CLI,
  which can take a devcontainer.json and create and configure a dev container
  from it.
- [[https://containers.dev/guide/dockerfile][DevContainers: Using Images, Dockerfiles, and Docker Compose]] - Steps to
  create the files to build dev containers.
- [[https://containers.dev/supporting][DevContainers: Supporting tools and services]] - IDE (eg. Visual Studio Code) /
  Tools / Services notes.
- [[https://happihacking.com/blog/posts/2023/dev-containers/][HappiHacking: Dev Containers: Consistency in Development]].
- [[https://happihacking.com/blog/posts/2023/dev-containers-emacs/][HappiHacking: Dev Containers Part 2: Setup, the devcontainer CLI & Emacs]].
- [[https://robbmann.io/posts/emacs-eglot-pyrightconfig/][Robbmann: Virtual Environments with Eglot, Tramp, and Pyright]].
** ~docker-compose~:
- ~docker-compose up --build~ to force a rebuild (and ignore any previous
  built images).
- ~docker-compose down~ stops (~docker-compose stop~) all running containers in
  the docker compose file and then cleans up containers/networks/images.
** Docker Desktop:
*** Workaround: [[https://github.com/docker/for-mac/issues/7075][Rosetta breaking docker builds on Silicon Macs from 4.25.x]]:
Currently hitting: [[https://github.com/docker/for-mac/issues/7075]["Use Rosetta" makes build for platform linux/amd64 extremely
slow #7075]], where builds are breaking on Silicon Mac.

Steps to downgrade Docker Desktop (until a fix is found):

#+BEGIN_SRC shell
  brew uninstall --cask docker
  brew install --cask https://raw.githubusercontent.com/Homebrew/homebrew-cask/da8973f03a3ff7cce56b0319fa51dd6fc80a2456/Casks/d/docker.rb
#+END_SRC

** Docker Swarm:
Orchestrator (similar to Kubernetes) but built by the Docker Team.
*** Visualize Docker Swarm Containers across Nodes:
- [[https://github.com/dockersamples/docker-swarm-visualizer][Github: dockersamples/docker-swarm-visualizer]] - Constrain to the Master node
  to visualise the containers across all nodes from the Web Browser.

  Vlisualizer deployed via ~docker run~:
  #+BEGIN_EXAMPLE shell
    docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer
  #+END_EXAMPLE

  Visualizer deployed via Docker Swarms ~docker service~:
  #+BEGIN_EXAMPLE shell
    docker service create --name=viz --publish=8080:8080/tcp --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock dockersamples/visualizer
  #+END_EXAMPLE
** Networks:
- Can use container name to connect between containers.
- ~docker run -d --name=app1 --link db:db my-app1~ The `--link` command writes
  the provided Container Name (+IP) into: ~/etc/hosts~, so that all references
  to the linked Container work.
*** ~bridge~:
- The default network that all docker containers (without network config) are
  created in.
- Assigns private IP's to each container (eg. ~172.17.0.x~).
- Requires explicit create command to create additional bridge networks.
- DNS defaults to: ~127.0.0.11~.
- Port Mapping to expose Container Ports to the Host.
  - Can run multiple Containers with the same internal port.
*** ~none~:
- Network with no external access.
*** ~Host~:
- Directly map Containers onto the Hosts IP + Port range.
- No ~port~ config required for mapping.
- Cannot support multiple Containers re-using the same Port, due to Host-side
  conflicts.
** Performance:
- Uses ~cgroups~ (Control Groups) to allocate Hosts CPU/Memory to containers.
- Use ~--cpu/--memory~~ to constrain the running container.
** Reduce image size:
- If using ~COPY~ to pull in directories. Add a ~.dockeringnore~ file to add
  exclusions. eg. ~.git~, ~**/tests~, ~**/*.ts~.
- Generate/install in the image at build time instead of ~COPY~ = Docker layer
  caching.
- Check for ~-slim~/~alpine~ versions of the base image.
- Move ~COPY~ commands near end of the file. Avoid Cache misses!
- Pull in versioned OS-packages. Avoid Cache misses, but more Platform burden!
- Use multi-stage docker files to build code in a fat stage, but copy in the
  artifacts in to the thin stage with an ~ENTRYPOINT~

  #+BEGIN_EXAMPLE dockerfile
    FROM microsoft/dotnet:2.2-sdk AS builder
    # 1730MB Fat Stage.
    WORKDIR /app

    COPY *.csproj  .
    RUN dotnet restore

    COPY . .
    RUN dotnet publish --output /out/ --configuration Release

    FROM microsoft/dotnet:2.2-aspnetcore-runtime-alpine
    # 161MB Thin stage.
    WORKDIR /app
    COPY --from=builder /out .
    EXPOSE 80
    ENTRYPOINT ["dotnet", "aspnet-core.dll"]
  #+END_EXAMPLE
* Emacs:
** Core:
*** Cleaning up an org buffer:
- ~M-x apropos~ to discover functions.
- ~org-next-block~ jump to next Org block of any type. ~C-cC-vC-n~ only does
  code blocks.
- ~M-q~ fills a paragraphs in a region correctly.
- ~C-c -~ on region to convert to an unordered (~-~) list.
- ~M-x org-cycle-list-bullet~ to cycle between list types. Twice = ordered
  numbered list.
*** Change font size in GUI Emacs buffer:
- Increase: ~C-xC-+~.
- Decrease: ~C-xC--~.
*** How to enter Diacritics (eg. caret) above characters?
See: [[https://www.masteringemacs.org/article/diacritics-in-emacs][Mastering Emacs: Diacritics in Emacs]].

#+BEGIN_EXAMPLE text
  C-x 8 <symbol> <character>
  ;; Example for: â.
  C-x 8 ^ a
  ;; With the caret being generated by pressing: =shift+6=.
#+END_EXAMPLE
*** Yasnippet:
- [[https://youtu.be/xmBovJvQ3KU][YouTube: Supercharge your Emacs / Spacemacs / Doom with Yasnippets!]] ~13mins
  walkthrough.
** org-mode:
- ~org-eww-copy-for-org-mode~ to copy text + links from Eww to Org. ~C-y~ to
  paste.
*** Build Your Website with Org Mode - System Crafters
[2022-11-05 Sat 08:50]
https://systemcrafters.net/publishing-websites-with-org-mode/building-the-site/
*** Convert markdown to org:
~docker run --rm  -v .:/data pandoc/latex -f markdown -t org -o <target.org> <source.md>~
*** Formatting:
- [[https://orgmode.org/manual/Emphasis-and-Monospace.html][Emphasis and Monospace]]
- *bold*
- /italic/
- _underlined_
- =verbatim=
- ~code~
- +strike-through+
- src_python{inline python}  # ~src_<lang>[<header_arguments>]{<code>}~ [[https://orgmode.org/manual/Structure-of-Code-Blocks.html#Structure-of-Code-Blocks][Structure of Code Blocks]]
- code blocks
#+NAME: <name>
#+BEGIN_SRC <language> <switches> <header arguments>
  <body>
#+END_SRC
- quote blocks
  #+BEGIN_QUOTE
  <body>
  #+END_QUOTE
*** PlantUML + Org Babel:
- https://orgmode.org/worg/org-contrib/babel/languages/ob-doc-plantuml.html
- plantuml block
  #+begin_src plantuml :file designs/hello-uml.png
  Bob -> Alice : Hello World!
  #+end_src
** regex:
*** How to rejoin multi-line hyphen split words?
The following example is how to replace a hyphen split word across multiple
lines and place it back onto one line. ie.

#+BEGIN_EXAMPLE text
# Before:
Sentence split across multi-
ple lines.

# After:
Sentence split across
multiple lines.
#+END_EXAMPLE

#+BEGIN_SRC emacs-lisp
M-x replace-regexp
\s-q\(\w+\)-\(^J\)\(\w+\) → ^J\1\3
#+END_SRC
*** How to upcase a group during ~M-x replace-regexp~?
Emacs step if I want to replace a replacement group and upcase it. eg. from:
~data_type: boolean~, to: ~data_type: BOOLEAN~.

- ~M-x replace-regexp~.
- Find: ~data_type: \(.*\)~.
- Replace: ~data_type: \,(upcase \1)~.

This will work for other elisp built-in's. eg.

- ~\,(downcase \1)~.
- ~\,(capitalize \1)~.
*** [[http://ergoemacs.org/emacs/find_replace_inter.html][ErgoEmacs: Find Replace in directories]] / [[https://www.gnu.org/software/emacs/manual/html_node/efaq/Replacing-text-across-multiple-files.html][GNU Emacs: Replacing text across multiple files]]:
- Either:
  - ~M-x find-name-dired~, enter filename wildcard.
  - Mark ~m~ files (~t~ marks all files), then press ~Q~.
  - ~<find> regex~ return, ~<replace> string~ return.
  - Confirm/deny replace with the usual: ~!~, ~y~, ~n~.
- Or:
  - ~C-x p r~ in a =project= managed repo.
  - ~<find> regex~ return, ~<replace> string~ return.
  - Confirm/deny replace with the usual: ~!~, ~y~, ~n~.
*** How to add an integer sequence to a region regex?
From: [[https://stackoverflow.com/questions/49519627/emacs-replace-regexp-with-incremental-sequence][StackOverflow: Emacs replace regexp with incremental sequence]].

I wanted to add values to an enum as part of a refactor (Add enum
integer values, add new enum names to map to existing values,
deprecate old enums). Here's an example to do it from 0-based
sequence:

#+BEGIN_EXAMPLE csharp
  public enum EnumToRefactor
  {
      badValueOne,
      badValueTwo,
      badValueThree,
  }
#+END_EXAMPLE

=M-x replace-regexp <ret> \(.*\), <ret> \1 = \# <ret>= gives:

#+BEGIN_EXAMPLE csharp
  public enum EnumToRefactor
  {
      badValueOne = 0,
      badValueTwo = 1,
      badValueThree = 2,
  }
#+END_EXAMPLE

The link also has an example with adding padded values that count up
with: =\,(let ((start (+ 1000 (* 4 \#)))) (format "%d-%d" start (+
start 3)))= to give:

#+BEGIN_EXAMPLE shell
  result_A_in_S1-S2.txt -> result_A_in_1000-1003.txt
  result_A_in_S1-S2.txt -> result_A_in_1004-1007.txt
  result_A_in_S1-S2.txt -> result_A_in_1008-1011.txt
  result_A_in_S1-S2.txt -> result_A_in_1012-1015.txt
#+END_EXAMPLE

*** How to continue an existing sequence from different start points with replace regex?
I wanted to renumber some values in a =dired= buffer, but instead of
increasing by a sequence, I wanted to take the existing numbers and
set a new start point for the sequence:

#+BEGIN_EXAMPLE sh
  01x5 - a.jpg
  01x5 - a.txt
  01x6 - b.jpg
  01x6 - b.txt
#+END_EXAMPLE

To pad the second value with zero's and start from =01=
=M-x replace-regexp <ret> 01x\(\w+\)\(.*\), <ret> 01x\,(format "%02d" (- (string-to-number \1) 4)) <ret>= gives:

#+BEGIN_EXAMPLE sh
  01x01 - a.jpg
  01x01 - a.txt
  01x02 - b.jpg
  01x02 - b.txt
#+END_EXAMPLE

To pad the second value with zero's but start from =15=:
=M-x replace-regexp <ret> 01x\(\w+\)\(.*\), <ret> 01x\,(format "%02d" (+ (string-to-number \1) 10)) <ret>= gives:

#+BEGIN_EXAMPLE sh
  01x15 - a.jpg
  01x15 - a.txt
  01x16 - b.jpg
  01x16 - b.txt
#+END_EXAMPLE

** DAP:
*** Registering a debug template for: ~dap-mode~, to use.
#+BEGIN_EXAMPLE emacs-lisp
(dap-register-debug-template
  "Python :: Run pytest (projectX buffer)"
  (list :type "python"
        :args ""
        :cwd "/Users/<user>/projects/projectX/"
        :program nil
        :module "pytest"
        :arguments "-p no:warnings"
        :request "launch"
        :name "Python :: Run pytest (projectX buffer)"))
#+END_EXAMPLE
** Jupyter:
- https://discourse.julialang.org/t/jupyter-integration-with-emacs/21496/5 -
  basic ~IJulia~ + ~jupyter~ install steps (no use-package).
* FrontEnd:
- https://builtwith.com/ - Get Tech Stack that a site is built with.
** GraphQL:
*** Making requests with curl:
See: https://www.maxivanov.io/make-graphql-requests-with-curl/

- Mutation query:
  #+BEGIN_EXAMPLE shell
    curl 'https://graphql-api-url' \
      -X POST \
      -H 'content-type: application/json' \
      --data '{
        "query":"mutation { createUser(name: \"John Doe\") }"
      }'
  #+END_EXAMPLE
- Mutation query with defined response data and specific endpoint:
  #+BEGIN_EXAMPLE shell
    curl 'https://<random-server>' \
      -X POST \
      -H 'content-type: application/json' \
      --data '{
        "query":"mutation {retrieveApplication(identifier: \"user@email.com\", pin_code: \"1234\") {id url}}"
      }'
  #+END_EXAMPLE
** Javascript:
- [[https://stdlib.io/][stdlib.io - a standard library for javascript and node.js]].
*** filter:
Create a new array with a callback function that applies a conditional
statement against each element of the array. elements are pushed into
the new array when the condition returns true.

#+BEGIN_SRC js
  const students = [
    { name: 'Quincy', grade: 96 },
    { name: 'Jason', grade: 84 },
    { name: 'Alexis', grade: 100 },
    { name: 'Sam', grade: 65 },
    { name: 'Katie', grade: 90 }
  ];

  const studentGrades = students.filter(student => student.grade >= 90);
  return studentGrades; // [ { name: 'Quincy', grade: 96 }, { name: 'Alexis', grade: 100 }, { name: 'Katie', grade: 90 } ]
#+END_SRC

#+RESULTS:
| { | name: | Quincy | grade: | 96 | } | { | name: | Alexis | grade: | 100 | } | { | name: | Katie | grade: | 90 | } |

*** map:
Create a new array from an existing one, by applying a callback
function to each element of the array.

#+BEGIN_SRC js
  const numbers = [1, 2, 3, 4];
  const doubled = numbers.map(item => item * 2);
  console.log(doubled); // [2, 4, 6, 8]
#+END_SRC

#+RESULTS:
: [2 (\, 4) (\, 6) (\, 8)]

*** reduce:
Reduce an array down to one value. The callback function is called
against every array item (only need =accumulator= and =currentValue=.

#+BEGIN_SRC js
  const numbers = [1, 2, 3, 4];
  const sum = numbers.reduce(function (result, item) {
    return result + item;
  }, 0);
  console.log(sum); // 10
#+END_SRC

#+RESULTS:
: 10
: undefined

** React:
- View cookies in browser: ~Developer Tools > Storage Tab > Cookies~.
- ~redux~ is the store of all BE DB state in the FE.
- Add ~&profile~ to an API call to get performance output!!
- ~npm install --target_arch=x64~ - until there is arm support.
- https://github.com/marmelab/react-admin
- Print all object properties: ~console.log(Object.getOwnPropertyNames(obj))~.
*** AST (Abstract Syntax Tree):
What is Abstract Syntax Tree?

#+BEGIN_QUOTE
It is a hierarchical program representation that presents source code structure
according to the grammar of a programming language, each AST node corresponds
to an item of a source code.
#+END_QUOTE

- https://itnext.io/ast-for-javascript-developers-3e79aeb08343
*** [[https://nextjs.org/docs][Next.js]] (React Framework):
Next.js is a React framework for building full-stack web
applications. You use React Components to build user interfaces, and
Next.js for additional features and optimizations.

Under the hood, Next.js also abstracts and automatically configures
tooling needed for React, like bundling, compiling, and more. This
allows you to focus on building your application instead of spending
time with configuration.

- App Router: New. Latest React Features (Server Components/Streaming).
- Pages Router: Old. Bespoke Server-rendered React apps.
** UI Testing:
*** [[https://playwright.dev/][playwright]]:
~playwright~ is a modern equivalent to [[https://www.selenium.dev][Selenium]]. Benefits include:

- Speed.
- Handles installation of isolated browsers to test/debug against.
- Support for [[https://playwright.dev/docs/test-parallel][sharding/parallelisation]] of tests.
- auto-wait.
- Built-in: [[https://playwright.dev/docs/trace-viewer-intro][Tracing]], [[https://playwright.dev/docs/codegen-intro][Recording (via Codegen)]], [[https://playwright.dev/docs/running-tests#test-reports][Reporting]].
- Good [[https://playwright.dev/docs/intro][Docs]].
- Cross-Platform.
- Cross-language API.
- Native [[https://playwright.dev/docs/ci-intro][CI]]/Local development support. eg. Auto-Trace on first retry (but not
  subsequent failures).
- [[https://playwright.dev/docs/test-snapshots][Visual Comparisons]] of screenshots.
- Uses [[https://playwright.dev/docs/test-assertions][Assertions]] via [[https://jestjs.io/docs/expect][~expect~]] library.
- Automatic install of Dependencies/CI on first install.

[[https://playwright.dev/docs/best-practices][Best Practices]].
* Git:
- https://www.conventionalcommits.org/en/v1.0.0/ - A specification for adding
  human and machine readable meaning to commit messages.
- https://github.com/conventional-changelog/conventional-changelog - Generate
  changelogs and release notes from a project's commit messages and metadata.
- https://github.com/conventional-changelog/releaser-tools - Create a
  GitHub/GitLab/etc. release using a project's commit messages and metadata.
** Alternative VCS's:
- [[https://www.fossil-scm.org/home/doc/trunk/www/index.wiki][Fossil]] - Self-contained with VCS as a binary with: Project Management, WebUI,
  Lightweight, self-host friendly, autosync.
- [[https://pijul.org/][Pijul]] ([[https://pijul.org/manual/introduction.html][Pijul Docs]]) - Strong focus on conflict resolution (beyond GIT),
  order-less applying of changes, partial clones. Support to import from Git
  (not optimised).
** Configure git repo with explicit SSH Key:
In cases where you need to use an explicit SSH key for a repo, eg. Personal +
Work Github account, and you want a personal repo accessiable by both
personal/work systems. Github prevents the re-use of an SSH key across Github
Accounts ([[https://docs.github.com/en/authentication/troubleshooting-ssh/error-key-already-in-use][Github Docs: Error: Key already in use]]). Therefore you need to create
a Personal SSH key on the Work System to clone the Personal repo.

#+BEGIN_EXAMPLE sh
  git clone git@provider.com:userName/projectName.git --config core.sshCommand="ssh -i ~/.ssh/private_ssh_key" --config user.email="<personal_email>"
  git -c core.sshCommand="ssh -i ~/.ssh/private_ssh_key" submodule update
  git submodule foreach git config core.sshCommand "ssh -i ~/.ssh/private_ssh_key"
#+END_SRC

Or after the fact with:
#+BEGIN_EXAMPLE sh
  git config --local --add core.sshCommand "ssh -i ~/.ssh/private_ssh_key"
#+END_EXAMPLE

eg. for my personal repos to be separate on a work laptop.
#+BEGIN_EXAMPLE sh
  git config core.sshcommand "ssh -i ~/.ssh/id_ed25519_personal"
  git config user.email jackson15j@hotmail.com
  git -c core.sshCommand="ssh -i ~/.ssh/id_ed25519_personal" submodule init
  git -c core.sshCommand="ssh -i ~/.ssh/id_ed25519_personal" submodule update
  git submodule foreach git config core.sshCommand "ssh -i ~/.ssh/id_ed25519_personal"
  git submodule foreach git config user.email "jackson15j@hotmail.com"
#+END_EXAMPLE

** Git Hooks:
- [[https://pre-commit.com][~pre-commit~]] - A framework for managing and maintaining mutli-language
  pre-commit hooks. Repo of hooks in YAML format.
*** Why is the failing exit code ignored and not blocking commits??
Calling commands like:~go-task~, will run in a separate sub-shell, but the exit
code is not passed to the shell running the ~pre-commit~. ~|| exit $?~, pipes
the exit code to the main shell process. See: [[https://stackoverflow.com/questions/29969093/exit-1-in-pre-commit-doesnt-abort-git-commit][SO: Exit in a ~pre-commit~ does
not abort ~git commit~]].

#+BEGIN_EXAMPLE shell
  go-task lint || exit $?
#+END_EXAMPLE
** How to show log history for a function?:
Call: ~git log -L :<function_name>:</path/to/file> [-n
<number_of_log_entries_to_show>]~.

*NOTE:* if it fails with: =fatal: -L parameter '<function_name>' starting at
line 1: no match=, then you need a local/global =.gitattributes= with the
following in: =*.<ext> diff=<language>=. eg. for PHP: =*.php diff=php=

Global =~/.gitattributes= can be found by: ~git config --global
core.attributesfile ~/.gitattributes~.
* Job hunting:
- https://github.com/readme/guides/technical-interviews
- https://www.codinginterview.com/
- https://www.pramp.com/#/
- https://hackingthesystemsdesigninterview.com
- https://blog.bytebytego.com - Newsletter by Alex Xu (Author of: /"System Design Interview/").
- https://www.siliconmilkroundabout.com - London-based Job Fair.
- https://www.hackerrank.com/ - practice coding sections.
* Kubernetes:
- [[https://kurl.sh/][kURL: Open Source Kubernetes Installer]].
- https://docs.k3s.io - Lightweight Kubernetes. Easy to install, half the
  memory, all in a binary of less than 100 MB.
- https://www.cncf.io/kubecon-cloudnativecon-events/
- [[https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/][Kubernetes docs: Pull image from a Private Registry]].
** Articles:
- [[https://medium.com/qonto-way/scaling-airflow-on-kubernetes-lessons-learned-a0d3d0417fc1][Medium: Scaling Airflow on Kubernetes: lessons learned (Qonto)]].
- [[https://medium.com/clarityai-engineering/running-airflow-in-kubernetes-and-aws-lessons-learned-part-1-77be9556846c][Medium: Running Airflow in Kubernetes and AWS: Lessons learned Part 1
  (Clarity AI)]].
- [[https://devpress.csdn.net/k8s/62fb6bed7e6682346618e98e.html][Devpres: Airflow: Scaling with AWS EKS]].
** Config:
*** Resources/Limits:
- Using monitoring to judge pod's average/peak usage. Repeat revaluation.
  - What is the monitoring tools granularity?
  - How long is your pod at peak for?
  - Does your pod workflow change by time? (top of the hour, first of the
    month/year, seasonal dates (eg. April (finance), Holiday sales)).
- Try to set =limit=/=request= within x1-1.5 of each other to avoid resource
  contention if all pods peak at the same time.
- ={local_task_job.py:149} INFO - Task exited with return code
  Negsignal.SIGKILL= is the Airflow message for a pod being killed from
  exceeding set resources.
- Use Pod Templates for resources and batch into sizes for ease.
*** Avoid Pod destruction for long-running tasks?
Set: ~"cluster-autoscaler.kubernetes.io/safe-to-evict": "false"~, if doing
long-running batch jobs (eg. ETL) and you need to avoid Kubernetes killing pods
partway through.

In AWS, check: =AWS AZRebalance=, which ignores =safe-to-evict= and re-balances
pods across Availability Zones. This feature can be deactivated through the
suspended_processes parameter in the [[https://registry.terraform.io/modules/lablabs/eks-workers/aws/0.7.1/examples/complete?tab=inputs][Terraform "eks-worker" module]].
** Helm Charts:
- Hierarchical to call sub-charts as sub-dependencies.
- Values to be passed into the charts.
*** [[https://eigentech.slack.com/archives/CH1CHKYP8/p1650553648237999][how does one deploy from a local helm chart without publishing it?]]
- ~helm upgrade --install <deployment_name> <local_chart_dir>~
*** Dagster docs + dump current helm chart values: https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm
*** [[https://helm.sh/docs/chart_template_guide/debugging/][Debugging Templates]]:
- ~helm lint~ is your go-to tool for verifying that your chart follows best
  practices.
- ~helm install --dry-run --debug~ or ~helm template --debug~: We've seen this
  trick already. It's a great way to have the server render your templates,
  then return the resulting manifest file.
- ~helm get manifest~: This is a good way to see what templates are installed
  on the server.
- **NOTE:** variable substitution still happens on commented out code in
  templates, so comment out broken sections if it fails to render with ~helm
  install --dry-run --debug~.
- YAML node typing eg. ~age: !!str 21~, or: ~port: !!int "80"~.
**** TODO Document Debugging Workflow                              :WORKFLOW:
- Are there docs already on Confluence on debugging.
- Raise Task to add vscode/emacs debug tasks to ~eigen~.
- Document the workflow with the debugger (include vscode/emacs tutorial links).
- How to debug into a Docker container? - new DockerFile section with ~debugpy~ ??
*** [[https://stackoverflow.com/questions/72126048/error-exec-plugin-invalid-apiversion-client-authentication-k8s-io-v1alpha1-c][SO: invalid apiVersion "client.authentication.k8s.io/v1alpha1"]]
- ~aws eks update-kubeconfig --name ${EKS_CLUSTER_NAME} --region ${REGION}~.
*** [[https://github.com/bitnami/charts/issues/10539][Github/bitnami: Helm charts repository ~index.yaml~ retention policy #10539]] - Drama!!
** Kubernetes Networks:
*** Ingress:
- [[https://www.youtube.com/watch?v=GhZi4DxaxxE][YouTube: Kubernetes Ingress Explained Completely for Beginners]].
- Ingress is the LoadBalancer/Routing defined within the Kubernetes Cluster
  config.
- Still require an external, to the Cluster, Load Balancer (or Proxy) but this
  will just have to deal with a single root URL that is passed into your
  Cluster's Ingress (and then routed to the correct Service's Pod(s)).
- Equivalent to a reverse-proxy like: nginx, HaProxy, Traefik.
**** Ingress Controller:
- Commonly use nginx (or others) as an Ingress Controller
  (eg. ~nginx-ingress-controller~ image).
- Deployment/Service/ConfigMap/Auth Yaml's.
**** Ingress Resource:
- Handles routing to respective service based off the requested URL.
- Can handle 1 or multiple Domain Paths, by creating a ~rule~ for each ~path~.
- ~kubectl describe ingress <image>~
** Local Development:
- https://necessaryeval.com/2021/09/01/kubernetes-primer/ - Local development
  with ~minikube~.
- https://kubernetes.io/blog/2018/05/01/developing-on-kubernetes/
  - Local vs. remote development.
  - Tools:
    - https://github.com/Azure/draft - aims to help you get started deploying
      any app to Kubernetes. It is capable of applying heuristics as to what
      programming language your app is written in and generates a Dockerfile
      along with a Helm chart. It then runs the build for you and deploys
      resulting image to the target cluster via the Helm chart. It also allows
      user to setup port forwarding to localhost very easily.
    - https://github.com/GoogleCloudPlatform/skaffold - tool that aims to
      provide portability for CI integrations with different build system,
      image registry and deployment tools.
    - https://github.com/solo-io/squash - consists of a debug server that is
      fully integrated with Kubernetes, and a IDE plugin.
    - https://www.telepresence.io/ - connects containers running on developer’s
      workstation with a remote Kubernetes cluster using a two-way proxy and
      emulates in-cluster environment as well as provides access to config maps
      and secrets.
    - https://github.com/vapor-ware/ksync - Synchronizes application code (and
      configuration) between your local machine and the container running in
      Kubernetes.
- https://kubernetes.io/docs/tasks/debug/debug-cluster/local-debugging/ -
  Developing and debugging services locally using telepresence.
- http://next.nemethgergely.com/blog/using-kubernetes-for-local-development -
  Local development via ~minikube~ & ~skaffold~.
** [[https://docs.replicated.com/][Replicated]]:
- https://docs.replicated.com/ - Replicated allows software vendors to package
  and securely distribute their application to diverse customer environments,
  including both on-premises and cloud environments.
- https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/
* ML:
** ML Articles:
- https://simonwillison.net/2022/Jul/9/gpt-3-explain-code/
** DagFlow
- [[https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm][Dagster: deploying with Helm]].
* Networks:
** Debugging Throughput vs Latency:
How to debug an issue from Throughput + Latency graphs?

- [[https://www.youtube.com/v/f7VsHLk_Z8c?version=3][YouTube: Throughput vs Latency: How to Debug a Latency Problem]].
  - *Intrinsic Latency:* Reqeust/Response Time.
    - Reduce processing time = reduced latency.
  - *Queuing Latency:* Time queued before being processed.
    - Max Throughput > Request rate = Drain queue.
    - Total Latency = Queuing Latency + Intrinsic Latency.
  - *Throughput:* Rate of work (cps).
    - Max Throughput = request-handlers / intrinsic-latency.
    - Increase Threads/Workers = increased Throughput.
- Development Aims:
  - Reduce Intrinsic Latency (Faster HW, Efficient code path, reduce hops).
  - Increase Processing Capacity (More threads, horizontally scale).

+---------------------+--------------------------+--------------------------+
| Performance         | Queuing (falling behind) | No Queue                 |
| Improvements        |                          |                          |
+---------------------+--------------------------+--------------------------+
| Increase processing | Increases max throughput | Increases max throughput |
| capacity (threads)  +--------------------------+--------------------------+
|                     | Reduces queuing latency  | Latency remains constant |
+---------------------+--------------------------+--------------------------+
| Reduce Intrinsic    | Increase max throughput  | Increases max throughput |
| Latency             +--------------------------+--------------------------+
|                     | Reduces queuing latency  | Reduces total latency    |
+---------------------+--------------------------+--------------------------+

** DNS:
- https://root-servers.org/ - Root DNS servers at the top of the DNS
  hierarchy. These root servers farm out requests down to Top-Level
  (io/com/net/edu/...) servers who farm out to down to Secondary-Level
  (amazon.com/github.com/...) DNS servers to complete Name-IP lookups.
- *Local Resolver Library:* Local DNS Cache.
- *Local DNS Server:* Hosted by ISP's as a DNS Cache + inspect
  traffic/requests.
*** Frizt box router - LAN DNS Settings location:
- =Home Network > Network > Network Settings=
  - =Additional Settings > IP Addresses > IPv4 Settings=
    - =Local DNS server=
** [[https://grpc.io/][gRPC]]:
#+BEGIN_QUOTE
  Why gRPC?

  gRPC is a modern open source high performance Remote Procedure Call
  (RPC) framework that can run in any environment. It can efficiently
  connect services in and across data centers with pluggable support
  for load balancing, tracing, health checking and authentication. It
  is also applicable in last mile of distributed computing to connect
  devices, mobile applications and browsers to backend services.
#+END_QUOTE

- [[https://grpc.io/docs/what-is-grpc/introduction/][gRPC Docs: What is gRPC?]]
  - [[https://protobuf.dev/overview/][Protocol Buffers]] Request/Response(s).
  - gRPC Server + Client Stub expose methods (parameters + return
    types).
  - Client/Server languages do not need to match.
- RPC: Call a function on a Remote System, as if it was a local
  object.
  - *RISK: Don't do thousands of small calls in a loop!*
    - *Issues:* Network latency/bandwidth hits, /potential/ call rate
      limits/rejection/IP-banning.
    - *Workaround:* Bulk network transfer, then loop local parsing of
      response.
*** [[https://protobuf.dev/overview/][Protocol Buffers]]:
- JSON-like binary serialization defined in =.proto= files.
  - Centralise =.proto= files to support generation (=protoc=
    compiler) of client-stubs/server classes in multiple languages.
  - Generated classes can then de/serialize & share data.
- Serialize MB's of Data.
- Suitable for Network Traffic or Data Storage.
- Extendable without invalidating existing data or Code changes.
  - Ignore new fields.
  - Use default if deleted.
  - Use Empty if deleted repeated fields.
  - New code reading old messages = defaults for missing fields.
- Language/platform-neutral structure.
- Compact.
- Fast parsing.
- Language integration's.
**** Don't use Protocol Buffers when:
- Data is larger than memory (suggested avoid past few MB's).
- You can not compare binary serialisations. ie. parse and then compare!
- Messages are not compressed (but can be externally).
- Use [[https://en.wikipedia.org/wiki/FITS][FITS]] format for speed/size efficiencies of scientific
  multi-dimensional arrays.
- Poor support in non-object-orientated languages.
- Not self-describing. ie. need =.proto= file to interpret!
- Not a formal standard!
* People Skills:
** Feedback:
- Aim for a learning opportunity.
- Constructive & Actionable feedback, based on facts (where possible).
- Follow up with questions on specifics.
*** The BID model.
*BID* stands for *Behaviour* > *Impact* > *Dialogue*, and has the power to
 transform relationships.

- *Behaviour:* Describe the behaviour you observed, keeping it non-judgemental
  and specific.
- *Impact:* Describe the impact of the behaviour. Again, keep it simple and
  non-judgemental. Note: Impact here might be at the individual emotional level
  (how you felt), or at a more cultural level or in relation to someone, or
  something else (the meeting over-ran, we missed our financial target).
- *Dialogue:* Open discussion around opportunities. Be future focussed where
  you can.

*** Triggers that get in the way of feedback:
- *Truth Trigger:* Reject feedback on belief it is factually wrong.
- *Relationship Trigger:* Reject feedback based on person giving it.
- *Identity Trigger:* Reject feedback that challenges your
  identity/self-perception.
*** Trigger Workarounds:
- Pause.
- Acknowledge.
- Question.
  - Use BID.
  - Ask for specifics (in good/bad feedback).
  - Don't use generic questions! (non-actionable questions/answers).
  - /"What is the one thing I can improve on?"/ (focused)
  - /"Can I get feedback on this new thing I am doing, after it is done?"/
    (pre-request feedback).
  - /"Can you walk me through that?"/
*** Finding Feedback Situations:
- *Look for outcomes:* Notice when someone creates a desirable one.
- Apply *BID* for positive/negative feedback.
- *Learn from praise:* Ask for specifics.
** STAR (Situation, Task, Action, Result):
STAR can be used to talk about past experiences. AR can be used for
future situations.

- *Situation:* Describe the situation you were in.
- *Task:* Describe the task you had to do.
  - Challenges.
  - Constraints.
  - Deadlines.
  - Issues.
- *Action:* Describe the action(s) you took / would take.
  - Teamwork.
  - Leadership.
  - Initiative.
  - Integrity.
- *Results:* Describe the outcome (or expected outcome of your action(s)).
  - Achievements.
  - Improvements.
  - Cost savings.
  - Delivery.
** Winning Arguments:
*** Tech Debt:

**** The benefits of upgrading Languages/Dependencies.
It can be hard to justify doing upgrades vs. Feature Development. Try following
Solutions:

- Make it so easy to do the task that it can be done, without scheduling,
  alongside Feature Development.
- Identify the User Value. eg. /"As a User I want to minimize the chances of
  being hacked by the flaws in current version of: <Language/dependency>./"

See: [[https://www.youtube.com/watch?v=vSuJqMRG1WM][YouTube: TECHNICAL STORIES DON'T WORK]].
* Python:
** Python Articles:
- [[https://gregoryszorc.com/blog/2023/10/30/my-user-experience-porting-off-setup.py/][My User Experience Porting Off Setup.py]] - Very thorough article of a package
  maintainer running the gauntlet to update their package from ~python
  setup.py~ to current =pyproject.toml= + build tools, to support Python3.12.
- https://pythonspeed.com/
- https://about.gitlab.com/blog/2022/09/06/test-your-software-supply-chain-security-know-how/
- https://pythoninsider.blogspot.com/2022/09/python-releases-3107-3914-3814-and-3714.html -
  Python releases 3.10.7, 3.9.14, 3.8.14, and 3.7.14 are now available + CVE fix.
** Build Tools:
- https://github.com/benfogle/crossenv - Virtual Environments for
  Cross-Compiling Python Extension Modules.
** CLI packages:
- https://typer.tiangolo.com/ - FastAPI's spin-off for CLI's.
- https://github.com/pallets/click - Command Line Interface Creation Kit
- https://cloup.readthedocs.io/en/stable/ - Click + Option Groups.
- https://github.com/astanin/python-tabulate - Pretty-print tabular data.
- https://github.com/termcolor/termcolor - Abstract out setting text colours.
** Debugging:
- https://github.com/ztlevi/LSP-Debug/blob/master/README.md#L4-L9 - debug
  python via DAP - editor support.
- https://github.com/bloomberg/memray - Python memory profiler.
- https://github.com/benfred/py-spy - Python sampling profiler.
*** [[https://github.com/pdbpp/pdbpp][Github: pdbpp/pdbpp]]:
Drop in replacement for ~pdb~ that does dot completions and syntax
highlighting.

- ~pytest --pdb~ to drop into a ~pdb~ session on test failure.
- ~list~ - show surrounding code at point.
- ~where|whatis~ - show traceback.
- Original ~pdb~ import is under: ~pdb.pdb.*~.
** Django:
- [[https://books.agiliq.com/projects/django-admin-cookbook/en/latest/index.html][Django Admin Cookbook]].
- [[https://django-extensions.readthedocs.io/en/latest/graph_models.html][django-extensions: Graph Models]].
** Celery:
*** Debugging:
**** Celery's remote debugger:
  #+BEGIN_EXAMPLE python
  from celery.contrib import rdb
  ...
  rdb.set_trace()
  #+END_EXAMPLE
- Then connect over telnet: ~telnet localhost 6900~.
- If in docker:
  - add: ~CELERY_RDB_HOST=0.0.0.0~ to ~.env~.
  - Expose Celery debug port in ~docker.compose.yml~. eg. ~6901~
  - ~telnet localhost 6901~ from host.
**** Debug Celery via PDB in Django:
- Add ~CELERY_TASK_ALWAYS_EAGER=True~ in: ~settings.py~.
** Conda:
- https://conda-forge.org/blog/posts/2020-10-29-macos-arm64/ - macOS ARM builds
  on conda-forge.
- [[https://github.com/conda/conda/issues/9957][conda/conda - conda update breaks conda with ImportError: libffi.so.6: cannot open shared object file #9957]]
*** Conda + Emacs:
- [[https://github.com/necaris/conda.el/issues/39][necaris/conda.el - Cannot activate any env on OSX #39]]
*** Conda + Docker:
- https://uwekorn.com/2021/03/01/deploying-conda-environments-in-docker-how-to-do-it-right.html
*** Mamba instead of Conda:
- https://mamba.readthedocs.io/en/latest/user_guide/mamba.html
- https://labs.epi2me.io/conda-or-mamba-for-production/
** [[https://fastapi.tiangolo.com/][FastAPI:]]
Fast to write/run API framework.

Features: Simple syntax, Renders JSON, Easy integration ([[https://strawberry.rocks/][Strawberry]]
(GraphQL), [[https://docs.pydantic.dev/latest/][Pydantic]] (Validation), [[https://www.python-httpx.org/][HTTPX]] (Test client)), Auto-generates
OpenAPI output (Swagger/Redoc UI's), Websockets, CORS, Cookie
Sessions, Auth (OAuth2, JWT, HTTP basic).

Looks to be much better than Flask for ease.
** Flask:
- [[https://flask.palletsprojects.com/en/latest/patterns/][Flask: Patterns]] - Pretty good list of typical use-cases & coding
  patterns to follow when using Flask to do things like: DB access,
  templating, Lazy loads, caching, factories, tasks (celery),
  streaming, etc.
** Packaging:
*** [[https://hatch.pypa.io/latest/][Hatch]]:
Not tried it yet but:

#+BEGIN_QUOTE
Hatch is a modern, extensible Python project manager.

Features:

- Standardized build system with reproducible builds by default.
- Robust environment management with support for custom scripts.
- Easy publishing to PyPI or other indexes.
- Version management.
- Configurable project generation with sane defaults.
- Responsive CLI, ~2-3x faster than equivalent tools.
#+END_QUOTE
*** poetry:
- [[https://python-poetry.org/docs/managing-environments/#switching-between-environments][Set poetry python version]]: ~poetry env use python<x.y>~.
- ~poetry show --tree~ for poetry dependency graph.
**** https://github.com/opeco17/poetry-audit-plugin
**** [[https://github.com/python-poetry/poetry/issues/2094#issuecomment-1243195601][python-poetry/poetry: Poetry is extremely slow when resolving the dependencies (#2094)]]:
@Kache, It appears to search through dependencies depth-first, rather than breadth-first. As a result, you've probably got a something earlier in your pyproject.toml that depends on ddtrace, so the dependency resolver grabbed that version and tried to resolve using that, rather than the ddtrace version you've specified.

I've had some success moving the dependencies I want exact version logic prioritizing earlier in the pyproject.toml file.

(I also disabled IPv6, upgraded to poetry 1.2x, and have reduced the possible space for the troubling aws libraries (boto3 and awsci, for me) so those go at the very end of my dependency file and have only a few recent versions to chew through.

I'm seeing dependency resolution time between 5 and 35 seconds most of the time now.
*** [[https://setuptools.pypa.io/en/latest/index.html][setuptools]] + [[https://github.com/pypa/setuptools_scm/][setuptools_scm]]:
~setuptools~ & ~pip~ / /"PyPa/" have moved on to fully support
~pyproject.toml~-only Python packages. With just a ~pyproject.toml~ file we
have:

- Metadata.
- Isolated builds.
- Tooling config.
- Dynamic versioning from Git.

#+BEGIN_EXAMPLE shell
  python -m venv .venv
  source .venv/bin/activate
  pip install .
  pip install .[<group>]
  python -m build  # `pip install build` if not in `pyproject.toml`.
#+END_EXAMPLE

See:

- [[https://github.com/pypa/setuptools_scm/][Github: pypa/setuptools_scm]] - Dynamic Versioning.
- [[https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html][SetupTools Docs: Configuring setuptools using ~pyproject.toml~ files]].
- [[https://peps.python.org/pep-0633/][PEP 633 – Dependency specification in pyproject.toml using an exploded TOML
  table]] - Detail on current TOML definitions.
- [[https://peps.python.org/pep-0621/][PEP 621 – Storing project metadata in ~pyproject.toml~]].
- [[https://packaging.python.org/en/latest/tutorials/packaging-projects/][Python Docs: Packaging Python Projects]].
- [[https://github.com/jackson15j/python_homework_config_file_parser][Github: jackson15j/python_homework_config_file_parser]] - a project that is
  pure python packaging and ~project.toml~-only.
** Security:
- https://github.com/sonatype-nexus-community/jake - report vulnerabilities.
- https://adamj.eu/tech/2019/04/10/how-to-score-a+-for-security-headers-on-your-django-website/
** Templating:
- https://www.makotemplates.org/ - Mako is a template library written in
  Python. It provides a familiar, non-XML syntax which compiles into Python
  modules for maximum performance.
** Testing:
*** [[https://hypothesis.readthedocs.io/en/latest/][hypothesis]]:
Hypothesis is a Python library for creating unit tests which are simpler to
write and more powerful when run, finding edge cases in your code you wouldn’t
have thought to look for. It is stable, powerful and easy to add to any
existing test suite.
- https://hypothesis.works/
- Uses ML to do [[https://en.wikipedia.org/wiki/QuickCheck][/"Property-based testing/".]]
*** pytest:
- [[https://docs.pytest.org/en/6.2.x/warnings.html#disabling-warning-capture-entirely][Disable warnings]] with: ~-p no:warnings~.
** Web Frameworks:
- [[https://www.tornadoweb.org/en/stable/][Tornado]] - Python web framework and asynchronous networking library. Ideal for
  long polling, WebSockets and other long-lived connections.
* Security:
** Attacks
*** Confused Deputy:
In an Implicit Trust system; the Confused Deputy is when a Service
without authorization to a Resource gets a Downstream Service with
authorization to interact with the Resource on its' behalf. ie. an
escalation of privileges attack:

- ServiceA has no scope for DataX/FunctionX.
- ServiceB has scope for DataX/FunctionX.
- ServiceB implicitly trusts ServiceA.
- ServiceA requests ServiceB for DataX/FunctionX on it's behalf.
  - ServiceB uses it's scope to get DataX/FunctionX and return response
    to ServiceA.

The fix for this is for ServiceB to have checked the ServiceA's
authorization to access the Resource. ie. move closer to Zero Trust
mindset.

*** Man in the Middle:
An attack where a third-party inserts themselves unknowingly between
two other communicating parties as a proxy. The attack is then able to
sniff, manipulate the data in-transit, fork the conversation, or
continue the conversation directly with one of the parties.

ie. The malicious equivalent of: transparent/munging proxies,
B2B (back-to-back) User Agents.

** Best Practices:
- [[https://cheatsheetseries.owasp.org/index.html][OWASP: Cheat Sheet Series]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Attack_Surface_Analysis_Cheat_Sheet.html][OWASP: Attack Surface Analysis Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Authentication_Cheat_Sheet.html][OWASP: Authentication Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Authorization_Cheat_Sheet.html][OWASP: Authorization Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Cryptographic_Storage_Cheat_Sheet.html][OWASP: Cryptographic Storage Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html][OWASP: Docker Security Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html][OWASP: Input Validation Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Threat_Modeling_Cheat_Sheet.html][OWASP: Threat Modeling Cheat Sheet]].

*** Categorising MicroServices:
Zero Trust can be used as a Spectrum to avoid the pain of implementing
it everywhere. eg. Categorisation of Services:

- *Public:* Public Domain data.
- *Private:* Data that requires authentication (+ authorization!?).
- *Secret:* PII data. Only by that User (+ select parties).

Then limit Services access to data by their tagged tier level (and
more open access).

*** CyberSecurity:
- *Identify:*
  - Who are potential attackers?
  - What will they target?
  - Where are you most vulnerable?
  - Threat Modelling.
- *Preventative:*
  - Steps to stop an attack happening.
  - Protect your key assets.
  - Disaster Recovery backups (different cloud account + location).
- *Detective:*
  - Alerting that an attack has/had happened.
  - Log Aggregation, Metrics, Intrusion Detection Systems,
    Vulnerability Scanning ([[https://www.aquasec.com/cloud-native-academy/vulnerability-management/vulnerability-scanner/][Aqua]]).
- *Responsive:*
  - Respond during/after an attack - Comms.
  - What is the scope of the Breach?
  - Follow Legal/Regulatory notification steps based off data type
    (eg. PII = [[https://gdpr-info.eu/art-33-gdpr/][72 hours for GDPR]]).
- *Recover:*
  - Post-incident restore (automated rebuilds) + implement fixes.
  - Remove access

*** Patching:
- Do you know the state of security patches for each level in your
  stack?
- How are you notified of Security Vulnerabilities?
- How are you notified of Security Patches?
- When do you implement Security Patches? And is it automated?

The Stack of a MicroService:
- *MicroService.*
- *Container OS.*
- Container.
- Kubernetes Subsystems.
- VM OS.
- VM.
- Hypervisor.
- OS.
- Hardware

*Bold* = Your responsibility if on a managed kubernetes Cloud
provider. ie. Need to know the state of:

- Application dependencies.
  - Tooling (Scanning): [[https://snyk.io/][Snyk]], [[https://docs.github.com/en/code-security/code-scanning][Github Docs: code-scanning]].
  - Tooling (updates): [[https://github.com/renovatebot/renovate][Github: renovatebot/renovate]]. [[https://github.com/dependabot][Github: dependabot]].
- Application language(s).
- Container base image.
  - Tooling: [[https://www.aquasec.com/cloud-native-academy/vulnerability-management/vulnerability-scanner/][Aqua]].
- Container dependencies.

*** Prevent debugging Production code.
- Prevent malicious actors attaching a debugger on Production instances to
  decompile the code.
- API's/Code that actively rejects Debuggers, crashes code, different code
  paths (obfuscation), reports home. ie. fail securely.

*** Secrets:
Secrets range from: TLS Certificates, SSH Keys, Public/Private API
keypairs, DB Credentials...

- *Creation:*
  - How do we create the Secret?
  - Make sure many hyper-limited scoped secrets to avoid broad
    failures from rotation. eg. Per-instance credentials.
- *Distribution:* How do we get the Secret to /only/ the right place?
- *Storage:* Can only authorised parties access the Secret?
- *Monitoring:* How is the Secret being used?
- *Rotation:*
  - Can the Secret be changed without causing problems?
  - Aim for small window eg. <1hour. - Requires tooling!
- *Revocation:*
  - How to revoke an exposed Secret before rotation?
  - What tooling are you using to scan for Secrets?
    - [[https://github.com/awslabs/git-secrets][Github: awslabs/git-secrets]] - prevent committing secrets.
    - [[https://github.com/gitleaks/gitleaks][Github: gitleaks/gitleaks]] - prevent committing secrets + detection.

What tooling is being used to manage the Secret?
- Kubernetes built-in secrets solution.
- [[https://www.hashicorp.com/products/vault][Hashicorp: Vault]] - supports [[https://developer.hashicorp.com/consul/tutorials/developer-configuration/consul-template][Hashicorp Docs: consul-template]] for
  dynamic updates of Service config with secrets.
- [[https://docs.aws.amazon.com/secretsmanager/latest/userguide/intro.html][AWS: Secrets Manager]], [[https://azure.microsoft.com/en-us/products/key-vault/][Azure: Key Vault]], [[https://cloud.google.com/security/products/secret-manager][Google Cloud: Secrets Manager]].

** Security Bodies/Sites:
- [[https://www.first.org/cvss/][CVSS (Common Vulnerability Scoring System)]] - Used in the scoring of PEN Tests.
- [[https://www.cve.org/][CVE (CyberSecurity Vulnerabilities)]] ([[https://cve.mitre.org/index.html][Old CVE site (Should be dead in
  2023)]]). - collection of all security vulnerabilities.
- [[https://owasp.org/][OWASP (Open Web Application Security Project)]] - Nonprofit looking to improve
  security through Open-Source projects.
- https://infosec.mozilla.org/guidelines/web_security
** Terminology:
*** Defence in Depth:
- Multiple-levels of Protections.
  - eg. Unique credentials, Encryption at Rest & in Transit, different
    network segments / physical locations, different languages (avoid
    common 0-day bugs)
- Breaking functionality into distributed Services ie. MicroServices.
*** Least Privilege:
- Grant minimum access to carry out required function and only for
  time period needed.
- Reduce scope of attack if credential is leaked!
- eg. Per-Instance, time-limited, R/O DB credentials.
*** Implicit Trust:
- Trust all calls made inside Our network perimeter.
- Trust is tested at perimeter.
- High Risk if breached!
  - Is this an explicit choice or unaware of risk?
*** Zero Trust:
- *Mindset:* Assume compromised environment!
  - all interactions are with malicious attackers or compromised
    systems.
    - What is the process to gain Trust?
  - all data written will be read by an attacker.
    - How are you securing data in-transit & at rest?
- No concept of a /"perimeter"/!
  - /potentially/ could make all Services public with enough zero
    trust protections (Extreme end of the scale).
- Requires a culture/mindset change of Company to build a zero trust
  System.

*** Vertical Separation Flaw:
- Access resources granted to more privileged accounts.
- eg. gaining Administrator privileges.
*** Horizontal Separation Flaw:
- Access to resources granted to a similarly configured account.
- eg. modifying data belonging to a different User of the same Application.
** Security Articles:
- https://www.cve.org/ -  Identify, define, and catalog publicly disclosed
  cybersecurity vulnerabilities.
- https://cwe.mitre.org/top25/archive/2022/2022_cwe_top25.html
- https://owasp.org/ - The Open Web Application Security Project® (OWASP) is a
  nonprofit foundation that works to improve the security of software.
- https://owasp.org/www-project-top-ten/
- https://signal.org/blog/building-faster-oram/
- https://arstechnica.com/?p=1872326 - 10 malicious Python packages exposed in
  latest repository attack.
- https://www.synopsys.com/blogs/software-security/sast-vs-dast-difference/ -
  Static (White box) vs Dynamic (Black box) Application Security Testing.
** Tools:
- https://www.rapid7.com/products/insightappsec/ - InsightAppSec performs
  black-box security testing to automate identification, triage
  vulnerabilities, prioritize actions, and remediate application risk.
- https://www.rapid7.com/products/insightvm/ - Discover risks across all your
  endpoints, cloud, and virtualized infrastructure.
- https://www.keycloak.org/ - Open Source Identity and Access Management Add
  authentication to applications and secure services with minimum effort.  No
  need to deal with storing users or authenticating users.  Keycloak provides
  user federation, strong authentication, user management, fine-grained
  authorization, and more.
- [[https://docs.snyk.io/scm-ide-and-ci-cd-integrations/snyk-ci-cd-integrations/azure-pipelines-integration][Snyk: Azure Pipelines integration using the Snyk Security Scan task]].
*** [[https://www.knowbe4.com/][Knowbe4]]:
General Security training delivered as mandatory in work. Usually on a cadence
of quarterly/monthly, via: games, quizzes, videos. Some of it's pretty dry, but
things are improving, especially /"The Inside Man"/ series:

- [[https://insideman.knowbe4.com/][Knowbe4: The Inside Man]].
- [[https://www.twistandshout.co.uk/][TwistAndShout]] - Production company of /"The Inside Man"/ and other security
  videos.
*** WireGuard: fast, modern, secure VPN tunnel
[2022-09-11 Sun 15:47]
https://www.wireguard.com/

WireGuard® is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances. Initially released for the Linux kernel, it is now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. It is currently under heavy development, but already it might be regarded as the most secure, easiest to use, and simplest VPN solution in the industry.
* System Design:
** Estimation:
- *QPS:* Queries Per Second.
  - Peak QPS ~= 2 * QPS.
- Storage (eg. Lifetime)(eg. =X <unit> * <time> * <users>=).
- Cache.
- Number of Servers.
- *DAU:* Daily Activated Users.
** Patterns
*** [[https://www.tim-wellhausen.de/papers/ExpandAndContract/ExpandAndContract.html][Expand & Contract Pattern]]:
Zero-downtime pattern to roll out a change in multiple, safe steps..

1. Introduce new structure (eg. New DB schema + write to both).
2. Migrate Data (ie. backfill).
3. Operate on New Structure.
4. Stop writing in Old Structure.
5. Delete Old Structure.
*** [[https://martinfowler.com/bliki/StranglerFigApplication.html][StranglerFig Pattern]]:
A pattern to choke functionality away from an existing codebase in an iterative
manner. Since it is a pass-through proxy, functionality can be added in small
chunks. Rollback should be runtime config to toggle routing back to old
application.

- Wrap existing application with a passthrough Proxy.
- Siphon/divert functionality into Proxy wrapper.
- Re-use same underlying DB.
- /Can/ cleanup be deleting code out of old application.
- /Eventually/ deprecate old application once all functionality has been
  replaced.
  - Can remain as a symbiotic relationship if some parts don't need to be
    extracted from old application.
** Principals:
*** [[https://en.wikipedia.org/wiki/Don%27t_repeat_yourself][DRY]]:
Don't Repeat Yourself.
*** [[https://en.wikipedia.org/wiki/SOLID][SOLID]]:
- *S*ingle-responsibility principle: eg. classes should have a single
  responsibility.
- *O*pen-closed principle: Open for extension, but closed for modification.
- *L*iskov substitution principle: Functions that use pointers or references to
  base classes must be able to use objects of derived classes without knowing
  it.
- *I*nterface segregation principle: Don't force Clients to depend on unused
  interfaces.
- *D*ependency inversion principle: Depend upon abstractions, not concretions.
** Ask Why a System Works?:
- learn how popular applications work at a high-level.
- Start to understand why some component is used instead of another.
- Build serious side projects. Start simple and iterate to improve & refine it.
- Build a system from scratch and get familiar with all the processes and details of its construction.
- Focus less on mechanics and more on trade-offs.
- Focus on the high-level. Low-level will crop up.
** Breakdown strategies:
Expectations > Requirements > Estimates > Solution  > Overview > Deep
Dive > Refine.

- Ask refining questions.
  - *Functional:* Requirements the Client needs directly. eg. Send messages in near real-time to contacts.
  - *Non-functional:* indirect requirements. eg. Performance shouldn't degrade with load.
  - Clarify assumptions.
  - Honesty when ignorant.
- Handle the data.
  - Data size now?
  - Data growth rate?
  - How data is consumed by User or othe SubSystems?
  - Read / Write heavy?
  - Strict or Eventual Consistency?
  - what's the durability target of the data?
  - Privacy/Regulatory concerns for storing/transferring User data?
- Discuss the components.
  - Highlight reasoning.
  - Talk around conflicts with examples of pain/components needed to work the other options.
  - High-level API design for User clarity.
- Discuss trade-offs.
  - Pros/Cons.
  - Monetary/Technical complexity (aim for Resource efficiency).
  - Plan for this designs weakness.
  - Highlight and explain weaknesses. eg. Design won't scale, but added monitoring, to reduce cost and time to do a new design.
  - Add fault tolerance and security to the design.
** Abstractions:
- Network: Use RPCs (Remote Procedure Calls) to abstract away network communications and make all calls appear to be local.
-
** Microservices:
*** [[https://world.hey.com/dhh/how-to-recover-from-microservices-ce3803cc][How to Recover from Microservices]].
Reaction blog to: [[https://www.primevideotech.com/video-streaming/scaling-up-the-prime-video-audio-video-monitoring-service-and-reducing-costs-by-90][Prime Video Tech: Scaling up the Prime Video audio/video
monitoring service and reducing costs by 90%]].

- Successful large, complex systems must evolve from successful small, simple
  systems.
- Extraction Candidates: Clearly modularized design with strong boundaries and
  no critical-flow dependencies.
  - Performance benefits from switching implementation?
  - Business benefits from a silo'd team, separate to the System?
- Focus on Microservice problems and view that Monolith's are generally
  better + how to return to a Monolith:
  - *Stop Digging:* Stop creating new services + add functionality to a Core
    service, to move towards fat services.
  - *Consolidate critical, dependent paths first:* Avoid breaking a coherent
    Flow across multiple Services. Causes change co-ordination + sync issues.
  - *Leave isolated performance hotspots for last:* Ideal is to have
    narrow/isolated performance critical segment in it's own System. Rewrite in
    a performant language.
  - *Prioritize dropping the most esoteric implementations:* Prune down to 2
    backend languages (General Purpose = 99%, High-Performance (hotspots) = 1%).
  - *Partition Large Systems on Modules not Networks:* Start simple monolith,
    then break on domain boundaries, as required.
** Components in Detail:
*** Addresses (postal):
- [[https://www.mjt.me.uk/posts/falsehoods-programmers-believe-about-addresses/][Falsehoods Programmers believe about Addresses]].
*** API's:
**** [[https://www.openapis.org/][OpenAPI]]:
- [[https://learn.openapis.org/][Getting Started | OpenAPI Documentation]].
- [[https://openapi.tools/#documentation][OpenAPI Tools]].
*** Data Pipelines:
Data Pipelines are used to [[*Normalisation:][normalise]] raw(/"messy/", not directly usable)
source(s) of data (eg. API's, No/SQL DBs, Files) into a /"structured/" target
DB schema for later [[*EDA (Exploratory Data Analysis):][EDA (Exploratory Data Analysis)]]/Processing/Storage (eg. [[*ML (Machine Learning):][ML
(Machine Learning)]], Data Lakes/BI (Business Intelligence) Dashboards).

Data Pipelines can do the processing either by:

- *Batch:*
  - Load data in /"batches/" (scheduled/off-peak).
  - /Usually/ optimal if there is no immediate need for the data.
  - Typically closely tied to an [[*ETL (Extract, Transform, Load):][ETL]] data integration process.
- *Streaming:*
  - Requirement for /Real-Time/ data.
    - Low-latency to [[*Data Repository:][Data Repository]] due to processing shortly after occurring.
  - [[*Event:][Events]] are transported by a [[*Message Broker:][Message Broker]] or Messaging System ([[*Queues:][Queue]]).
  - Less Reliable - Messages /may/ be dropped/lost or stuck in a queue.
    - Reduced by Message Broker acknowledgements by Consumer to remove from the
      queue.
  - Tooling: [[http://kafka.apache.org/][Kafka]].

The Core of a Data Pipeline is:

- [[*Data Ingestion:][Data Ingestion]].
- Data Transformation (see: [[*Normalisation:][Normalisation]]).
- Data Storage (See: [[*Data Repository:][Data Repository]]).

**** Glossary:
***** Data Ingestion:
The process of reading in raw data from Un/Structured Data sources.

*Suggestion:* Store the raw data (eg. Cloud Data Warehouse) before processing.
Allows re-processing in the future.

Streaming name convention: Producers/Publishers/Senders.
***** Data Repository:
The Target DB that the Data Pipeline writes into. Often called a /"Data
Warehouse/" or /"Data Lake/".

Streaming name convention is: Consumers/Subscribers/Recipients.
***** Data Visualisation:
Visually display the Data (Charts/graphics/animations/etc), to communicate
complex data relationships and data-driven insights into an understandable
form. See: [[https://www.ibm.com/topics/data-visualization][IBM: Data Visualisation]].
***** EDA (Exploratory Data Analysis):
EDA is used be Data Scientists to analyse/investigate data sets and summarise
their main characteristics. Often using [[*Data Visualisation:][Data Visualisation]] to discover
patterns/anomalies, test hypothesis/assumptions. See: [[https://www.ibm.com/topics/exploratory-data-analysis][IBM: Exploratory Data
Analysis]].
***** ETL (Extract, Transform, Load):
When batch processing Data into the target DB Schema, you would often write an
ETL integration process to normalise the data.

An ETL Pipeline is a sub-category of a Data Pipeline, because:

- ETL is strictly Extract/Transform/Load(store in a [[*Data Repository:][Data Repository]]), but a
  Data Pipeline may not follow this sequence.
- ETL is typically Batch Processing-only.
***** Event:
Data that describes a single /"action/". eg. A sale at a checkout.
***** ML (Machine Learning):
Machine learning is a branch of artificial intelligence (AI) and computer
science which focuses on the use of data and algorithms to imitate the way that
humans learn, gradually improving its accuracy. Through the use of statistical
methods, algorithms are trained to make classifications or predictions,
uncovering key insights within data mining projects.
***** Normalisation:
The process of converting/serialising messy (/"noisy/") source data to the
structured target DB Schema. The types of data transformation steps that may be
done are:

- Filtering.
- Masking.
- Aggregation/Merging.
- Summarising.

The above steps are usually chained together as a pipeline of
steps. eg. =Ingestion > filtering to X columns > aggregation > ...=, that
eventually write into the [[*Data Repository:][Data Repository]].
***** Stream:
Same as [[*Topic:][Topic]].
***** Topic:
A grouping of Related [[*Event:][Events]]. eg. Adding an item to a checkout.
**** Links:
- [[https://www.ibm.com/topics/data-pipeline][IBM: Data Pipelines]].
- [[https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html][AWS Docs: What is AWS Data Pipeline?]]
*** Distributed Networking:
**** [[https://www.envoyproxy.io/][Envoy Proxy]] ([[https://www.envoyproxy.io/docs/envoy/latest/][Envoy docs]]):
#+BEGIN_QUOTE
The network should be transparent to applications. When network and application
problems do occur it should be easy to determine the source of the problem.
#+END_QUOTE

As on the ground microservice practitioners quickly realize, the majority of
operational problems that arise when moving to a distributed architecture are
ultimately grounded in two areas: networking and observability. It is simply an
orders of magnitude larger problem to network and debug a set of intertwined
distributed services versus a single monolithic application.

Originally built at Lyft, Envoy is a high performance C++ distributed proxy
designed for single services and applications, as well as a communication bus
and “universal data plane” designed for large microservice “service mesh”
architectures. Built on the learnings of solutions such as NGINX, HAProxy,
hardware load balancers, and cloud load balancers, Envoy runs alongside every
application and abstracts the network by providing common features in a
platform-agnostic manner. When all service traffic in an infrastructure flows
via an Envoy mesh, it becomes easy to visualize problem areas via consistent
observability, tune overall performance, and add substrate features in a single
place.

- Out of Process Architecture - Self-contained, low memory footprint server
  that runs alongside Application.
- HTTP/2, (HTTP/3 in alpha ~1.19.0~) gRPC support.
  - Transparent HTTP/1.1 / HTTP/2 proxy.
- Load Balancing - retries, circuit breaking, global rate limiting, request
  shadowing, zone local load balancing, etc.
- Configuration management API's.
- Service Discovery - eg. via DNS resolution.
- Front/Edge Proxy Support.
- Observability - L7 traffic, distributed tracing, wire-level observations of
  MongoDB, DynamoDB, Redis, Postgres.
- Health Checking - Assume Eventual Consistency.
- YAML config files.

See: [[https://www.envoyproxy.io/docs/envoy/latest/intro/life_of_a_request][Envoy (Docs): Life of a Request]].
*** Queues:
**** Glossary:
***** Message Broker:
- Direct communication between explicit Services (one-to-one).
- Responsibilities: Validate, Route, Store, Deliver messages to designated
  recipient.
- Intermediary between Services/Applications.
  - ie. *Decouple knowledge of Receivers/Consumers location from Sender*.
  - May have: 0, 1, Many Consumers (unknown to Sender).
***** Publish/Subscribe:
- Message distribution pattern.
- Broadcast-style distribution (one-to-many).
**** [[http://kafka.apache.org/][Kafka]]:
Apache Kafka is an open-source distributed event *streaming* platform used by
thousands of companies for high-performance data pipelines, streaming
analytics, data integration, and mission-critical applications.
- 2011.
- Java/Scala-based.
  - SDK for adding custom support of other languages.
- Streams.
  - Ideal for *A* to *B* streaming for max throughput and simple routing.
  - Ideal for:
    - Event Sourcing.
    - Stream Processing.
    - Modelling Changes to a System as a Sequence of Events.
    - Processing Data in multi-stage pipelines.
    - Routine System auditing.
    - Storing messages permanently.
  - *Framework for storing, reading, re-reading and analysing streaming data.*
- Throughput.
- Uses Pub/Sub pattern.
- Uses a Message Log, instead of a Message Queue.
  - *Pull-based*
  - Consumer must request to get batches of messages (offsets).
    - PRO: network efficiency.
    - CON: High-latency.
- *Use Data Lake analysis tools to efficiently store, manage, analyse the Kafka
  streams.*
***** Breakdown:
See: [[https://www.simplilearn.com/kafka-vs-rabbitmq-article][SimpliLearn: Kafka VS RabitMQ]].

- *Performance:* 1 million messages per second.
- *Message Retention:* Policy-based.
- *Data Type:* Operational.
- *Consumer Mode:* Dumb Broker / Smart Consumer.
- *Topology:* Pub/Sub.
- *Payload Size:* Default 1MB limit.
- *Usage Cases:* Massive data / High throughput cases.
**** MSSQL [[https://learn.microsoft.com/en-us/sql/database-engine/service-broker/building-applications-with-service-broker?view=sql-server-ver16][Service Broker]]:
Applies to: ￼ SQL Server (all supported versions) ￼ Azure SQL Managed Instance

Any program that can run *Transact-SQL statements* can use Service Broker. A
Service Broker application can be implemented as a program running outside of
SQL Server, or as a stored procedure written in Transact-SQL or a .NET
language.

A program that uses Service Broker is typically composed of a number of
components working together to accomplish a task. A program that initiates a
conversation creates and sends a message to another service. That program may
wait for a response, or exit immediately and rely on another program to process
the response. For a service that is the target of a conversation, the program
receives an incoming message from the queue for the service, reads the message
data, does any necessary processing, and then creates and sends a response
message if appropriate.

Service Broker extends Transact-SQL. An application does not need a special
object model or library to work with Service Broker. Instead, programs send
Transact-SQL commands to SQL Server and process the results of those
commands. An application can be activated by Service Broker, can run as a
background service, can run as a scheduled job, or can be started in response
to an event.
***** Uses:
- [[https://learn.microsoft.com/en-us/sql/database-engine/service-broker/messages?view=sql-server-ver16][Messages]].
- [[https://learn.microsoft.com/en-us/sql/database-engine/service-broker/queues?view=sql-server-ver16][Queues]].
***** Why?:
- Consolidation if already in Windows (MSSQL) Eco-system.
  - Service Broker is a part of the MSSQL deployment.
  - Messages are R/W from the same DB that Application(s) uses.
  - Offload message queuing outside of the Application to the Platform (DB).
**** Python Module Queues:
These are low-level queues that can be used within Python Applications, where
the same module is used on both sides of the queue:

- [[https://docs.python.org/3/library/asyncio-queue.html][~asyncio.queue~]] - Async. Not thread-safe, but designed for ~async~ / ~await~
  code.
- [[https://docs.python.org/3/library/queue.html#module-queue][~queue~ (built-in)]] - Synchronous. Thread-safe. Multi-Producer /
  Multi-Consumer queues.
- [[https://www.tornadoweb.org/en/stable/queues.html?highlight=queue][~tornado.queues~]] - Async. Queues for Tornado coroutines, like:
  [[https://docs.python.org/3/library/asyncio-queue.html][~asyncio.queue~]].
**** [[https://www.rabbitmq.com][RabbitMQ]]:
- Distributed Message Broker
  - Deploy a Cluster of Nodes = HA.
- *Push-based*
  - Consumer prefetch limits.
  - Low-latency messaging.
  - Ideal for:
    - Complex (non-trivial) routing to multiple Consumers/Applications.
    - High-throughput & reliable background jobs.
    - Rapid request-response.
    - Load balance across Worker nodes (20k+ messages/second).
    - Long running tasks.
    - Communication/Integration between and within Applications.
- Implements AMQP natively (and AMQP (future versions), HTTP, STOMP, MQTT via
  plugins).
- Large official support for popular languages + plugins.
***** Breakdown:

See: [[https://www.simplilearn.com/kafka-vs-rabbitmq-article][SimpliLearn: Kafka VS RabitMQ]].

- *Performance:* 4k-10k messages per second.
- *Message Retention:* Acknowledgement-based.
- *Data Type:* Transactional.
- *Consumer Mode:* Smart Broker / Dumb Consumer.
- *Topology:* Exchange Type: Direct, Fan out, Topic, Header-based.
- *Payload Size:* No constraints.
- *Usage Cases:* Simple use cases.
**** [[https://zeromq.org][ZeroMQ]]:
ZeroMQ (also known as ØMQ, 0MQ, or zmq) looks like an embeddable networking
library but acts like a concurrency framework. It gives you sockets that carry
atomic messages across various transports like in-process, inter-process, TCP,
and multicast. You can connect sockets N-to-N with patterns like fan-out,
pub-sub, task distribution, and request-reply. It's fast enough to be the
fabric for clustered products. Its asynchronous I/O model gives you scalable
multicore applications, built as asynchronous message-processing tasks. It has
a score of language APIs and runs on most operating systems.
** Design Tools:
*** [[https://plantuml.com][PlantUML:]]
- Renders UML into multiple formats (ASCII, PNG, SVG, PDF).
- Supports designing multiple Design diagrams (Sequence, Flow, Block, Class,
  User, Gantt, etc...).
- Java-based, supplied as a `jar`.
- Integrates with everything. See: [[https://plantuml.com/running][PlantUML: Running (Integrations)]].
*** [[https://mermaid.js.org][Mermaid]]:
JavaScript based diagramming and charting tool that renders Markdown-inspired
text definitions to create and modify diagrams dynamically.

- [[https://github.com/mermaid-js/mermaid][Github: mermaid-js/mermaid]].
- [[https://github.blog/2022-02-14-include-diagrams-markdown-files-mermaid/][Github Blog: Include diagrams in your Markdown files with Mermaid]].
** [[https://github.com/donnemartin/system-design-primer][Github: donnemartin/system-design-primer]]:

#+BEGIN_SRC shell :dir "~/github_repos/"
  git clone git@github.com:donnemartin/system-design-primer.git
#+END_SRC

NOTE: I generated the epub by modifying the script to call pandoc via
docker and to do it based off the tempfile as input, instead of
~cat~-ing it over stdin.

* Task Queuing:
** [[https://kafka.apache.org/intro][Apache Kafka]]:
- Topic message broker (server) for streaming/storing Events.
- Client connectors.
- Producer/Consumer API's.
** [[https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/welcome.html][AWS MQ]]:
- Managed Message Broker Service.
  - [[https://www.amqp.org/][AMPQ]] (Advanced Message Queuing Protocol).
  - [[https://mqtt.org/getting-started/][MQTT]] (Message Queuing Telemetry Transport).
- Supported Engines:
  - [[http://activemq.apache.org/][Apache ActiveMQ]].
  - [[https://www.rabbitmq.com/][RabbitMQ]].
- Resource Type: Async/Synch, Queues, Pub-Sub, Message Broker.
** [[https://docs.aws.amazon.com/sns/latest/dg/welcome.html][AWS SNS]]:
- Pub-Sub Service.
  - Message Topics.
  - Can chain AWS SQS as a subscriber (along with
    notification/email/AWS Lambda/HTTPS/Application Services).
- AWS SNS is a Real-Time Message Router. ie. Does not store messages
  for later retrieval!
- Resource Type: Synch, Pub-Sub.
** [[https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/welcome.html][AWS SQS]]:
- Message Queue.
  - API / SDK to access.
- Message is processed by a single Subscriber/Worker.
- Resource Type: Async, Queues.
** [[https://www.rabbitmq.com/][RabbitMQ]]:
- OpenSource Message Broker.
  - [[https://www.amqp.org/][AMPQ]] (Advanced Message Queuing Protocol).
  - [[https://mqtt.org/getting-started/][MQTT]] (Message Queuing Telemetry Transport).
  - [[https://stomp.github.io/][STOMP]] (Simple Text Oriented Messaging Protocol).
- Cloud/Self-hosting.

* Testing Philosophy:
** TDD:
TDD (Test Driven Development), is the practice of defining (and implementing)
tests before writing Production code. Links:

- [[http://butunclebob.com/ArticleS.UncleBob.TheThreeRulesOfTdd][Uncle Bob: The Three Rules of TDD]].
- [[https://www.codecademy.com/article/tdd-red-green-refactor][Codecademy: TDD Red/Green/Refactor]].
- [[https://www.jamesshore.com/v2/blog/2005/red-green-refactor][James Shore: Red/Green/Refactor]].
*** [[elisp:(elfeed-tube-fetch "https://www.youtube.com/watch?v=EZ05e7EMOLM")][TDD, Where Did It All Go Wrong (Ian Cooper)]].
- **Test behaviours, not implementation!**
  - Refactor/Re-implementation **must not** require code changes!!
  - Implementation tests slow changes due to keeping sync.
- **Test contract boundaries, not classes!**
  - eg. modules/API's/ports.
  - Avoid mocks!
  - Speed at all cost!
- **Don't test internal functions!**
- **Can write exploratory implementation tests, but always delete afterwards!!**
  - Controversial, but it makes sense. I believe the implementation
    knowledge/reasoning should then be written into good Production-side
    implementation docs.
- [[*Red, Green, Refactor:][Red, Green, Refactor]].
- [[*TDD Books:][Books]].
*** Red, Green, Refactor:
Baby steps to always releasable code from constantly cycling through design,
hypothesis & validation.
- Think:
  - Invest time.
  - What test will move code towards completion.
- Red:
  - ~30secs.
  - Write simplest behaviour tests.
- Green:
  - ~30secs.
  - Duct-tape/dirty code to get test passing quickly.
  - Copy from StackOverflow.
  - no structure.
- Refactor:
  - Invest time.
  - Don't write new tests!
  - Remove duplication!
  - Apply pattern(s).
  - Remove code smells.
  - Clean up.
- Repeat:
  - ~20-40 cycles per hour.
  - Expect cycle time to ebb & flow.
*** TDD Books:
- [[https://www.amazon.co.uk/Refactoring-Improving-Existing-Addison-Wesley-Technology-ebook/dp/B007WTFWJ6/ref=sr_1_4][Refactoring: Improving the Design of Existing Code (1st Edition)(Martin Fowler)]].
- [[https://www.amazon.co.uk/Refactoring-Improving-Existing-Addison-Wesley-Technology/dp/0134757599/ref=sr_1_1][Refactoring: Improving the Design of Existing Code (2nd Edition)(Martin Fowler)]].
- [[https://www.amazon.co.uk/Test-Driven-Development-Addison-Wesley-Signature/dp/0321146530/ref=sr_1_1][Test Driven Development: By Example (Kent Beck)]].
* Training Sites:
- https://www.educative.io/ - Text-based teaching resources (instead of video)
  and web-based coding environments.`
- https://scratch.mit.edu/ - Programming for kids.
* vim/neovim:
** keyboard shortcuts:
- ~:e </path/to/file>~
** config:
- Windows: ~%localappdata%\nvim\init.lua~.
- Package Management: [[https://lazy.folke.io/][Lazy.nvim]].
  - Add plugins into: ~/path/to/nvim/init.lua~.
  - Restart nvim to auto install.
** packages:
Examples from other users:
- [[https://dev.to/slydragonn/ultimate-neovim-setup-guide-lazynvim-plugin-manager-23b7][Dev.to: Slydragon - Neovim plugins via Lazy]].
*** [[https://github.com/NeogitOrg/neogit][NeoGit]]:
- ~:Neogit [cwd=/path/to/repo/root].
- Same keyboard shortcuts as Emacs [[https://magit.vc/][magit]]!
*** [[https://nvim-orgmode.github.io/][OrgMode]]:
- ~:Org <options>~
- See: Treesitter install notes.
*** TreeSitter:
- Windows requires a Compiler installed: ~winget install zig.zig~, or: ~winget install LLVM.LLVM~ or ~gcc~.

* Workflows:
** Bullet Journaling:
- Spread: 2 pages.
- Index: link to pages and allow threading.
- Future log: 6months high level. 1 spread.
  - Use threading to link multiple books.
  - Year view of Birthdays?
- Monthly Log: next month. Schedule tasks/events from Future Log.
- Daily: do tasks from Monthly Log. Concise.
- Collection: spread+ of collocated tasks/notes/trackers.
  - People Collection for notes on real people (dis/likes, abilities,
    wants, etc).
- Tracker: habit table. X completed days.
- Review:
  - Daily.
  - Backport to Future Log (copy into next journal).
  - groom daily from Future/monthly to daily.
  - concise collection domains.
  - What is missing?
  - Is everything done?
  - Any migrations?
    - Reschedule?
    - Drop?
    - Move to Tracker?
- Reflect:
  - Monthly/Weekly.
  - What have I learnt?
  - What should change?
  - Am I using the information that I am logging?
** Rule of 3:
A task tracking style that focuses on 3 Must do, results-based, priorities that are tracked across a Daily+ period.

- 3 outcomes for: Today/Week/Month/Year.
    - Results orientated.
        - Good: Secure new Client.
        - Bad: Prepare proposal.
    - /"Priorities"/ not /"Items"/.
- Work on 3 actions at most Productive Time.
    - Block out calendar.
    - ignore Distractions (secondary tasks).
    - Automate secondary tasks.
- Must > Should > Could.
- Do it, Queue it, Schedule it, Delegate it.
- Micro Tasks to fill spare time.
- Start day with a Plan.
    - Plan, Do, Review.
** 5, 4, 3, 2, 1:
Focusing goals/aspirations with a breakdown in the form of:

- 5 years
- 4 quarters (1 year)
- 3 months
- 2 weeks
- 1 day

See: [[https://bulletjournal.com/blogs/bulletjournalist/my-end-of-year-process][My End of Year Process - Bullet Journal]]
