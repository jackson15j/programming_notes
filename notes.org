#+title: Programming/Development Notes
#+author: Craig Astill
#+OPTIONS: toc:2
* Arch:
** Package Management:
*** [[https://forum.manjaro.org/t/how-do-i-limit-pamac-or-yays-cpu-core-usage-while-compiling/55043][How do I limit Pamac or yay's CPU core usage while compiling?]]
- Edit ~/etc/makepkg.conf~.
- Modify: ~MAKEFLAGS="-j$(($(nproc)+1))"~ to another value.
  - eg. ~MAKEFLAGS="-j$(($(nproc)-2))"~, to leave 2 cores free.
*** [[https://www.reddit.com/r/archlinux/comments/woh8fr/list_packages_installed_by_yay/][List packages installed by yay?]]
- List packages: ~<yay|pacman> -Qm~.
- List Dependencies for a package: ~yay -Rcps <package>~.
- Remove package + dependencies: ~yay -Rcs <package>~.
*** How to view `.heic` images from an iphone on arch?
Install `libheif` and then set to use an image application by default
(eg. viewnior).
#+BEGIN_SRC shell :results silent
  yay -S libheif
#+END_SRC
** Power Management:
*** Broken Hibernate:
Swap keeps being set to the main partition, instead of the swap partition.
Manually updated the ~resume=~ in: ~/etc/default/grub~ to be correct, then
regenerated Grub with: ~sudo grub-mkconfig -o /boot/grub/grub.cfg.~ See:
https://bbs.archlinux.org/viewtopic.php?pid=1928690#p1928690.
* BackEnd:
** Go:
- [[https://go.dev/][go dev site]].
- [[https://github.com/golang/tools][Github: golang/tools]] - Repo of all =golang/x/tools= tools & static checkers.
- [[https://www.jetbrains.com/go/][Jetbrains: GoLand]] editor.
*** Go Tutorials:
- https://gobyexample.com/ - Minimal site with lots of good examples.
- [[https://go.dev/tour/welcome/3][Go: A Tour of Go]] - This will place a tour binary in your GOPATH's bin
  directory (=~/go/bin/tour=). When you run the tour program, it will open a
  web browser displaying your local version of the tour.
  #+BEGIN_SRC shell :results quiet
    go install golang.org/x/website/tour@latest
  #+END_SRC

*** Temporarily rewrite mod to use an unpublished package.
From the [[https://go.dev/doc/tutorial/call-module-code][Go Tutorial: Call Module Code]], you can set the ~go.mod~ to search
locally for an unpublished package instead of looking up from the
namespace. ie. no Code changes between local/remote packages.

#+BEGIN_EXAMPLE shell
  go mod edit -replace <namespace/of/package>=<../local/path/to/package>
  go mod tidy
#+END_EXAMPLE
* Bash:
- ~ps -aef --forest~ for tree view of processes.
** Curl:
- [[https://tldr.ostera.io/curl][Curl examples - tldr]] | simplified, community driven man pages. Transfers data
  from or to a server. Supports most protocols, including HTTP, FTP, and
  POP3. More information: https://curl.se.
- [[http://cht.sh/curl][cheat.sh/curl]].
** [[https://surf.suckless.org/][surf | suckless.org]] software that sucks less
surf is a simple web browser based on WebKit2/GTK+. It is able to display
websites and follow links. It supports the XEmbed protocol which makes it
possible to embed it in another application. Furthermore, one can point surf to
another URI by setting its XProperties.
** RDP:
- =xfreerdp= example connection string: ~xfreerdp /w:1920 /h:1080 /u:<username>
  /v:<hostname>~
* BuildTools:
- [[https://www.gnu.org/software/make/][GNU Make: Makefiles with build targets]].
- [[https://taskfile.dev/#/][Task: task runner / build tool that aims to be simpler than GNU Make]].
- [[https://nx.dev/][NX: Build system for FrontEnd code, with CI/Tasks/Caching]].
* CI/CD:
** Azure DevOps:
- [[https://marketplace.visualstudio.com/items?itemName=tingle-software.dependabot][Dependabot - Visual Studio Marketplace]].
- [[https://oshamrai.wordpress.com/2019/12/27/automated-creation-of-git-pull-requests-through-azure-devops-build-pipelines/][Automated creation of GIT Pull Requests through Azure DevOps Build pipelines]].
** Gitlab:
- [[https://docs.gitlab.com/ee/user/project/quick_actions.html][Gitlab Docs: Quick Actions]] - slash commands in Gitlab.
- [[https://docs.gitlab.com/ee/user/markdown.html#gitlab-specific-references][Gitlab Docs: Special markdown syntax]] - Markdown syntax to do actions from
  commit/comments in Gitlab.
- [[https://docs.gitlab.com/ee/administration/integration/plantuml.html][Gitlab Docs: PlantUML integration]] - Configure a docker container to generate
  in-line PlantUML code blocks into images when rendering Markdown/Restructred
  Text.
- [[https://gitlab.com/gitlab-org/gitlab/-/tree/master/.gitlab/merge_request_templates][Gitlab: ~.gitlab/merge_request_templates/~]] - Gitlab's current [[https://docs.gitlab.com/ee/user/project/description_templates.html][Gitlab Docs: MR
  templates]].
- https://docs.gitlab.com/ee/user/project/releases/release_cicd_examples.html
  - Release stage for an Agent to explicitly tag the repo and handle generating
    tagged artifacts in a release job.
    - https://docs.gitlab.com/ee/ci/yaml/#release.
  - This is different to using a tag trigger and having a job that does work
    when a tag has been pushed.
    - https://docs.gitlab.com/ee/ci/yaml/#rules.
** Gitlab Articles:
- https://about.gitlab.com/blog/2022/09/06/speed-up-your-monorepo-workflow-in-git/
- https://about.gitlab.com/blog/2022/08/31/the-changing-roles-in-devsecops/ - Why and How DevOps roles are changing.
- https://about.gitlab.com/blog/2022/08/30/the-ultimate-guide-to-software-supply-chain-security/
- https://about.gitlab.com/blog/2022/08/30/top-reasons-for-software-release-delays/
- https://about.gitlab.com/blog/2022/07/21/quickly-onboarding-engineers-successfully/
- https://about.gitlab.com/blog/2022/06/29/a-story-of-runner-scaling/
- https://about.gitlab.com/blog/2022/02/16/a-community-driven-advisory-database/
- https://about.gitlab.com/blog/2022/01/20/securing-the-container-host-with-falco/
- https://about.gitlab.com/blog/2021/11/15/top-five-actions-owasp-2021/
- https://about.gitlab.com/blog/2021/11/11/situational-leadership-strategy/
- https://about.gitlab.com/blog/2021/10/11/how-ten-steps-over-ten-years-led-to-the-devops-platform/
- https://about.gitlab.com/blog/2022/08/10/securing-the-software-supply-chain-through-automated-attestation/
- https://about.gitlab.com/blog/2022/08/15/the-importance-of-compliance-in-devops/
- https://about.gitlab.com/blog/2022/08/16/eight-steps-to-prepare-your-team-for-a-devops-platform-migration/
- https://about.gitlab.com/blog/2022/08/17/why-devops-and-zero-trust-go-together/
- https://about.gitlab.com/blog/2022/08/18/the-gitlab-guide-to-modern-software-testing/
- https://about.gitlab.com/blog/2022/08/23/gitlabs-2022-global-devsecops-survey-security-is-the-top-concern-investment/
- [[https://about.gitlab.com/blog/2022/09/20/mobile-devops-with-gitlab-part-1/][Mobile DevOps with GitLab, Part 1 - Code signing with Project-level Secure Files]].
** Releases:
- https://github.com/changesets/changesets - A tool to manage versioning and changelogs
with a focus on multi-package repositories .
* Databases:
- [[https://github.com/AltimateAI/awesome-data-contracts][Github: AltimateAI/awesome-data-contracts]] - A curated list of awesome blogs,
  videos, tools and resources about Data Contracts.
** CAP Theorem:
You can only achieve 2 of these 3 properties of databases:

- *Consistency:* All Clients see the same data at the same time, regardless of
  Node connected to.
- *Availability:* Respond to Client Requests, even during partial Node failure.
- *Partition Tolerance:* System can tolerate network partitions (breaks)
  between some Nodes.
*** Distributed Database:
Typically will have a CP or AP database cluster since CA is not possible in a
distributed scenario due to needing to handle network partitions! ie. *There
will always be partitions, so the choices is Consistency vs Availability!*

- *Consistency (CP):* requires block further writes to all other nodes until data is
  written across them all. Need to return warnings during this
  period. eg. Banking.
- *Availability (AP):*
  - Reads: Keep accepting, but may return stale data.
  - Writes: Keep accepting writes, sync once network partition is resolved.
** Database vs Data Lake vs Data Warehouse:
Quick summary: [[https://youtu.be/-bSkREem8dM][YouTube: Database vs Data Warehouse vs Data Lake | What is the
Difference?]]
*** Database:
- OLTP - Designed to capture and record data (transactions).
- Live, Real-time data.
- Highly detailed data.
- Flexible Schema.
- Can be a bottleneck for Application/System processing.
*** Data Lake:
- Designed to capture large amounts of raw ([un-|semi-]structured) data.
  - ML/AI in current state.
  - Analytics/Reporting after processing.
- Normalised from a Lake to a Database or Data Warehosue.
*** Data Warehouse:
- OLAP (Online Analytical Processing) - Designed for Analytics/Reporting.
- Data is historical to near-real-time based on when it is updated from Source
  systems.
  - ETL process to push data into the Warehouse..
- Summarized data.
- Rigid Schema (Normalised).
- Decoupled from Application/System, so queries do not affect processing.
** Data Pipelines:
*** ETL: Extract, Transform, Load.
The mechanism of Extracting data from a Source (API, file, DB, Web Scraping,
...), transforming that data (PII redaction, schema changes, ...) and then
Loading it into a Target location (DB, Data Lake, Data Warehouse) for later
use.

- *Source(s) to Data Lake:* may be an EL or ETL process with minimal PII
  transforms. to keep the data RAW (or near-RAW) in the Data Lake.
- *Data Lake to Data Warehouse:* is usually an ETL process with schema
  changing + data sanitising transforms to make it suitable for consistent
  Analysis/Reporting.
*** [[https://meltano.com/][Meltano]] (Data Pipeline):
[[https://meltano.com/][meltano]] - /"Your CLI for ELT+: Open Source, Flexible, and Scalable."/

/"Move, transform and test your data with confidence using a streamlined data
engineering workflow you’ll love."/

Basically it uses plugins to create an ETL (Extraction, Transform, Loader)
pipeline, which can be configured in YAML.

- [[https://docs.meltano.com/][Meltano Docs]].
- [[https://github.com/meltano/meltano][Github: meltano/meltano]].
- [[https://docs.meltano.com/reference/command-line-interface][Meltano Docs: CLI Reference.]]
- [[https://youtu.be/sL3RvXZOTvE][YouTube: Meltano Speedrun 2.0]] - Quick demo of: Extraction, Loading,
  Transformation + Dashboard of transformed data in ~7mins (Suggest play at
  x1.5 speed).

*** DBT (Transforms):
- [[https://docs.getdbt.com/docs/quickstarts/dbt-core/quickstart][Docs DBT: DBT Core - Quick Start]] - Pretty thorough tutorial. Worth going
  through!
- [[https://github.com/dbt-labs/dbt-utils/][Github: dbt-labs/dbt-utils/]] - Additional utilities and test schema's.

** DB Admin:
- [[https://hub.docker.com/_/adminer/][Docker Hub: adminer]] - Adminer (formerly phpMinAdmin) is a full-featured
  database management tool written in PHP. Conversely to phpMyAdmin, it consist
  of a single file ready to deploy to the target server. Adminer is available
  for MySQL, PostgreSQL, SQLite, MS SQL, Oracle, Firebird, SimpleDB,
  Elasticsearch and MongoDB.
  - https://www.adminer.org/ - Replace phpMyAdmin with Adminer and you will get
    a tidier user interface, better support for MySQL features, higher
    performance and more security.
** Postgres:
- [[https://postgrest.org/en/stable/][PostgREST: Serve a RESTful API from any Postgres database]].
- [[https://www.docker.com/blog/how-to-use-the-postgres-docker-official-image/][Docker Blog: How to use the Postgres Docker Official Image]].
** Reporting/Visualisation:
- https://observablehq.com/ - Jupiter Notebooks style notebooks that can
  connect to multiple data sources (no need for a Data Lake??) to provide
  customisable graphs for Analytics. Also supports comments/collaboration.
*** ERD (Entity Relationship Diagram):
An ERD (Entity Relationship Diagram) is used to describe the Database Schema
with the inter-relationships between each table (entity). Typically it is a UML
style diagram. Similar to a UML Class diagram for programming.
* Dev Environment Setup:
** Browsers:
*** Chrome:
**** How to enable scrolling the tab strip?
- Goto: =chrome://flags/#scrollable-tabstrip=
- Select one of the options to enable.
** Drawing tablets:
- [[https://linuxwacom.github.io/][The Linux Wacom Project]] – Wacom device support on Linux.
- [[https://docs.krita.org/en/index.html][Krita Manual]] — Krita is a sketching and painting program designed for digital
  artists.
- [[https://linux.die.net/man/1/xsetwacom][xsetwacom(1)]] - commandline utility to query and modify wacom driver settings.
- [[https://github.com/Huion-Linux/DIGImend-kernel-drivers-for-Huion
][Github: Huion-Linux/DIGImend-kernel-drivers-for-Huion]] - This is a collection of
  huion graphics tablet drivers for the Linux kernel, produced and maintained
  by the DIGImend project.
- [[https://github.com/linuxwacom/xf86-input-wacom/wiki/Calibration
][Github: linuxwacom/xf86-input-wacom - Wiki/Calibration]].
** Factory Reset / Erase / Format / Wipe:
*** Mac:
- Reboot and hold ~Command + r~ until you see the Apple logo and/or hear a
  chime.
  - On an M1 mac, you need to hold the power button down until the ~Start up
    Options~ appears.
- A macOS Utilities window should pop up.
- Select: ~Disk Utility > Drive > Erase~.
**** Secure erase an SSD:
Need to get to the ~Secure Erase Options~ to do full disk erasing.
- Pick: ~Mac OS Extended (Journaled, Encrypted)~ and set an easy password.
- After first erase, change to: ~Mac OS Extended (Journaled)~ and then select
  a: ~Secure Erase Options~, to do full disk wipe.
** Mac config:
*** iterm2
- ~Preferences > Profiles > Keys > General > <Left/Right> Option Key = Esc+~ -
  to fix ~Alt~ to be the ~Meta~ key again.
- ~Preferences > Profiles > Keys > Key Mappings~ Added a new mapping: ~Send:
  "#"~, when ~Alt+3~ is pressed. Fixes sending ~#~ when my keyboard is on the
  Mac layer + ~Esc+~ is set above.
- ~Preferences > Profiles > Colors~ - Tweak the Blue to be brighter to make it
  readable.
- ~Preferences > Profiles > Terminal > Infinite Scrollback~.
*** System
- changed mouse scrolling direction to be normal.
- ~scaled~ + ~smallest~ font = native display resolution.
- Up display timeout time in Power menu.
- Finder: [[https://discussions.apple.com/thread/251374769][How to show hidden files in finder?]] ~Command+Shift+.~ in a Finder
  window.
- ~Preferences > Sharing > AirPlayReceiver~ Disabled due to port conflict
  on 5000.
*** Brew
- [[https://brew.sh][Homebrew]].
  #+BEGIN_SRC sh
    /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
  #+END_SRC
- ~brew leaves~ list packages without dependencies.
- Backup/Restore via: ~brew bundle~:
  #+BEGIN_SRC shell
    echo "---- Brew Bundle. Restore with: brew bundle ..."
    brew bundle dump -f --describe
    echo "---- Brew Bundle contents..."
    brew bundle list
  #+END_SRC

**** emacs:
- [[https://github.com/d12frosted/homebrew-emacs-plus][Github: d12frosted/homebrew-emacs-plus]] set to the latest branch:
  #+BEGIN_SRC sh
    brew tap d12frosted/emacs-plus
    brew install emacs-plus@30 --with-native-comp --with-imagemagick --with-mailutils --with-dbus
  #+END_SRC
- *NOTE:* need to do the reinstall dance because of the use of options:
  #+BEGIN_SRC sh
    brew uninstall emacs-plus@30
    brew install emacs-plus@30 --with-native-comp --with-imagemagick --with-mailutils --with-dbus
  #+END_SRC
- mu.
- aspell.
- cmake.
- cmake-docs
- ~markdown~ (markdown-preview).
***** Fix =Ctrl+<arrow>= getting swallowed.
Check =Settings > Keyboard Shortcuts > Mission Control=, to see if they have
the control arrow keys (=^<arrow>=) in use.
**** Dev:
- git-lfs (had to pin, see wiki).
- ~helm~.
- ~lens~ (GUI Kubernetes).
- ~awscli~
- ~xquartz~ for X11 server.
- ~wget~
- ~swig~.
- ~miniforge~ (M1 macs need this instead of miniconda to work).
- ~poetry~.
- ~docker --cask~ to pull down the Docker Desktop (https://formulae.brew.sh/cask/docker).
- ~dive~ (inspect size of docker layers).
- ~yq~ (YAML/XML/TOML CLI
  processor)(https://github.com/kislyuk/yq)(https://github.com/wagoodman/dive/issues/300
  ~yq -r .services[].image docker-compose.yml | xargs -n 1 dive --ci~
- ~hadolint~ - lint dockerfiles (https://github.com/hadolint/hadolint))
***** DBT:
#+BEGIN_SRC shell :results silent
  brew tap dbt-labs/dbt
  brew install dbt-postgres
#+END_SRC
***** postgres:
- Utilities (like =psql=) without installing =postgres=: :results drawer
  #+BEGIN_SRC shell
    brew reinstall libpq
  #+END_SRC
  - Then add: ~export PATH="/usr/local/opt/libpq/bin:$PATH"~, to: =~/.zshrc=.
  - See: [[https://stackoverflow.com/questions/44654216/correct-way-to-install-psql-without-full-postgres-on-macos][StackOverflow: Correct way to install =psql= without full postgress on MacOS]].
***** [[https://postgrest.org/en/stable/][postgrest]]:
PostgREST is a standalone web server that turns your PostgreSQL database
directly into a RESTful API. The structural constraints and permissions in the
database determine the API endpoints and operations.

- ~brew services stop postgres~ to avoid conflict with any dev containers.
- Install:
  #+BEGIN_EXAMPLE shell
    brew install postgrest
  #+END_EXAMPLE
***** python:
You can install python via brew, but it doesn't symlink: ~python3~ to
~python~. This is how to install + fix:

#+BEGIN_SRC shell :results silent
  brew install python
  rm -rf /usr/local/bin/python
  ln -s /usr/local/bin/python3 /usr/local/bin/python
#+END_SRC
**** Experiments:
- ~rust~, ~rustup~.
**** Fix symlink not making =<program>.app= show up in spotlight:
Problem is that standard symlinks (~ln -s /path/to/program.app /Applications/~)
doesn't work as an alias for discovery in spotlight since the Mac may confuse
the link as a path to a folder (~.app~ files are really folders).

[[https://apple.stackexchange.com/questions/106249/spotlight-and-alfred-cant-find-alias-to-emacs-app][Workaround]]:

- Open =Finder= and search for Program e.g. ~Cmd+Shift+G~ type path.
- Create an alias by ~Cmd+Opt~ clicking Program and dragging to ~/Applications/
  folder.
**** laptop:
- iterm2
- [[https://github.com/ankitpokhrel/jira-cli][Github: ankitpokhrel/jira-cli]].
***** autoraise:
- [[https://github.com/sbmpost/AutoRaise][Github: sbmpost/AutoRaise]] - focus follows mouse.
- [[https://github.com/Dimentium/homebrew-autoraise][Github: Dimentium/homebrew-autoraise]] - Brew formulae.
#+BEGIN_SRC shell :results silent
  brew tap dimentium/autoraise
  brew install autoraise
  brew services start autoraise
#+END_SRC
***** [[https://rectangleapp.com/][rectangle]]:
rectangle (snap to area shortcuts).
#+BEGIN_SRC shell :results silent
  brew install rectangle
#+END_SRC
*** FireFox
- ~about:config~ ~browser.tabs.tabMinWidth = 0~ to disable tab scrolling.
*** Docker
**** Install [[https://formulae.brew.sh/cask/docker][Docker Desktop]]:
#+BEGIN_SRC shell :results silent
  brew install --cask docker
#+END_SRC
- Follow [[https://docs.docker.com/desktop/mac/permission-requirements/][Docker Docs: Understanding permission requirements for Mac]] to update
  =/etc/hosts= to have the following:
  #+BEGIN_EXAMPLE shell
    127.0.0.1	localhost
    127.0.0.1	kubernetes.docker.internal
  #+END_EXAMPLE
**** Best-Practices
- https://pythonspeed.com/articles/poetry-vs-docker-caching/
- Create an explicit Bridge network for Host access to a container. Default
  network is locked down. eg.
  #+BEGIN_EXAMPLE yaml
    services:
      container-name:
      image: app:tag
      networks:
        - backend

    networks:
      # Without setting a `driver` field, this is a User-defined `bridge` network.
      # Which will be ideal for Production environments for inter-cluster connections.
      backend:
  #+END_EXAMPLE
**** Run AMD64 containers on ADM64:
- https://erica.works/docker-on-mac-m1/
- https://forums.macrumors.com/threads/docker-on-m1-max-horrible-performance.2321545/
- https://stackoverflow.com/questions/70649002/running-docker-amd64-images-on-arm64-architecture-apple-m1-without-rebuilding
- https://enjoi.dev/posts/2021-07-23-docker-using-amd64-images-on-apple-m1/
- https://www.reddit.com/r/docker/comments/o7u8uy/run_linuxamd64_images_on_m1_mac/
- https://medium.com/homullus/beating-some-performance-into-docker-for-mac-f5d1e732032c
-
**** Building AMD64 containers on ARM64:
- https://docs.docker.com/desktop/multi-arch/
- https://hublog.hubmed.org/archives/002027
- [[https://github.com/docker/for-mac/issues/5364][Github: docker/for-mac: "platform" option in docker-compose.yml ignored (preview version) #5364]]
- https://tongfamily.com/2021/12/15/the-weirdness-that-is-amd64-on-apple-m1-silicon/
- http://www.randallkent.com/2021/12/31/how-to-build-an-amd64-and-arm64-docker-image-on-a-m1-mac/
- https://docs.docker.com/buildx/working-with-buildx/
-
**** Podman (Docker alternative)
- https://medium.com/team-rockstars-it/how-to-implement-a-docker-desktop-alternative-in-macos-with-podman-bbf728d033da
- https://stackoverflow.com/questions/70892894/run-docker-compose-with-podman-as-a-backend-on-macos
- [[https://github.com/containers/podman/issues/13456][Github: containers/podman -  MacOS helper daemon (podman-mac-helper) fails to start and "mount" /var/run/docker.sock #13456]]
- https://devopscube.com/podman-tutorial-beginners/
-
**** Tooling
- [[https://github.com/emacs-lsp/dap-mode/issues/406][Github emacs-lsp/dap-mode: Feature request: support docker #406]]
** Raspberry Pi:
*** [[https://forum.manjaro.org/t/guide-install-manjaro-arm-minimal-headless-on-rpi4-with-wifi/96515][Manjaro headless install directly to a MicroSD card]]:
- Download minimal ARM iso from: https://manjaro.org/download/.
- Unpack compressed image.
- Burn to MicroSD card with: ~sudo dd if=~/Downloads/Manjaro-ARM-minimal*.img of=/dev/mmcblk0 bs=1M status=progress && sync~
- Mount ~ROOT_MNJRO~
  - Click in Thunar, which auto-mounts to: ~/var/run/media/root/~.
  - Or: ~sudo mount -o rw /dev/mmcblk0p2 /mnt~.
- Add WiFi config:
  #+BEGIN_SRC bash
    sudo mkdir -p /mnt/var/lib/iwd
    sudo touch /mnt/var/lib/iwd/<ssid>.psk
    echo "[Security]" >> /mnt/var/lib/<ssid>.psk
    echo "Passphrase=<password>" >> /mnt/var/lib/<ssid>.psk
  #+END_SRC
- Unmount and plug into the Pi and boot.
- ~ssh root@<ip>~
- You'll connect into the CLI Wizard.
*** Kiosk mode:
- *TODO:* Fill out with other details (retroactively looking at an existing
  Pi3B+ with a [[https://shop.pimoroni.com/products/hyperpixel-4?variant=12569539706963][Pimoroni: HyperPixel 4.0 (non-touch) display).]]
- Autostart Chromium by editing:
  ~/rootfs/home/pi/.config/lxsession/LXDE-pi/autostart~ with:
  #+BEGIN_EXAMPLE shell
    @xset s off
    @xset -dpms
    @xset s noblank
    @chromium-browser --kiosk http://<ip/fqdn> --start-fullscreen --incognito
  #+END_EXAMPLE
** Window Managers:
- [[https://polybar.github.io/][Polybar]] - A fast and easy-to-use tool for creating status bars
- [[https://suckless.org/][Dwm, dmenu | suckless.org]] software that sucks less. Home of dwm, dmenu and
  other quality software with a focus on simplicity, clarity, and frugality.
- [[https://github.com/i3/i3/discussions][Github: i3/i3 - Discussions]].
** Terminals:
- [[https://github.com/alacritty/alacritty][Github: alacritty/alacritty]]: A cross-platform, OpenGL terminal emulator.
- [[https://sw.kovidgoyal.net/kitty/][kitty]] - The fast, feature-rich, GPU based terminal emulator.
* Docker:
- [[https://www.youtube.com/watch?v=fqMOX6JJhGo][YouTube: Docker Tutorial for Beginners - A Full DevOps Course on How to Run
  Applications in Containers]].
** Best Practices:
*** No Root Access:
A container should never be run with root-level access. A role-based access
control system will reduce the possibility of accidental access to other
processes running in the same namespace. Either:

- Create a non-root user in the container:
  #+BEGIN_EXAMPLE dockerfile
    FROM python:3.5
    RUN groupadd -r myuser && useradd -r -g myuser myuser
    <HERE DO WHAT YOU HAVE TO DO AS A ROOT USER LIKE INSTALLING PACKAGES ETC.>
    USER myuser
  #+END_EXAMPLE
- Or while running a container from the image use, ~docker run -u 4000
  python:3.5~. This will run the container as a non-root user.
*** Trusted Image Source:
- Docker 1.8 feature that is disabled by default.
- ~export DOCKER_CONTENT_TRUST=1~ to enable.
- Verifies the integrity, authenticity, and publication date of all Docker
  images from the Docker Hub registry, by preventing access to unsigned images.
** Clean-up:
- Removing containers, volumes and dangling images:

  #+BEGIN_SRC shell :results quiet
    docker container prune -f
    docker volume prune -f
    docker image prune -f
  #+END_SRC
- Remove unused images: ~docker image prune --all~.
** ~docker-compose~:
- ~docker-compose up --build~ to force a rebuild (and ignore any previous
  built images).
- ~docker-compose down~ stops (~docker-compose stop~) all running containers in
  the docker compose file and then cleans up containers/networks/images.
** Docker Swarm:
Orchestrator (similar to Kubernetes) but built by the Docker Team.
*** Visualize Docker Swarm Containers across Nodes:
- [[https://github.com/dockersamples/docker-swarm-visualizer][Github: dockersamples/docker-swarm-visualizer]] - Constrain to the Master node
  to visualise the containers across all nodes from the Web Browser.

  Vlisualizer deployed via ~docker run~:
  #+BEGIN_EXAMPLE shell
    docker run -it -d -p 8080:8080 -v /var/run/docker.sock:/var/run/docker.sock dockersamples/visualizer
  #+END_EXAMPLE

  Visualizer deployed via Docker Swarms ~docker service~:
  #+BEGIN_EXAMPLE shell
    docker service create --name=viz --publish=8080:8080/tcp --constraint=node.role==manager --mount=type=bind,src=/var/run/docker.sock,dst=/var/run/docker.sock dockersamples/visualizer
  #+END_EXAMPLE
** Networks:
- Can use container name to connect between containers.
- ~docker run -d --name=app1 --link db:db my-app1~ The `--link` command writes
  the provided Container Name (+IP) into: ~/etc/hosts~, so that all references
  to the linked Container work.
*** ~bridge~:
- The default network that all docker containers (without network config) are
  created in.
- Assigns private IP's to each container (eg. ~172.17.0.x~).
- Requires explicit create command to create additional bridge networks.
- DNS defaults to: ~127.0.0.11~.
- Port Mapping to expose Container Ports to the Host.
  - Can run multiple Containers with the same internal port.
*** ~none~:
- Network with no external access.
*** ~Host~:
- Directly map Containers onto the Hosts IP + Port range.
- No ~port~ config required for mapping.
- Cannot support multiple Containers re-using the same Port, due to Host-side
  conflicts.
** Performance:
- Uses ~cgroups~ (Control Groups) to allocate Hosts CPU/Memory to containers.
- Use ~--cpu/--memory~~ to constrain the running container.
** Reduce image size:
- If using ~COPY~ to pull in directories. Add a ~.dockeringnore~ file to add
  exclusions. eg. ~.git~, ~**/tests~, ~**/*.ts~.
- Generate/install in the image at build time instead of ~COPY~ = Docker layer
  caching.
- Check for ~-slim~/~alpine~ versions of the base image.
- Move ~COPY~ commands near end of the file. Avoid Cache misses!
- Pull in versioned OS-packages. Avoid Cache misses, but more Platform burden!
- Use multi-stage docker files to build code in a fat stage, but copy in the
  artifacts in to the thin stage with an ~ENTRYPOINT~

  #+BEGIN_EXAMPLE dockerfile
    FROM microsoft/dotnet:2.2-sdk AS builder
    # 1730MB Fat Stage.
    WORKDIR /app

    COPY *.csproj  .
    RUN dotnet restore

    COPY . .
    RUN dotnet publish --output /out/ --configuration Release

    FROM microsoft/dotnet:2.2-aspnetcore-runtime-alpine
    # 161MB Thin stage.
    WORKDIR /app
    COPY --from=builder /out .
    EXPOSE 80
    ENTRYPOINT ["dotnet", "aspnet-core.dll"]
  #+END_EXAMPLE
* Emacs:
** Core:
*** Change font size in GUI Emacs buffer:
- Increase: ~C-xC-+~.
- Decrease: ~C-xC--~.
*** How to enter Diacritics (eg. caret) above characters?
See: [[https://www.masteringemacs.org/article/diacritics-in-emacs][Mastering Emacs: Diacritics in Emacs]].

#+BEGIN_EXAMPLE text
  C-x 8 <symbol> <character>
  ;; Example for: â.
  C-x 8 ^ a
  ;; With the caret being generated by pressing: =shift+6=.
#+END_EXAMPLE
*** Yasnippet:
- [[https://youtu.be/xmBovJvQ3KU][YouTube: Supercharge your Emacs / Spacemacs / Doom with Yasnippets!]] ~13mins
  walkthrough.
** org-mode:
- ~org-eww-copy-for-org-mode~ to copy text + links from Eww to Org. ~C-y~ to
  paste.
*** Build Your Website with Org Mode - System Crafters
[2022-11-05 Sat 08:50]
https://systemcrafters.net/publishing-websites-with-org-mode/building-the-site/
*** Formatting:
- [[https://orgmode.org/manual/Emphasis-and-Monospace.html][Emphasis and Monospace]]
- *bold*
- /italic/
- _underlined_
- =verbatim=
- ~code~
- +strike-through+
- src_python{inline python}  # ~src_<lang>[<header_arguments>]{<code>}~ [[https://orgmode.org/manual/Structure-of-Code-Blocks.html#Structure-of-Code-Blocks][Structure of Code Blocks]]
- code blocks
#+NAME: <name>
#+BEGIN_SRC <language> <switches> <header arguments>
  <body>
#+END_SRC
- quote blocks
  #+BEGIN_QUOTE
  <body>
  #+END_QUOTE
*** PlantUML + Org Babel:
- https://orgmode.org/worg/org-contrib/babel/languages/ob-doc-plantuml.html
- plantuml block
  #+begin_src plantuml :file designs/hello-uml.png
  Bob -> Alice : Hello World!
  #+end_src
** regex:
*** How to rejoin multi-line hyphen split words?
The following example is how to replace a hyphen split word across multiple
lines and place it back onto one line. ie.

#+BEGIN_EXAMPLE text
# Before:
Sentence split across multi-
ple lines.

# After:
Sentence split across
multiple lines.
#+END_EXAMPLE

#+BEGIN_SRC emacs-lisp
M-x replace-regexp
\s-q\(\w+\)-\(^J\)\(\w+\) → ^J\1\3
#+END_SRC
*** How to upcase a group during ~M-x replace-regexp~?
Emacs step if I want to replace a replacement group and upcase it. eg. from:
~data_type: boolean~, to: ~data_type: BOOLEAN~.

- ~M-x replace-regexp~.
- Find: ~data_type: \(.*\)~.
- Replace: ~data_type: \,(upcase \1)~.

This will work for other elisp built-in's. eg.

- ~\,(downcase \1)~.
- ~\,(capitalize \1)~.
*** [[http://ergoemacs.org/emacs/find_replace_inter.html][ErgoEmacs: Find Replace in directories]] / [[https://www.gnu.org/software/emacs/manual/html_node/efaq/Replacing-text-across-multiple-files.html][GNU Emacs: Replacing text across multiple files]]:
- Either:
  - ~M-x find-name-dired~, enter filename wildcard.
  - Mark ~m~ files (~t~ marks all files), then press ~Q~.
  - ~<find> regex~ return, ~<replace> string~ return.
  - Confirm/deny replace with the usual: ~!~, ~y~, ~n~.
- Or:
  - ~C-x p r~ in a =project= managed repo.
  - ~<find> regex~ return, ~<replace> string~ return.
  - Confirm/deny replace with the usual: ~!~, ~y~, ~n~.
** DAP:
*** Registering a debug template for: ~dap-mode~, to use.
#+BEGIN_EXAMPLE emacs-lisp
(dap-register-debug-template
  "Python :: Run pytest (projectX buffer)"
  (list :type "python"
        :args ""
        :cwd "/Users/<user>/projects/projectX/"
        :program nil
        :module "pytest"
        :arguments "-p no:warnings"
        :request "launch"
        :name "Python :: Run pytest (projectX buffer)"))
#+END_EXAMPLE
** Jupyter:
- https://discourse.julialang.org/t/jupyter-integration-with-emacs/21496/5 -
  basic ~IJulia~ + ~jupyter~ install steps (no use-package).
* FrontEnd:
** React:
- View cookies in browser: ~Developer Tools > Storage Tab > Cookies~.
- ~redux~ is the store of all BE DB state in the FE.
- Add ~&profile~ to an API call to get performance output!!
- ~npm install --target_arch=x64~ - until there is arm support.
- https://github.com/marmelab/react-admin
- Print all object properties: ~console.log(Object.getOwnPropertyNames(obj))~.
*** AST (Abstract Syntax Tree):
What is Abstract Syntax Tree?

#+BEGIN_QUOTE
It is a hierarchical program representation that presents source code structure
according to the grammar of a programming language, each AST node corresponds
to an item of a source code.
#+END_QUOTE

- https://itnext.io/ast-for-javascript-developers-3e79aeb08343
** UI Testing:
*** [[https://playwright.dev/][playwright]]:
~playwright~ is a modern equivalent to [[https://www.selenium.dev][Selenium]]. Benefits include:

- Speed.
- Handles installation of isolated browsers to test/debug against.
- Support for [[https://playwright.dev/docs/test-parallel][sharding/parallelisation]] of tests.
- auto-wait.
- Built-in: [[https://playwright.dev/docs/trace-viewer-intro][Tracing]], [[https://playwright.dev/docs/codegen-intro][Recording (via Codegen)]], [[https://playwright.dev/docs/running-tests#test-reports][Reporting]].
- Good [[https://playwright.dev/docs/intro][Docs]].
- Cross-Platform.
- Cross-language API.
- Native [[https://playwright.dev/docs/ci-intro][CI]]/Local development support. eg. Auto-Trace on first retry (but not
  subsequent failures).
- [[https://playwright.dev/docs/test-snapshots][Visual Comparisons]] of screenshots.
- Uses [[https://playwright.dev/docs/test-assertions][Assertions]] via [[https://jestjs.io/docs/expect][~expect~]] library.
- Automatic install of Dependencies/CI on first install.

[[https://playwright.dev/docs/best-practices][Best Practices]].
* Git:
- https://www.conventionalcommits.org/en/v1.0.0/ - A specification for adding
  human and machine readable meaning to commit messages.
- https://github.com/conventional-changelog/conventional-changelog - Generate
  changelogs and release notes from a project's commit messages and metadata.
- https://github.com/conventional-changelog/releaser-tools - Create a
  GitHub/GitLab/etc. release using a project's commit messages and metadata.
** Alternative VCS's:
- [[https://www.fossil-scm.org/home/doc/trunk/www/index.wiki][Fossil]] - Self-contained with VCS as a binary with: Project Management, WebUI,
  Lightweight, self-host friendly, autosync.
- [[https://pijul.org/][Pijul]] ([[https://pijul.org/manual/introduction.html][Pijul Docs]]) - Strong focus on conflict resolution (beyond GIT),
  order-less applying of changes, partial clones. Support to import from Git
  (not optimised).
** Configure git repo with explicit SSH Key:
In cases where you need to use an explicit SSH key for a repo, eg. Personal +
Work Github account, and you want a personal repo accessiable by both
personal/work systems. Github prevents the re-use of an SSH key across Github
Accounts ([[https://docs.github.com/en/authentication/troubleshooting-ssh/error-key-already-in-use][Github Docs: Error: Key already in use]]). Therefore you need to create
a Personal SSH key on the Work System to clone the Personal repo.

#+BEGIN_EXAMPLE sh
  git clone git@provider.com:userName/projectName.git --config core.sshCommand="ssh -i ~/.ssh/private_ssh_key"
#+END_SRC

Or after the fact with:
#+BEGIN_EXAMPLE sh
  git config --local --add core.sshCommand "ssh -i ~/.ssh/private_ssh_key"
#+END_EXAMPLE
** Git Hooks:
- [[https://pre-commit.com][~pre-commit~]] - A framework for managing and maintaining mutli-language
  pre-commit hooks. Repo of hooks in YAML format.
*** Why is the failing exit code ignored and not blocking commits??
Calling commands like:~go-task~, will run in a separate sub-shell, but the exit
code is not passed to the shell running the ~pre-commit~. ~|| exit $?~, pipes
the exit code to the main shell process. See: [[https://stackoverflow.com/questions/29969093/exit-1-in-pre-commit-doesnt-abort-git-commit][SO: Exit in a ~pre-commit~ does
not abort ~git commit~]].

#+BEGIN_EXAMPLE shell
  go-task lint || exit $?
#+END_EXAMPLE
* Job hunting:
- https://github.com/readme/guides/technical-interviews
- https://www.codinginterview.com/
- https://www.pramp.com/#/
- https://hackingthesystemsdesigninterview.com
- https://blog.bytebytego.com - Newsletter by Alex Xu (Author of: /"System Design Interview/").
- https://www.siliconmilkroundabout.com - London-based Job Fair.
* Kubernetes:
- [[https://kurl.sh/][kURL: Open Source Kubernetes Installer]].
- https://docs.k3s.io - Lightweight Kubernetes. Easy to install, half the
  memory, all in a binary of less than 100 MB.
- https://www.cncf.io/kubecon-cloudnativecon-events/
- [[https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/][Kubernetes docs: Pull image from a Private Registry]].
** Helm Charts:
- Hierarchical to call sub-charts as sub-dependencies.
- Values to be passed into the charts.
*** [[https://eigentech.slack.com/archives/CH1CHKYP8/p1650553648237999][how does one deploy from a local helm chart without publishing it?]]
- ~helm upgrade --install <deployment_name> <local_chart_dir>~
*** Dagster docs + dump current helm chart values: https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm
*** [[https://helm.sh/docs/chart_template_guide/debugging/][Debugging Templates]]:
- ~helm lint~ is your go-to tool for verifying that your chart follows best
  practices.
- ~helm install --dry-run --debug~ or ~helm template --debug~: We've seen this
  trick already. It's a great way to have the server render your templates,
  then return the resulting manifest file.
- ~helm get manifest~: This is a good way to see what templates are installed
  on the server.
- **NOTE:** variable substitution still happens on commented out code in
  templates, so comment out broken sections if it fails to render with ~helm
  install --dry-run --debug~.
- YAML node typing eg. ~age: !!str 21~, or: ~port: !!int "80"~.
**** TODO Document Debugging Workflow                              :WORKFLOW:
- Are there docs already on Confluence on debugging.
- Raise Task to add vscode/emacs debug tasks to ~eigen~.
- Document the workflow with the debugger (include vscode/emacs tutorial links).
- How to debug into a Docker container? - new DockerFile section with ~debugpy~ ??
*** [[https://stackoverflow.com/questions/72126048/error-exec-plugin-invalid-apiversion-client-authentication-k8s-io-v1alpha1-c][SO: invalid apiVersion "client.authentication.k8s.io/v1alpha1"]]
- ~aws eks update-kubeconfig --name ${EKS_CLUSTER_NAME} --region ${REGION}~.
*** [[https://github.com/bitnami/charts/issues/10539][Github/bitnami: Helm charts repository ~index.yaml~ retention policy #10539]] - Drama!!
** Kubernetes Networks:
*** Ingress:
- [[https://www.youtube.com/watch?v=GhZi4DxaxxE][YouTube: Kubernetes Ingress Explained Completely for Beginners]].
- Ingress is the LoadBalancer/Routing defined within the Kubernetes Cluster
  config.
- Still require an external, to the Cluster, Load Balancer (or Proxy) but this
  will just have to deal with a single root URL that is passed into your
  Cluster's Ingress (and then routed to the correct Service's Pod(s)).
- Equivalent to a reverse-proxy like: nginx, HaProxy, Traefik.
**** Ingress Controller:
- Commonly use nginx (or others) as an Ingress Controller
  (eg. ~nginx-ingress-controller~ image).
- Deployment/Service/ConfigMap/Auth Yaml's.
**** Ingress Resource:
- Handles routing to respective service based off the requested URL.
- Can handle 1 or multiple Domain Paths, by creating a ~rule~ for each ~path~.
- ~kubectl describe ingress <image>~
** Local Development:
- https://necessaryeval.com/2021/09/01/kubernetes-primer/ - Local development
  with ~minikube~.
- https://kubernetes.io/blog/2018/05/01/developing-on-kubernetes/
  - Local vs. remote development.
  - Tools:
    - https://github.com/Azure/draft - aims to help you get started deploying
      any app to Kubernetes. It is capable of applying heuristics as to what
      programming language your app is written in and generates a Dockerfile
      along with a Helm chart. It then runs the build for you and deploys
      resulting image to the target cluster via the Helm chart. It also allows
      user to setup port forwarding to localhost very easily.
    - https://github.com/GoogleCloudPlatform/skaffold - tool that aims to
      provide portability for CI integrations with different build system,
      image registry and deployment tools.
    - https://github.com/solo-io/squash - consists of a debug server that is
      fully integrated with Kubernetes, and a IDE plugin.
    - https://www.telepresence.io/ - connects containers running on developer’s
      workstation with a remote Kubernetes cluster using a two-way proxy and
      emulates in-cluster environment as well as provides access to config maps
      and secrets.
    - https://github.com/vapor-ware/ksync - Synchronizes application code (and
      configuration) between your local machine and the container running in
      Kubernetes.
- https://kubernetes.io/docs/tasks/debug/debug-cluster/local-debugging/ -
  Developing and debugging services locally using telepresence.
- http://next.nemethgergely.com/blog/using-kubernetes-for-local-development -
  Local development via ~minikube~ & ~skaffold~.
** [[https://docs.replicated.com/][Replicated]]:
- https://docs.replicated.com/ - Replicated allows software vendors to package
  and securely distribute their application to diverse customer environments,
  including both on-premises and cloud environments.
- https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/
* ML:
** ML Articles:
- https://simonwillison.net/2022/Jul/9/gpt-3-explain-code/
** DagFlow
- [[https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm][Dagster: deploying with Helm]].
* Networks:
** DNS:
- https://root-servers.org/ - Root DNS servers at the top of the DNS
  hierarchy. These root servers farm out requests down to Top-Level
  (io/com/net/edu/...) servers who farm out to down to Secondary-Level
  (amazon.com/github.com/...) DNS servers to complete Name-IP lookups.
- *Local Resolver Library:* Local DNS Cache.
- *Local DNS Server:* Hosted by ISP's as a DNS Cache + inspect
  traffic/requests.
* People Skills:
** Feedback:
- Aim for a learning opportunity.
- Constructive & Actionable feedback, based on facts (where possible).
- Follow up with questions on specifics.
*** The BID model.
*BID* stands for *Behaviour* > *Impact* > *Dialogue*, and has the power to
 transform relationships.

- *Behaviour:* Describe the behaviour you observed, keeping it non-judgemental
  and specific.
- *Impact:* Describe the impact of the behaviour. Again, keep it simple and
  non-judgemental. Note: Impact here might be at the individual emotional level
  (how you felt), or at a more cultural level or in relation to someone, or
  something else (the meeting over-ran, we missed our financial target).
- *Dialogue:* Open discussion around opportunities. Be future focussed where
  you can.

*** Triggers that get in the way of feedback:
- *Truth Trigger:* Reject feedback on belief it is factually wrong.
- *Relationship Trigger:* Reject feedback based on person giving it.
- *Identity Trigger:* Reject feedback that challenges your
  identity/self-perception.
*** Trigger Workarounds:
- Pause.
- Acknowledge.
- Question.
  - Use BID.
  - Ask for specifics (in good/bad feedback).
  - Don't use generic questions! (non-actionable questions/answers).
  - /"What is the one thing I can improve on?"/ (focused)
  - /"Can I get feedback on this new thing I am doing, after it is done?"/
    (pre-request feedback).
  - /"Can you walk me through that?"/
*** Finding Feedback Situations:
- *Look for outcomes:* Notice when someone creates a desirable one.
- Apply *BID* for positive/negative feedback.
- *Learn from praise:* Ask for specifics.
** Winning Arguments:
*** Tech Debt:

**** The benefits of upgrading Languages/Dependencies.
It can be hard to justify doing upgrades vs. Feature Development. Try following
Solutions:

- Make it so easy to do the task that it can be done, without scheduling,
  alongside Feature Development.
- Identify the User Value. eg. /"As a User I want to minimize the chances of
  being hacked by the flaws in current version of: <Language/dependency>./"

See: [[https://www.youtube.com/watch?v=vSuJqMRG1WM][YouTube: TECHNICAL STORIES DON'T WORK]].
* Python:
** Python Articles:
- https://pythonspeed.com/
- https://about.gitlab.com/blog/2022/09/06/test-your-software-supply-chain-security-know-how/
- https://pythoninsider.blogspot.com/2022/09/python-releases-3107-3914-3814-and-3714.html -
  Python releases 3.10.7, 3.9.14, 3.8.14, and 3.7.14 are now available + CVE fix.
** Build Tools:
- https://github.com/benfogle/crossenv - Virtual Environments for
  Cross-Compiling Python Extension Modules.
** CLI packages:
- https://github.com/pallets/click - Command Line Interface Creation Kit
- https://cloup.readthedocs.io/en/stable/ - Click + Option Groups.
- https://github.com/astanin/python-tabulate - Pretty-print tabular data.
- https://github.com/termcolor/termcolor - Abstract out setting text colours.
** Debugging:
- https://github.com/ztlevi/LSP-Debug/blob/master/README.md#L4-L9 - debug
  python via DAP - editor support.
- https://github.com/bloomberg/memray - Python memory profiler.
- https://github.com/benfred/py-spy - Python sampling profiler.
*** [[https://github.com/pdbpp/pdbpp][Github: pdbpp/pdbpp]]:
Drop in replacement for ~pdb~ that does dot completions and syntax
highlighting.

- ~pytest --pdb~ to drop into a ~pdb~ session on test failure.
- ~list~ - show surrounding code at point.
- ~where|whatis~ - show traceback.
- Original ~pdb~ import is under: ~pdb.pdb.*~.
** Django:
- [[https://books.agiliq.com/projects/django-admin-cookbook/en/latest/index.html][Django Admin Cookbook]].
- [[https://django-extensions.readthedocs.io/en/latest/graph_models.html][django-extensions: Graph Models]].
** Celery:
*** Debugging:
**** Celery's remote debugger:
  #+BEGIN_EXAMPLE python
  from celery.contrib import rdb
  ...
  rdb.set_trace()
  #+END_EXAMPLE
- Then connect over telnet: ~telnet localhost 6900~.
- If in docker:
  - add: ~CELERY_RDB_HOST=0.0.0.0~ to ~.env~.
  - Expose Celery debug port in ~docker.compose.yml~. eg. ~6901~
  - ~telnet localhost 6901~ from host.
**** Debug Celery via PDB in Django:
- Add ~CELERY_TASK_ALWAYS_EAGER=True~ in: ~settings.py~.
** Conda:
- https://conda-forge.org/blog/posts/2020-10-29-macos-arm64/ - macOS ARM builds
  on conda-forge.
- [[https://github.com/conda/conda/issues/9957][conda/conda - conda update breaks conda with ImportError: libffi.so.6: cannot open shared object file #9957]]
*** Conda + Emacs:
- [[https://github.com/necaris/conda.el/issues/39][necaris/conda.el - Cannot activate any env on OSX #39]]
*** Conda + Docker:
- https://uwekorn.com/2021/03/01/deploying-conda-environments-in-docker-how-to-do-it-right.html
*** Mamba instead of Conda:
- https://mamba.readthedocs.io/en/latest/user_guide/mamba.html
- https://labs.epi2me.io/conda-or-mamba-for-production/
** Packaging:
*** [[https://hatch.pypa.io/latest/][Hatch]]:
Not tried it yet but:

#+BEGIN_QUOTE
Hatch is a modern, extensible Python project manager.

Features:

- Standardized build system with reproducible builds by default.
- Robust environment management with support for custom scripts.
- Easy publishing to PyPI or other indexes.
- Version management.
- Configurable project generation with sane defaults.
- Responsive CLI, ~2-3x faster than equivalent tools.
#+END_QUOTE
*** poetry:
- [[https://python-poetry.org/docs/managing-environments/#switching-between-environments][Set poetry python version]]: ~poetry env use python<x.y>~.
- ~poetry show --tree~ for poetry dependency graph.
**** https://github.com/opeco17/poetry-audit-plugin
**** [[https://github.com/python-poetry/poetry/issues/2094#issuecomment-1243195601][python-poetry/poetry: Poetry is extremely slow when resolving the dependencies (#2094)]]:
@Kache, It appears to search through dependencies depth-first, rather than breadth-first. As a result, you've probably got a something earlier in your pyproject.toml that depends on ddtrace, so the dependency resolver grabbed that version and tried to resolve using that, rather than the ddtrace version you've specified.

I've had some success moving the dependencies I want exact version logic prioritizing earlier in the pyproject.toml file.

(I also disabled IPv6, upgraded to poetry 1.2x, and have reduced the possible space for the troubling aws libraries (boto3 and awsci, for me) so those go at the very end of my dependency file and have only a few recent versions to chew through.

I'm seeing dependency resolution time between 5 and 35 seconds most of the time now.
*** [[https://setuptools.pypa.io/en/latest/index.html][setuptools]] + [[https://github.com/pypa/setuptools_scm/][setuptools_scm]]:
~setuptools~ & ~pip~ / /"PyPa/" have moved on to fully support
~pyproject.toml~-only Python packages. With just a ~pyproject.toml~ file we
have:

- Metadata.
- Isolated builds.
- Tooling config.
- Dynamic versioning from Git.

#+BEGIN_EXAMPLE shell
  python -m venv .venv
  source .venv/bin/activate
  pip install .
  pip install .[<group>]
  python -m build  # `pip install build` if not in `pyproject.toml`.
#+END_EXAMPLE

See:

- [[https://github.com/pypa/setuptools_scm/][Github: pypa/setuptools_scm]] - Dynamic Versioning.
- [[https://setuptools.pypa.io/en/latest/userguide/pyproject_config.html][SetupTools Docs: Configuring setuptools using ~pyproject.toml~ files]].
- [[https://peps.python.org/pep-0633/][PEP 633 – Dependency specification in pyproject.toml using an exploded TOML
  table]] - Detail on current TOML definitions.
- [[https://peps.python.org/pep-0621/][PEP 621 – Storing project metadata in ~pyproject.toml~]].
- [[https://packaging.python.org/en/latest/tutorials/packaging-projects/][Python Docs: Packaging Python Projects]].
- [[https://github.com/jackson15j/python_homework_config_file_parser][Github: jackson15j/python_homework_config_file_parser]] - a project that is
  pure python packaging and ~project.toml~-only.
** Security:
- https://github.com/sonatype-nexus-community/jake - report vulnerabilities.
- https://adamj.eu/tech/2019/04/10/how-to-score-a+-for-security-headers-on-your-django-website/
** Templating:
- https://www.makotemplates.org/ - Mako is a template library written in
  Python. It provides a familiar, non-XML syntax which compiles into Python
  modules for maximum performance.
** Testing:
*** [[https://hypothesis.readthedocs.io/en/latest/][hypothesis]]:
Hypothesis is a Python library for creating unit tests which are simpler to
write and more powerful when run, finding edge cases in your code you wouldn’t
have thought to look for. It is stable, powerful and easy to add to any
existing test suite.
- https://hypothesis.works/
- Uses ML to do [[https://en.wikipedia.org/wiki/QuickCheck][/"Property-based testing/".]]
*** pytest:
- [[https://docs.pytest.org/en/6.2.x/warnings.html#disabling-warning-capture-entirely][Disable warnings]] with: ~-p no:warnings~.
** Web Frameworks:
- [[https://www.tornadoweb.org/en/stable/][Tornado]] - Python web framework and asynchronous networking library. Ideal for
  long polling, WebSockets and other long-lived connections.
* Security:
** Best Practices:
- [[https://cheatsheetseries.owasp.org/index.html][OWASP: Cheat Sheet Series]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Attack_Surface_Analysis_Cheat_Sheet.html][OWASP: Attack Surface Analysis Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Authentication_Cheat_Sheet.html][OWASP: Authentication Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Authorization_Cheat_Sheet.html][OWASP: Authorization Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Cryptographic_Storage_Cheat_Sheet.html][OWASP: Cryptographic Storage Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Docker_Security_Cheat_Sheet.html][OWASP: Docker Security Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Input_Validation_Cheat_Sheet.html][OWASP: Input Validation Cheat Sheet]].
  - [[https://cheatsheetseries.owasp.org/cheatsheets/Threat_Modeling_Cheat_Sheet.html][OWASP: Threat Modeling Cheat Sheet]].
*** Prevent debugging Production code.
- Prevent malicious actors attaching a debugger on Production instances to
  decompile the code.
- API's/Code that actively rejects Debuggers, crashes code, different code
  paths (obfuscation), reports home. ie. fail securely.
** Security Bodies/Sites:
- [[https://www.first.org/cvss/][CVSS (Common Vulnerability Scoring System)]] - Used in the scoring of PEN Tests.
- [[https://www.cve.org/][CVE (CyberSecurity Vulnerabilities)]] ([[https://cve.mitre.org/index.html][Old CVE site (Should be dead in
  2023)]]). - collection of all security vulnerabilities.
- [[https://owasp.org/][OWASP (Open Web Application Security Project)]] - Nonprofit looking to improve
  security through Open-Source projects.
- https://infosec.mozilla.org/guidelines/web_security
** Terminology:
*** Vertical Separation Flaw:
- Access resources granted to more privileged accounts.
- eg. gaining Administrator privileges.
*** Horizontal Separation Flaw:
- Access to resources granted to a similarly configured account.
- eg. modifying data belonging to a different User of the same Application.
** Security Articles:
- https://www.cve.org/ -  Identify, define, and catalog publicly disclosed
  cybersecurity vulnerabilities.
- https://cwe.mitre.org/top25/archive/2022/2022_cwe_top25.html
- https://owasp.org/ - The Open Web Application Security Project® (OWASP) is a
  nonprofit foundation that works to improve the security of software.
- https://owasp.org/www-project-top-ten/
- https://signal.org/blog/building-faster-oram/
- https://arstechnica.com/?p=1872326 - 10 malicious Python packages exposed in
  latest repository attack.
- https://www.synopsys.com/blogs/software-security/sast-vs-dast-difference/ -
  Static (White box) vs Dynamic (Black box) Application Security Testing.
** Tools:
- https://www.rapid7.com/products/insightappsec/ - InsightAppSec performs
  black-box security testing to automate identification, triage
  vulnerabilities, prioritize actions, and remediate application risk.
- https://www.rapid7.com/products/insightvm/ - Discover risks across all your
  endpoints, cloud, and virtualized infrastructure.
- https://www.keycloak.org/ - Open Source Identity and Access Management Add
  authentication to applications and secure services with minimum effort.  No
  need to deal with storing users or authenticating users.  Keycloak provides
  user federation, strong authentication, user management, fine-grained
  authorization, and more.
*** WireGuard: fast, modern, secure VPN tunnel
[2022-09-11 Sun 15:47]
https://www.wireguard.com/

WireGuard® is an extremely simple yet fast and modern VPN that utilizes state-of-the-art cryptography. It aims to be faster, simpler, leaner, and more useful than IPsec, while avoiding the massive headache. It intends to be considerably more performant than OpenVPN. WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances. Initially released for the Linux kernel, it is now cross-platform (Windows, macOS, BSD, iOS, Android) and widely deployable. It is currently under heavy development, but already it might be regarded as the most secure, easiest to use, and simplest VPN solution in the industry.
* System Design:
** Principals:
*** [[https://en.wikipedia.org/wiki/Don%27t_repeat_yourself][DRY]]:
Don't Repeat Yourself.
*** [[https://en.wikipedia.org/wiki/SOLID][SOLID]]:
- *S*ingle-responsibility principle: eg. classes should have a single
  responsibility.
- *O*pen-closed principle: Open for extension, but closed for modification.
- *L*iskov substitution principle: Functions that use pointers or references to
  base classes must be able to use objects of derived classes without knowing
  it.
- *I*nterface segregation principle: Don't force Clients to depend on unused
  interfaces.
- *D*ependency inversion principle: Depend upon abstractions, not concretions.
** Ask Why a System Works?:
- learn how popular applications work at a high-level.
- Start to understand why some component is used instead of another.
- Build serious side projects. Start simple and iterate to improve & refine it.
- Build a system from scratch and get familiar with all the processes and details of its construction.
- Focus less on mechanics and more on trade-offs.
- Focus on the high-level. Low-level will crop up.
** Breakdown strategies:
- Ask refining questions.
  - *Functional:* Requirements the Client needs directly. eg. Send messages in near real-time to contacts.
  - *Non-functional:* indirect requirements. eg. Performance shouldn't degrade with load.
  - Clarify assumptions.
  - Honesty when ignorant.
- Handle the data.
  - Data size now?
  - Data growth rate?
  - How data is consumed by User or othe SubSystems?
  - Read / Write heavy?
  - Strict or Eventual Consistency?
  - what's the durability target of the data?
  - Privacy/Regulatory concerns for storing/transferring User data?
- Discuss the components.
  - Highlight reasoning.
  - Talk around conflicts with examples of pain/components needed to work the other options.
  - High-level API design for User clarity.
- Discuss trade-offs.
  - Pros/Cons.
  - Monetary/Technical complexity (aim for Resource efficiency).
  - Plan for this designs weakness.
  - Highlight and explain weaknesses. eg. Design won't scale, but added monitoring, to reduce cost and time to do a new design.
  - Add fault tolerance and security to the design.
** Abstractions:
- Network: Use RPCs (Remote Procedure Calls) to abstract away network communications and make all calls appear to be local.
-
** Components in Detail:
*** API's:
**** [[https://www.openapis.org/][OpenAPI]]:
- [[https://learn.openapis.org/][Getting Started | OpenAPI Documentation]].
- [[https://openapi.tools/#documentation][OpenAPI Tools]].
*** Data Pipelines:
Data Pipelines are used to [[*Normalisation:][normalise]] raw(/"messy/", not directly usable)
source(s) of data (eg. API's, No/SQL DBs, Files) into a /"structured/" target
DB schema for later [[*EDA (Exploratory Data Analysis):][EDA (Exploratory Data Analysis)]]/Processing/Storage (eg. [[*ML (Machine Learning):][ML
(Machine Learning)]], Data Lakes/BI (Business Intelligence) Dashboards).

Data Pipelines can do the processing either by:

- *Batch:*
  - Load data in /"batches/" (scheduled/off-peak).
  - /Usually/ optimal if there is no immediate need for the data.
  - Typically closely tied to an [[*ETL (Extract, Transform, Load):][ETL]] data integration process.
- *Streaming:*
  - Requirement for /Real-Time/ data.
    - Low-latency to [[*Data Repository:][Data Repository]] due to processing shortly after occurring.
  - [[*Event:][Events]] are transported by a [[*Message Broker:][Message Broker]] or Messaging System ([[*Queues:][Queue]]).
  - Less Reliable - Messages /may/ be dropped/lost or stuck in a queue.
    - Reduced by Message Broker acknowledgements by Consumer to remove from the
      queue.
  - Tooling: [[http://kafka.apache.org/][Kafka]].

The Core of a Data Pipeline is:

- [[*Data Ingestion:][Data Ingestion]].
- Data Transformation (see: [[*Normalisation:][Normalisation]]).
- Data Storage (See: [[*Data Repository:][Data Repository]]).

**** Glossary:
***** Data Ingestion:
The process of reading in raw data from Un/Structured Data sources.

*Suggestion:* Store the raw data (eg. Cloud Data Warehouse) before processing.
Allows re-processing in the future.

Streaming name convention: Producers/Publishers/Senders.
***** Data Repository:
The Target DB that the Data Pipeline writes into. Often called a /"Data
Warehouse/" or /"Data Lake/".

Streaming name convention is: Consumers/Subscribers/Recipients.
***** Data Visualisation:
Visually display the Data (Charts/graphics/animations/etc), to communicate
complex data relationships and data-driven insights into an understandable
form. See: [[https://www.ibm.com/topics/data-visualization][IBM: Data Visualisation]].
***** EDA (Exploratory Data Analysis):
EDA is used be Data Scientists to analyse/investigate data sets and summarise
their main characteristics. Often using [[*Data Visualisation:][Data Visualisation]] to discover
patterns/anomalies, test hypothesis/assumptions. See: [[https://www.ibm.com/topics/exploratory-data-analysis][IBM: Exploratory Data
Analysis]].
***** ETL (Extract, Transform, Load):
When batch processing Data into the target DB Schema, you would often write an
ETL integration process to normalise the data.

An ETL Pipeline is a sub-category of a Data Pipeline, because:

- ETL is strictly Extract/Transform/Load(store in a [[*Data Repository:][Data Repository]]), but a
  Data Pipeline may not follow this sequence.
- ETL is typically Batch Processing-only.
***** Event:
Data that describes a single /"action/". eg. A sale at a checkout.
***** ML (Machine Learning):
Machine learning is a branch of artificial intelligence (AI) and computer
science which focuses on the use of data and algorithms to imitate the way that
humans learn, gradually improving its accuracy. Through the use of statistical
methods, algorithms are trained to make classifications or predictions,
uncovering key insights within data mining projects.
***** Normalisation:
The process of converting/serialising messy (/"noisy/") source data to the
structured target DB Schema. The types of data transformation steps that may be
done are:

- Filtering.
- Masking.
- Aggregation/Merging.
- Summarising.

The above steps are usually chained together as a pipeline of
steps. eg. =Ingestion > filtering to X columns > aggregation > ...=, that
eventually write into the [[*Data Repository:][Data Repository]].
***** Stream:
Same as [[*Topic:][Topic]].
***** Topic:
A grouping of Related [[*Event:][Events]]. eg. Adding an item to a checkout.
**** Links:
- [[https://www.ibm.com/topics/data-pipeline][IBM: Data Pipelines]].
- [[https://docs.aws.amazon.com/datapipeline/latest/DeveloperGuide/what-is-datapipeline.html][AWS Docs: What is AWS Data Pipeline?]]
*** Distributed Networking:
**** [[https://www.envoyproxy.io/][Envoy Proxy]] ([[https://www.envoyproxy.io/docs/envoy/latest/][Envoy docs]]):
#+BEGIN_QUOTE
The network should be transparent to applications. When network and application
problems do occur it should be easy to determine the source of the problem.
#+END_QUOTE

As on the ground microservice practitioners quickly realize, the majority of
operational problems that arise when moving to a distributed architecture are
ultimately grounded in two areas: networking and observability. It is simply an
orders of magnitude larger problem to network and debug a set of intertwined
distributed services versus a single monolithic application.

Originally built at Lyft, Envoy is a high performance C++ distributed proxy
designed for single services and applications, as well as a communication bus
and “universal data plane” designed for large microservice “service mesh”
architectures. Built on the learnings of solutions such as NGINX, HAProxy,
hardware load balancers, and cloud load balancers, Envoy runs alongside every
application and abstracts the network by providing common features in a
platform-agnostic manner. When all service traffic in an infrastructure flows
via an Envoy mesh, it becomes easy to visualize problem areas via consistent
observability, tune overall performance, and add substrate features in a single
place.

- Out of Process Architecture - Self-contained, low memory footprint server
  that runs alongside Application.
- HTTP/2, (HTTP/3 in alpha ~1.19.0~) gRPC support.
  - Transparent HTTP/1.1 / HTTP/2 proxy.
- Load Balancing - retries, circuit breaking, global rate limiting, request
  shadowing, zone local load balancing, etc.
- Configuration management API's.
- Service Discovery - eg. via DNS resolution.
- Front/Edge Proxy Support.
- Observability - L7 traffic, distributed tracing, wire-level observations of
  MongoDB, DynamoDB, Redis, Postgres.
- Health Checking - Assume Eventual Consistency.
- YAML config files.

See: [[https://www.envoyproxy.io/docs/envoy/latest/intro/life_of_a_request][Envoy (Docs): Life of a Request]].
*** Queues:
**** Glossary:
***** Message Broker:
- Direct communication between explicit Services (one-to-one).
- Responsibilities: Validate, Route, Store, Deliver messages to designated
  recipient.
- Intermediary between Services/Applications.
  - ie. *Decouple knowledge of Receivers/Consumers location from Sender*.
  - May have: 0, 1, Many Consumers (unknown to Sender).
***** Publish/Subscribe:
- Message distribution pattern.
- Broadcast-style distribution (one-to-many).
**** [[http://kafka.apache.org/][Kafka]]:
Apache Kafka is an open-source distributed event *streaming* platform used by
thousands of companies for high-performance data pipelines, streaming
analytics, data integration, and mission-critical applications.
- 2011.
- Java/Scala-based.
  - SDK for adding custom support of other languages.
- Streams.
  - Ideal for *A* to *B* streaming for max throughput and simple routing.
  - Ideal for:
    - Event Sourcing.
    - Stream Processing.
    - Modelling Changes to a System as a Sequence of Events.
    - Processing Data in multi-stage pipelines.
    - Routine System auditing.
    - Storing messages permanently.
  - *Framework for storing, reading, re-reading and analysing streaming data.*
- Throughput.
- Uses Pub/Sub pattern.
- Uses a Message Log, instead of a Message Queue.
  - *Pull-based*
  - Consumer must request to get batches of messages (offsets).
    - PRO: network efficiency.
    - CON: High-latency.
- *Use Data Lake analysis tools to efficiently store, manage, analyse the Kafka
  streams.*
***** Breakdown:
See: [[https://www.simplilearn.com/kafka-vs-rabbitmq-article][SimpliLearn: Kafka VS RabitMQ]].

- *Performance:* 1 million messages per second.
- *Message Retention:* Policy-based.
- *Data Type:* Operational.
- *Consumer Mode:* Dumb Broker / Smart Consumer.
- *Topology:* Pub/Sub.
- *Payload Size:* Default 1MB limit.
- *Usage Cases:* Massive data / High throughput cases.
**** MSSQL [[https://learn.microsoft.com/en-us/sql/database-engine/service-broker/building-applications-with-service-broker?view=sql-server-ver16][Service Broker]]:
Applies to: ￼ SQL Server (all supported versions) ￼ Azure SQL Managed Instance

Any program that can run *Transact-SQL statements* can use Service Broker. A
Service Broker application can be implemented as a program running outside of
SQL Server, or as a stored procedure written in Transact-SQL or a .NET
language.

A program that uses Service Broker is typically composed of a number of
components working together to accomplish a task. A program that initiates a
conversation creates and sends a message to another service. That program may
wait for a response, or exit immediately and rely on another program to process
the response. For a service that is the target of a conversation, the program
receives an incoming message from the queue for the service, reads the message
data, does any necessary processing, and then creates and sends a response
message if appropriate.

Service Broker extends Transact-SQL. An application does not need a special
object model or library to work with Service Broker. Instead, programs send
Transact-SQL commands to SQL Server and process the results of those
commands. An application can be activated by Service Broker, can run as a
background service, can run as a scheduled job, or can be started in response
to an event.
***** Uses:
- [[https://learn.microsoft.com/en-us/sql/database-engine/service-broker/messages?view=sql-server-ver16][Messages]].
- [[https://learn.microsoft.com/en-us/sql/database-engine/service-broker/queues?view=sql-server-ver16][Queues]].
***** Why?:
- Consolidation if already in Windows (MSSQL) Eco-system.
  - Service Broker is a part of the MSSQL deployment.
  - Messages are R/W from the same DB that Application(s) uses.
  - Offload message queuing outside of the Application to the Platform (DB).
**** Python Module Queues:
These are low-level queues that can be used within Python Applications, where
the same module is used on both sides of the queue:

- [[https://docs.python.org/3/library/asyncio-queue.html][~asyncio.queue~]] - Async. Not thread-safe, but designed for ~async~ / ~await~
  code.
- [[https://docs.python.org/3/library/queue.html#module-queue][~queue~ (built-in)]] - Synchronous. Thread-safe. Multi-Producer /
  Multi-Consumer queues.
- [[https://www.tornadoweb.org/en/stable/queues.html?highlight=queue][~tornado.queues~]] - Async. Queues for Tornado coroutines, like:
  [[https://docs.python.org/3/library/asyncio-queue.html][~asyncio.queue~]].
**** [[https://www.rabbitmq.com][RabbitMQ]]:
- Distributed Message Broker
  - Deploy a Cluster of Nodes = HA.
- *Push-based*
  - Consumer prefetch limits.
  - Low-latency messaging.
  - Ideal for:
    - Complex (non-trivial) routing to multiple Consumers/Applications.
    - High-throughput & reliable background jobs.
    - Rapid request-response.
    - Load balance across Worker nodes (20k+ messages/second).
    - Long running tasks.
    - Communication/Integration between and within Applications.
- Implements AMQP natively (and AMQP (future versions), HTTP, STOMP, MQTT via
  plugins).
- Large official support for popular languages + plugins.
***** Breakdown:

See: [[https://www.simplilearn.com/kafka-vs-rabbitmq-article][SimpliLearn: Kafka VS RabitMQ]].

- *Performance:* 4k-10k messages per second.
- *Message Retention:* Acknowledgement-based.
- *Data Type:* Transactional.
- *Consumer Mode:* Smart Broker / Dumb Consumer.
- *Topology:* Exchange Type: Direct, Fan out, Topic, Header-based.
- *Payload Size:* No constraints.
- *Usage Cases:* Simple use cases.
**** [[https://zeromq.org][ZeroMQ]]:
ZeroMQ (also known as ØMQ, 0MQ, or zmq) looks like an embeddable networking
library but acts like a concurrency framework. It gives you sockets that carry
atomic messages across various transports like in-process, inter-process, TCP,
and multicast. You can connect sockets N-to-N with patterns like fan-out,
pub-sub, task distribution, and request-reply. It's fast enough to be the
fabric for clustered products. Its asynchronous I/O model gives you scalable
multicore applications, built as asynchronous message-processing tasks. It has
a score of language APIs and runs on most operating systems.
** Design Tools:
*** [[https://plantuml.com][PlantUML:]]
- Renders UML into multiple formats (ASCII, PNG, SVG, PDF).
- Supports designing multiple Design diagrams (Sequence, Flow, Block, Class,
  User, Gantt, etc...).
- Java-based, supplied as a `jar`.
- Integrates with everything. See: [[https://plantuml.com/running][PlantUML: Running (Integrations)]].
*** [[https://mermaid.js.org][Mermaid]]:
JavaScript based diagramming and charting tool that renders Markdown-inspired
text definitions to create and modify diagrams dynamically.

- [[https://github.com/mermaid-js/mermaid][Github: mermaid-js/mermaid]].
- [[https://github.blog/2022-02-14-include-diagrams-markdown-files-mermaid/][Github Blog: Include diagrams in your Markdown files with Mermaid]].
* Testing Philosophy:
** TDD:
TDD (Test Driven Development), is the practice of defining (and implementing)
tests before writing Production code. Links:

- [[http://butunclebob.com/ArticleS.UncleBob.TheThreeRulesOfTdd][Uncle Bob: The Three Rules of TDD]].
- [[https://www.codecademy.com/article/tdd-red-green-refactor][Codecademy: TDD Red/Green/Refactor]].
- [[https://www.jamesshore.com/v2/blog/2005/red-green-refactor][James Shore: Red/Green/Refactor]].
*** [[elisp:(elfeed-tube-fetch "https://www.youtube.com/watch?v=EZ05e7EMOLM")][TDD, Where Did It All Go Wrong (Ian Cooper)]].
- **Test behaviours, not implementation!**
  - Refactor/Re-implementation **must not** require code changes!!
  - Implementation tests slow changes due to keeping sync.
- **Test contract boundaries, not classes!**
  - eg. modules/API's/ports.
  - Avoid mocks!
  - Speed at all cost!
- **Don't test internal functions!**
- **Can write exploratory implementation tests, but always delete afterwards!!**
  - Controversial, but it makes sense. I believe the implementation
    knowledge/reasoning should then be written into good Production-side
    implementation docs.
- [[*Red, Green, Refactor:][Red, Green, Refactor]].
- [[*TDD Books:][Books]].
*** Red, Green, Refactor:
Baby steps to always releasable code from constantly cycling through design,
hypothesis & validation.
- Think:
  - Invest time.
  - What test will move code towards completion.
- Red:
  - ~30secs.
  - Write simplest behaviour tests.
- Green:
  - ~30secs.
  - Duct-tape/dirty code to get test passing quickly.
  - Copy from StackOverflow.
  - no structure.
- Refactor:
  - Invest time.
  - Don't write new tests!
  - Remove duplication!
  - Apply pattern(s).
  - Remove code smells.
  - Clean up.
- Repeat:
  - ~20-40 cycles per hour.
  - Expect cycle time to ebb & flow.
*** TDD Books:
- [[https://www.amazon.co.uk/Refactoring-Improving-Existing-Addison-Wesley-Technology-ebook/dp/B007WTFWJ6/ref=sr_1_4][Refactoring: Improving the Design of Existing Code (1st Edition)(Martin Fowler)]].
- [[https://www.amazon.co.uk/Refactoring-Improving-Existing-Addison-Wesley-Technology/dp/0134757599/ref=sr_1_1][Refactoring: Improving the Design of Existing Code (2nd Edition)(Martin Fowler)]].
- [[https://www.amazon.co.uk/Test-Driven-Development-Addison-Wesley-Signature/dp/0321146530/ref=sr_1_1][Test Driven Development: By Example (Kent Beck)]].
* Training Sites:
- https://www.educative.io/ - Text-based teaching resources (instead of video)
  and web-based coding environments.`
